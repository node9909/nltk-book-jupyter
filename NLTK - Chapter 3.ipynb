{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important imports\n",
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Import to do many outputs per cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Limit total total number of characters that can be displayed on outputs\n",
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager().update('notebook', {'limit_output': 200})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Electronic Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffThe Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\\r'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1176968"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accessing a text from project Gutenberg\n",
    "from urllib import request\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "\n",
    "raw[:75]\n",
    "\n",
    "len(raw)\n",
    "\n",
    "type(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\ufeffThe',\n",
       " 'Project',\n",
       " 'Gutenberg',\n",
       " 'EBook',\n",
       " 'of',\n",
       " 'Crime',\n",
       " 'and',\n",
       " 'Punishment',\n",
       " ',',\n",
       " 'by']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokens = word_tokenize(raw)\n",
    "\n",
    "tokens[:10]\n",
    "\n",
    "type(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.text.Text"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['EBOOK',\n",
       " 'CRIME',\n",
       " 'AND',\n",
       " 'PUNISHMENT',\n",
       " '***',\n",
       " 'Produced',\n",
       " 'by',\n",
       " 'John',\n",
       " 'Bickers',\n",
       " ';',\n",
       " 'and',\n",
       " 'Dagny']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Katerina Ivanovna; Pyotr Petrovitch; Pulcheria Alexandrovna; Avdotya\n",
      "Romanovna; Rodion Romanovitch; Marfa Petrovna; Sofya Semyonovna; old\n",
      "woman; Project Gutenberg-tm; Porfiry Petrovitch; Amalia Ivanovna;\n",
      "great deal; young man; Nikodim Fomitch; Ilya Petrovitch; Project\n",
      "Gutenberg; Andrey Semyonovitch; Hay Market; Dmitri Prokofitch; Good\n",
      "heavens\n"
     ]
    }
   ],
   "source": [
    "# Some regular list operations are readily available\n",
    "text = nltk.Text(tokens)\n",
    "\n",
    "type(text)\n",
    "\n",
    "text[100:112]\n",
    "\n",
    "text.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting just the book text \n",
    "start = raw.find(\"PART I\")\n",
    "end   = raw.rfind(\"End of Project Gutenberg's Crime\") # Reverse find\n",
    "\n",
    "raw   = raw[start:end]\n",
    "raw.find(\"PART I\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN\" \"http://www.w3.org/TR/REC-html40/loose.dtd\">\r\n",
      "<html>\r\n",
      "<head>\r\n",
      "<title>BBC NEWS | Health | Blondes 'to die out in 200 years'</title>\r\n",
      "<meta name=\"keywords\" content=\"BBC, News, BBC News, news online, world, uk, international, foreign, british, online, service\">\r\n",
      "<meta name=\"OriginalPublicationDate\" content=\"2002/09/27 11:51:55\">\r\n",
      "<meta name=\"UKFS_URL\" content=\"/1/hi/health/2284783.stm\">\r\n",
      "<meta name=\"IFS_URL\" content=\"/2/hi/health/2284783.stm\">\r\n",
      "<meta name=\"HTTP-EQUIV\" content=\"text/html;charset=iso-8859-1\">\r\n",
      "<meta name=\"Headline\" content=\"Blondes 'to die out in 200 years'\">\r\n",
      "<meta name=\"Section\" content=\"Health\">\r\n",
      "<meta name=\"Description\" content=\"Natural blondes are an endangered species and will die out by 2202, a study suggests.\">\r\n",
      "<!-- GENMaps-->\r\n",
      "<map name=\"banner\">\r\n",
      "<area alt=\"BBC NEWS\" coords=\"7,9,167,32\" href=\"http://news.bbc.co.uk/1/hi.html\" shape=\"RECT\">\r\n",
      "</map>\r\n",
      "\r\n",
      "<script src=\"/nol/shared/js/livestats_v1_1.js\" language=\"JavaScript\" type=\"text/javascript\"></script>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\t\t\t\r\n",
      "\t<map name=\"world_map\">\r\n",
      "<area alt=\"Americas\" coords=\"40,55,40,4,6,4,7,55\" href=\"/2/hi/americas/default.stm\" shape=\"POLY\">\r\n",
      "<area alt=\"Africa\" shape=\"POLY\" coords=\"41,54,41,27,46,27,46,32,46,33,55,33,55,36,58,36,58,54\" href=\"/2/hi/africa/default.stm\">\r\n",
      "<area alt=\"Europe\" shape=\"POLY\" coords=\"41,5,41,25,63,25,63,17,73,17,73,4\" href=\"/2/hi/europe/default.stm\">\r\n",
      "<area alt=\"Middle East\" shape=\"POLY\" coords=\"60,54,60,54,60,34,57,34,57,31,48,31,48,27,63,27,63,54\" href=\"/2/hi/middle_east/default.stm\">\r\n",
      "<area alt=\"South Asia\" coords=\"67,55,65,54,65,27,71,27,71,54\" href=\"/2/hi/south_asia/default.stm\" shape=\"POLY\">\r\n",
      "<area alt=\"Asia Pacific\" shape=\"POLY\" coords=\"75,54,73,54,73,25,65,25,65,19,75,19,75,5,94,5,94,56\" href=\"/2/hi/asia-pacific/default.stm\">\r\n",
      "</map>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<link type=\"text/css\" rel=\"stylesheet\" href=\"/nol/shared/stylesheets/uki_globalstylesheet.css\">\r\n",
      "<script src=\"/nol/shared/js_ifs/nol.js\" language=\"JavaScript\"></script>\r\n",
      "</head>\r\n",
      "\r\n",
      "<body bgcolor=\"#FFFFFF\" text=\"#000000\" link=\"#000066\" alink=\"#000066\" vlink=\"#993333\" topmargin=\"0\" leftmargin=\"0\" marginheight=\"0\" marginwidth=\"0\">\r\n",
      "<!--[START]--(( BBCi TOOLBAR )) IFS News -->\r\n",
      "<table width=\"610\" cellpadding=\"0\" cellspacing=\"0\" border=\"0\">\r\n",
      "<tr>\r\n",
      "<td class=\"bbcpageShadow\"><a name=\"top\"><img src=\"/nol/shared/img/global_toolbar/t.gif\" width=\"600\" height=\"2\" alt=\"\"/></a></td>\r\n",
      "</tr>\r\n",
      "<form action=\"http://newssearch.bbc.co.uk/cgi-bin/search/results.pl\">\r\n",
      "<input type=\"hidden\" name=\"scope\" value=\"newsifs\">\r\n",
      "<input type=\"hidden\" name=\"tab\" value=\"news\">\r\n",
      "<tr>\r\n",
      "<td class=\"bbcpageGrey\">\r\n",
      "<table cellpadding=\"0\" cellspacing=\"0\" border=\"0\">\r\n",
      "<tr>\r\n",
      "<td class=\"bbcpageShadowLeft\" width=\"100%\"><a href=\"http://www.bbc.co.uk/\"><img src=\"/nol/shared/img/global_toolbar/logo.gif\" width=\"62\" height=\"20\" alt=\"BBCi\" border=\"0\" hspace=\"7\" vspace=\"2\" /></a></td>\r\n",
      "<td class=\"bbcpageGreyT\" align=\"right\"><font color=\"#000000\"><font face=\"tahoma,arial,helvetica,sans-serif\" size=\"1\" class=\"bbcpageWhite\"><b><a href=\"http://news.bbc.co.uk\" class=\"bbcpageWhite\">NEWS</a></b></font></td>\r\n",
      "<td class=\"bbcpageBar\"><font face=\"tahoma,arial,helvetica,sans-serif\" size=\"1\" class=\"bbcpageWhite\">&nbsp;&nbsp;<b><a href=\"http://news.bbc.co.uk/sport/\" class=\"bbcpageWhite\">SPORT</a></b></font></td>\r\n",
      "<td class=\"bbcpageBar2\"><font face=\"tahoma,arial,helvetica,sans-serif\" size=\"1\" class=\"bbcpageWhite\">&nbsp;&nbsp;<b><a href=\"http://www.bbc.co.uk/weather/\" class=\"bbcpageWhite\">WEATHER</a></b></font></td>\r\n",
      "<td class=\"bbcpageBar\"><font face=\"tahoma,arial,helvetica,sans-serif\" size=\"1\" class=\"bbcpageWhite\">&nbsp;&nbsp;<b><a href=\"http://www.bbc.co.uk/worldservice/index.shtml\" class=\"bbcpageWhite\">WORLD SERVICE</a></b></font></td>\r\n",
      "<!-- <td class=\"bbcpageBar2\"><font face=\"tahoma,arial,helvetica,sans-serif\" size=\"1\" class=\"bbcpageWhite\">&nbsp;&nbsp;<b><a href=\"http://www.bbc.co.uk/whereilive/\" class=\"bbcpageWhite\">WHERE&nbsp;I&nbsp;LIVE</a></b></font></td> -->\r\n",
      "<td class=\"bbcpageBar\"><font face=\"tahoma,arial,helvetica,sans-serif\" size=\"1\" class=\"bbcpageWhite\">&nbsp;&nbsp;<b><a href=\"http://www.bbc.co.uk/a-z/\" class=\"bbcpageWhite\">A-Z INDEX</a>&nbsp;</b></font></td>\r\n",
      "<td class=\"bbcpageSearchL\"><img src=\"/furniture/nothing.gif\" width=\"2\" height=\"30\" alt=\"\"/></td>\r\n",
      "<td class=\"bbcpageSearch\"><font face=\"tahoma,arial,helvetica,sans-serif\" size=\"1\" class=\"bbcpageCream\">&nbsp;&nbsp;<label for=\"bbcpagesearchbox\"><b>SEARCH</b></label>&nbsp;</font></td>\r\n",
      "<td class=\"bbcpageSearch2\" style=\"font-family:tahoma,arial,helvetica,sans-serif;\"><img src=\"/shared/img/o.gif\" width=\"1\" height=\"3\" alt=\"\" /><br /><INPUT type=\"text\" name=\"q\" id=\"bbcpagesearchbox\" size=\"5\" style=\"width:95px;\" /></td>\r\n",
      "<td class=\"bbcpageSearch\"><img src=\"/shared/img/o.gif\" width=\"3\" height=\"1\" alt=\"\" /></td>\r\n",
      "<td class=\"bbcpageSearch2\"><img src=\"/shared/img/o.gif\" width=\"1\" height=\"5\" alt=\"\" /><br /><input type=\"image\" value=\"go\" src=\"/nol/shared/img/global_toolbar/go.gif\" width=\"20\" height=\"16\" border=\"0\" alt=\"Go\" align=\"top\" /></td>\r\n",
      "<td class=\"bbcpageSearch\"><img src=\"/shared/img/o.gif\" width=\"3\" height=\"1\" alt=\"\" /></td>\r\n",
      "<td class=\"bbcpageSearchR\"><img src=\"/shared/img/o.gif\" width=\"1\" height=\"30\" alt=\"\" /></td>\r\n",
      "</tr>\r\n",
      "\r\n",
      "<tr bgcolor=\"#000000\">\r\n",
      "<td><img src=\"/shared/img/o.gif\" width=\"76\" height=\"1\" alt=\"\"/></td>\r\n",
      "<td><img src=\"/shared/img/o.gif\" width=\"76\" height=\"1\" alt=\"\"/></td>\r\n",
      "<td><img src=\"/shared/img/o.gif\" width=\"20\" height=\"1\" alt=\"\"/></td>\r\n",
      "<td><img src=\"/shared/img/o.gif\" width=\"42\" height=\"1\" alt=\"\"/></td>\r\n",
      "<td><img src=\"/shared/img/o.gif\" width=\"94\" height=\"1\" alt=\"\"/></td>\r\n",
      "<td><img src=\"/shared/img/o.gif\" width=\"80\" height=\"1\" alt=\"\"/></td>\r\n",
      "<!-- <td><img src=\"/shared/img/o.gif\" width=\"42\" height=\"1\" alt=\"\"/></td> -->\r\n",
      "<td><img src=\"/shared/img/o.gif\" width=\"2\" height=\"1\" alt=\"\"/></td>\r\n",
      "<td><img src=\"/shared/img/o.gif\" width=\"51\" height=\"1\" alt=\"\"/></td>\r\n",
      "<td><img src=\"/shared/img/o.gif\" width=\"100\" height=\"1\" alt=\"\"/></td>\r\n",
      "<td><img src=\"/shared/img/o.gif\" width=\"3\" height=\"1\" alt=\"\"/></td>\r\n",
      "<td><img src=\"/shared/img/o.gif\" width=\"20\" height=\"1\" alt=\"\"/></td>\r\n",
      "<td><img src=\"/shared/img/o.gif\" width=\"3\" height=\"1\" alt=\"\"/></td>\r\n",
      "<td><img src=\"/shared/img/o.gif\" width=\"1\" height=\"1\" alt=\"\"/></td>\r\n",
      "</tr>\r\n",
      "\r\n",
      "</table></td>\r\n",
      "\r\n",
      "</tr>\r\n",
      "</form>\r\n",
      "</table>\r\n",
      "<!-- end news toolbar 1.0 -->\r\n",
      "<table width=\"610\" cellspacing=\"0\" cellpadding=\"0\" border=\"0\">\r\n",
      "<tr><td bgcolor=\"#9C0000\"><img src=\"/nol/shared/img/banners/ifs_banner.gif\" width=\"610\" height=\"40\" border=\"0\" alt=\"BBC News World Edition\" usemap=\"#banner\"></td></tr></table>\r\n",
      "<!--END OF BANNER-->\r\n",
      "\r\n",
      "<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"610\">\r\n",
      "<tr>\r\n",
      "<td width=\"10\"><img src=\"/shared/img/o.gif\" width=\"10\" height=\"1\" alt=\"\"></td>\r\n",
      "<td width=\"100\"><img src=\"/shared/img/o.gif\" width=\"100\" height=\"1\" alt=\"\"></td>\r\n",
      "<td width=\"5\"><img src=\"/shared/img/o.gif\" width=\"5\" height=\"1\" alt=\"\"></td>\r\n",
      "<td bgcolor=\"#999966\" width=\"315\"><img src=\"/shared/img/o.gif\" width=\"315\" height=\"1\" alt=\"\"></td>\r\n",
      "<td bgcolor=\"#999966\" width=\"10\"><img src=\"/shared/img/o.gif\" width=\"10\" height=\"1\" alt=\"\"></td>\r\n",
      "<td bgcolor=\"#999966\" width=\"170\"><img src=\"/shared/img/o.gif\" width=\"170\" height=\"1\" alt=\"\"></td>\r\n",
      "</tr>\r\n",
      "\r\n",
      "<tr>\r\n",
      "<td width=\"10\"><img src=\"/shared/img/o.gif\" width=\"10\" height=\"1\" alt=\"\"></td>\r\n",
      "<td width=\"100\"><img src=\"/shared/img/o.gif\" width=\"100\" height=\"1\" alt=\"\"></td>\r\n",
      "<td width=\"5\"><img src=\"/shared/img/o.gif\" width=\"5\" height=\"3\" alt=\"\"></td>\r\n",
      "<td colspan=\"3\" width=\"495\" class=\"crumbtraila\" bgcolor=\"#CCCC99\">\r\n",
      "    &nbsp;You are in:&nbsp;<a href=\"/2/hi/health/default.stm\"><b>Health</b> </a>&nbsp;\r\n",
      "    \r\n",
      "    \r\n",
      "</td>\r\n",
      "</tr>\r\n",
      "<tr>\r\n",
      "<td width=\"10\"><img src=\"/shared/img/o.gif\" width=\"10\" height=\"1\" alt=\"\"></td>\r\n",
      "<td align=\"LEFT\" valign=\"TOP\">\r\n",
      "    \r\n",
      "\t<table width=\"100\" cellspacing=\"0\" cellpadding=\"0\" border=\"0\">\r\n",
      "\t<tr>\r\n",
      "\t<td><img height=\"1\" border=\"0\" width=\"96\" alt=\"\" src=\"http://newsimg.bbc.co.uk/shared/img/o.gif\"></td>\r\n",
      "\t<td><img height=\"1\" border=\"0\" width=\"4\" alt=\"\" src=\"http://newsimg.bbc.co.uk/shared/img/o.gif\"></td>\r\n",
      "\t</tr>\r\n",
      "\t\r\n",
      "\t\r\n",
      "\t\t\r\n",
      "\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t<tr>\r\n",
      "\t\t\t\t\t<td bgColor=\"#FFFFFF\" class=\"sectionStyleTight\" align=\"right\"><a href=\"/2/hi/default.stm\" class=\"index\"><b>News Front Page</b></a></td><td bgcolor=\"#FFFFFF\"><img height=\"1\" width=\"1\" alt=\"\" src=\"http://newsimg.bbc.co.uk/shared/img/o.gif\"></td>\r\n",
      "\t\t\t\t\t</tr>\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t<tr>\r\n",
      "\t\t\t\t\t<td colspan=\"2\"><img height=\"60\" usemap=\"#world_map\" border=\"0\" width=\"100\" alt=\"\" src=\"http://newsimg.bbc.co.uk/nol/shared/img/nav/blue_map_world.gif\"></td>\r\n",
      "\t\t\t\t\t</tr>\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\r\n",
      "\t\r\n",
      "\r\n",
      "\t\r\n",
      "\r\n",
      "\t\r\n",
      "\t\t\r\n",
      "\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t<tr>\r\n",
      "\t\t\t\t\t<td bgColor=\"#FFFFFF\" class=\"sectionStyle\" align=\"right\"><a href=\"/2/hi/africa/default.stm\" class=\"index\"><b>Africa</b></a></td><td bgcolor=\"#FFFFFF\"><img height=\"1\" width=\"1\" alt=\"\" src=\"http://newsimg.bbc.co.uk/shared/img/o.gif\"></td>\r\n",
      "\t\t\t\t\t</tr>\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\r\n",
      "\t\r\n",
      "\r\n",
      "\t\r\n",
      "\r\n",
      "\t\r\n",
      "\t\t\r\n",
      "\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t<tr>\r\n",
      "\t\t\t\t\t<td bgColor=\"#FFFFFF\" class=\"sectionStyle\" align=\"right\"><a href=\"/2/hi/americas/default.stm\" class=\"index\"><b>Americas</b></a></td><td bgcolor=\"#FFFFFF\"><img height=\"1\" width=\"1\" alt=\"\" src=\"http://newsimg.bbc.co.uk/shared/img/o.gif\"></td>\r\n",
      "\t\t\t\t\t</tr>\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\r\n",
      "\t\r\n",
      "\r\n",
      "\t\r\n",
      "\r\n",
      "\t\r\n",
      "\t\t\r\n",
      "\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t<tr>\r\n",
      "\t\t\t\t\t<td bgColor=\"#FFFFFF\" class=\"sectionStyle\" align=\"right\"><a href=\"/2/hi/asia-pacific/default.stm\" class=\"index\"><b>Asia-Pacific</b></a></td><td bgcolor=\"#FFFFFF\"><img height=\"1\" width=\"1\" alt=\"\" src=\"http://newsimg.bbc.co.uk/shared/img/o.gif\"></td>\r\n",
      "\t\t\t\t\t</tr>\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\r\n",
      "\t\r\n",
      "\r\n",
      "\t\r\n",
      "\r\n",
      "\t\r\n",
      "\t\t\r\n",
      "\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t<tr>\r\n",
      "\t\t\t\t\t<td bgColor=\"#FFFFFF\" class=\"sectionStyle\" align=\"right\"><a href=\"/2/hi/europe/default.stm\" class=\"index\"><b>Europe</b></a></td><td bgcolor=\"#FFFFFF\"><img height=\"1\" width=\"1\" alt=\"\" src=\"http://newsimg.bbc.co.uk/shared/img/o.gif\"></td>\r\n",
      "\t\t\t\t\t</tr>\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\r\n",
      "\t\r\n",
      "\r\n",
      "\t\r\n",
      "\r\n",
      "\t\r\n",
      "\t\t\r\n",
      "\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t<tr>\r\n",
      "\t\t\t\t\t<td bgColor=\"#FFFFFF\" class=\"sectionStyle\" align=\"right\"><a href=\"/2/hi/middle_east/default.stm\" class=\"index\"><b>Middle East</b></a></td><td bgcolor=\"#FFFFFF\"><img height=\"1\" width=\"1\" alt=\"\" src=\"http://newsimg.bbc.co.uk/shared/img/o.gif\"></td>\r\n",
      "\t\t\t\t\t</tr>\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\r\n",
      "\t\r\n",
      "\r\n",
      "\t\r\n",
      "\r\n",
      "\t\r\n",
      "\t\t\r\n",
      "\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t<tr>\r\n",
      "\t\t\t\t\t<td bgColor=\"#FFFFFF\" class=\"sectionStyle\" align=\"right\"><a href=\"/2/hi/south_asia/default.stm\" class=\"index\"><b>South Asia</b></a></td><td bgcolor=\"#FFFFFF\"><img height=\"1\" width=\"1\" alt=\"\" src=\"http://newsimg.bbc.co.uk/shared/img/o.gif\"></td>\r\n",
      "\t\t\t\t\t</tr>\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\r\n",
      "\t\r\n",
      "\r\n",
      "\t\r\n",
      "\r\n",
      "\t\r\n",
      "\t\t\r\n",
      "\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t<tr>\r\n",
      "\t\t\t\t\t<td bgColor=\"#FFFFFF\" class=\"sectionStyle\" align=\"right\"><a href=\"/2/hi/uk_news/default.stm\" class=\"index\"><b>UK</b></a></td><td bgcolor=\"#FFFFFF\"><img height=\"1\" width=\"1\" alt=\"\" src=\"http://newsimg.bbc.co.uk/shared/img/o.gif\"></td>\r\n",
      "\t\t\t\t\t</tr>\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\r\n",
      "\t\r\n",
      "\r\n",
      "\t\r\n",
      "\r\n",
      "\t\r\n",
      "\t\t\r\n",
      "\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t<tr>\r\n",
      "\t\t\t\t\t<td bgColor=\"#FFFFFF\" class=\"sectionStyle\" align=\"right\"><a href=\"/2/hi/business/default.stm\" class=\"index\"><b>Business</b></a></td><td bgcolor=\"#FFFFFF\"><img height=\"1\" width=\"1\" alt=\"\" src=\"http://newsimg.bbc.co.uk/shared/img/o.gif\"></td>\r\n",
      "\t\t\t\t\t</tr>\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\r\n",
      "\t\r\n",
      "\r\n",
      "\t\r\n",
      "\r\n",
      "\t\r\n",
      "\t\t\r\n",
      "\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t<tr>\r\n",
      "\t\t\t\t\t<td bgColor=\"#FFFFFF\" class=\"sectionStyle\" align=\"right\"><a href=\"/2/hi/entertainment/default.stm\" class=\"index\"><b>Entertainment</b></a></td><td bgcolor=\"#FFFFFF\"><img height=\"1\" width=\"1\" alt=\"\" src=\"http://newsimg.bbc.co.uk/shared/img/o.gif\"></td>\r\n",
      "\t\t\t\t\t</tr>\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\r\n",
      "\t\r\n",
      "\r\n",
      "\t\r\n",
      "\r\n",
      "\t\r\n",
      "\t\t\r\n",
      "\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t<tr>\r\n",
      "\t\t\t\t\t<td bgColor=\"#FFFFFF\" class=\"sectionStyleTight\" align=\"right\"><a href=\"/2/hi/science/nature/default.stm\" class=\"index\"><b>Science/Nature</b></a></td><td bgcolor=\"#FFFFFF\"><img height=\"1\" width=\"1\" alt=\"\" src=\"http://newsimg.bbc.co.uk/shared/img/o.gif\"></td>\r\n",
      "\t\t\t\t\t</tr>\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\r\n",
      "\t\r\n",
      "\r\n",
      "\t\r\n",
      "\r\n",
      "\t\r\n",
      "\t\t\r\n",
      "\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t<tr>\r\n",
      "\t\t\t\t\t<td bgColor=\"#FFFFFF\" class=\"sectionStyle\" align=\"right\"><a href=\"/2/hi/technology/default.stm\" class=\"index\"><b>Technology</b></a></td><td bgcolor=\"#FFFFFF\"><img height=\"1\" width=\"1\" alt=\"\" src=\"http://newsimg.bbc.co.uk/shared/img/o.gif\"></td>\r\n",
      "\t\t\t\t\t</tr>\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\r\n",
      "\t\r\n",
      "\r\n",
      "\t\r\n",
      "\r\n",
      "\t\r\n",
      "\t\t\r\n",
      "\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t<tr>\r\n",
      "\t\t\t\t\t<td bgColor=\"#999999\" class=\"sectionStyle\" align=\"right\"><a href=\"/2/hi/health/default.stm\" class=\"index\"><b>Health</b></a></td><td bgColor=\"#CC3300\"><img height=\"1\" width=\"1\" alt=\"\" src=\"http://newsimg.bbc.co.uk/shared/img/o.gif\"></td>\r\n",
      "\t\t\t\t\t</tr>\r\n",
      "\t\t\t\t\t\r\n",
      "\t\r\n",
      "\t\t\r\n",
      "\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t<tr>\r\n",
      "\t\t\t\t\t<td bgColor=\"#CCCCCC\" align=\"right\" class=\"sectionStyle\"><a href=\"/2/hi/health/medical_notes/default.stm\" class=\"index\"><b>Medical notes</b></a></td><td bgcolor=\"#CCCCCC\"><img height=\"1\" width=\"1\" alt=\"\" src=\"http://newsimg.bbc.co.uk/shared/img/o.gif\"></td>\r\n",
      "\t\t\t\t\t</tr>\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\r\n",
      "\t\r\n",
      "\r\n",
      "\t\r\n",
      "\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t<tr>\r\n",
      "\t\t\t\t\t<td align=\"CENTER\" colspan=\"2\"><span class=\"lhsNavSeparator\">-------------</span></td>\r\n",
      "\t\t\t\t\t</tr>\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\r\n",
      "\t\r\n",
      "\r\n",
      "\t\r\n",
      "\r\n",
      "\t\r\n",
      "\t\t\r\n",
      "\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t<tr>\r\n",
      "\t\t\t\t\t<td bgColor=\"#FFFFFF\" class=\"sectionStyle\" align=\"right\"><a href=\"/2/hi/talking_point/default.stm\" class=\"index\"><b>Talking Point</b></a></td><td bgcolor=\"#FFFFFF\"><img height=\"1\" width=\"1\" alt=\"\" src=\"http://newsimg.bbc.co.uk/shared/img/o.gif\"></td>\r\n",
      "\t\t\t\t\t</tr>\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t<tr>\r\n",
      "\t\t\t\t\t<td align=\"CENTER\" colspan=\"2\"><span class=\"lhsNavSeparator\">-------------</span></td>\r\n",
      "\t\t\t\t\t</tr>\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\r\n",
      "\t\r\n",
      "\r\n",
      "\t\r\n",
      "\r\n",
      "\t\r\n",
      "\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t<tr>\r\n",
      "\t\t\t\t<td bgColor=\"#FFFFFF\" class=\"sectionStyleTight\" align=\"right\"><a href=\"/2/shared/bsp/hi/country_profiles/html/default.stm\" class=\"index\"><b>Country Profiles</b></a></td><td bgcolor=\"#FFFFFF\"><img height=\"1\" width=\"1\" alt=\"\" src=\"http://newsimg.bbc.co.uk/shared/img/o.gif\"></td>\r\n",
      "\t\t\t\t</tr>\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\r\n",
      "\t\t\r\n",
      "\t\r\n",
      "\r\n",
      "\t\r\n",
      "\r\n",
      "\t\r\n",
      "\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t<tr>\r\n",
      "\t\t\t\t<td bgColor=\"#FFFFFF\" class=\"sectionStyle\" align=\"right\"><a href=\"/2/shared/bsp/hi/in_depth/html/default.stm\" class=\"index\"><b>In Depth</b></a></td><td bgcolor=\"#FFFFFF\"><img height=\"1\" width=\"1\" alt=\"\" src=\"http://newsimg.bbc.co.uk/shared/img/o.gif\"></td>\r\n",
      "\t\t\t\t</tr>\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t<tr>\r\n",
      "\t\t\t\t<td align=\"CENTER\" colspan=\"2\"><span class=\"lhsNavSeparator\">-------------</span></td>\r\n",
      "\t\t\t\t</tr>\r\n",
      "\t\t\t\r\n",
      "\t\t\r\n",
      "\t\t\r\n",
      "\t\r\n",
      "\r\n",
      "\t\r\n",
      "\r\n",
      "\t\r\n",
      "\t\t\r\n",
      "\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t<tr>\r\n",
      "\t\t\t\t\t<td bgColor=\"#FFFFFF\" class=\"sectionStyle\" align=\"right\"><a href=\"/2/hi/programmes/default.stm\" class=\"index\"><b>Programmes</b></a></td><td bgcolor=\"#FFFFFF\"><img height=\"1\" width=\"1\" alt=\"\" src=\"http://newsimg.bbc.co.uk/shared/img/o.gif\"></td>\r\n",
      "\t\t\t\t\t</tr>\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t<tr>\r\n",
      "\t\t\t\t\t<td align=\"CENTER\" colspan=\"2\"><span class=\"lhsNavSeparator\">-------------</span></td>\r\n",
      "\t\t\t\t\t</tr>\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\r\n",
      "\t\r\n",
      "\r\n",
      "\t\r\n",
      "\r\n",
      "\t<tr><td colspan=\"2\"><img height=\"5\" border=\"0\" width=\"1\" alt=\"\" src=\"http://newsimg.bbc.co.uk/shared/img/o.gif\"></td></tr>\r\n",
      "\t</table>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "    \r\n",
      "    \r\n",
      "\r\n",
      "\r\n",
      "<table width=\"100\" cellspacing=\"0\" cellpadding=\"0\" border=\"0\">\r\n",
      "<tr>\r\n",
      "<td><a href=\"/sport2/hi/default.stm\"><img src=\"/nol/shared/img/nav/sport.gif\" width=\"100\" height=\"13\" alt=\"BBC Sport\" border=\"0\"></a></td>\r\n",
      "</tr>\r\n",
      "<tr>\r\n",
      "<td><img src=\"/shared/img/o.gif\" height=\"3\" width=\"1\" border=\"0\"></td>\r\n",
      "</tr>\r\n",
      "<tr>\r\n",
      "<td><a href=\"http://www.bbc.co.uk/weather/worldweather/index.shtml\"><img src=\"/nol/shared/img/nav/weather.gif\" width=\"100\" height=\"13\" alt=\"BBC Weather\" border=\"0\"></a></td>\r\n",
      "</tr>\r\n",
      "<tr>\r\n",
      "<td><img src=\"/shared/img/o.gif\" height=\"10\" width=\"1\" border=\"0\"></td>\r\n",
      "</tr>\r\n",
      "\r\n",
      "</table>\r\n",
      "\r\n",
      "\r\n",
      "<table width=\"100\" cellspacing=\"0\" cellpadding=\"0\" border=\"0\">\r\n",
      "<tr bgcolor=\"#990000\">\r\n",
      "<td><img src=\"/shared/img/o.gif\" width=\"1\" height=\"1\" alt=\"\"></td>\r\n",
      "<td><img src=\"/shared/img/o.gif\" width=\"3\" height=\"1\" alt=\"\"></td>\r\n",
      "<td><img src=\"/shared/img/o.gif\" width=\"90\" height=\"1\" alt=\"\"></td>\r\n",
      "<td><img src=\"/shared/img/o.gif\" width=\"5\" height=\"1\" alt=\"\"></td>\r\n",
      "<td><img src=\"/shared/img/o.gif\" width=\"1\" height=\"1\" alt=\"\"></td>\r\n",
      "</tr>\r\n",
      "<tr bgcolor=\"#990000\">\r\n",
      "<td colspan=\"3\" align=\"right\"><div class=\"servicehead\">SERVICES\r\n",
      "</div></td>\r\n",
      "<td><img src=\"/shared/img/o.gif\" width=\"5\" height=\"1\" alt=\"\"></td>\r\n",
      "<td><img src=\"/shared/img/o.gif\" width=\"1\" height=\"1\" alt=\"\"></td>\r\n",
      "</tr>\r\n",
      "<tr>\r\n",
      "<td bgcolor=\"#CCCCCC\"><img src=\"/shared/img/o.gif\" width=\"1\" height=\"1\" alt=\"\"></td>\r\n",
      "<td class=\"servicebg1\"><img src=\"/shared/img/o.gif\" width=\"3\" height=\"1\" alt=\"\"></td>\r\n",
      "<td align=\"right\" class=\"servicebg1\"><div class=\"servicesnav\"><a href=\"http://www.bbc.co.uk/email/news\">Daily E-mail\r\n",
      "</a></div></td>\r\n",
      "<td class=\"serviceoption1\"><img src=\"/shared/img/o.gif\" width=\"5\" height=\"1\" alt=\"\"></td>\r\n",
      "<td bgcolor=\"#CCCCCC\"><img src=\"/shared/img/o.gif\" width=\"1\" height=\"1\" alt=\"\"></td>\r\n",
      "</tr> \r\n",
      "<tr>\r\n",
      "<td bgcolor=\"#CCCCCC\"><img src=\"/shared/img/o.gif\" width=\"1\" height=\"1\" alt=\"\"></td>\r\n",
      "<td class=\"servicebg2\"><img src=\"/shared/img/o.gif\" width=\"3\" height=\"1\" alt=\"\"></td>\r\n",
      "<td align=\"right\" class=\"servicebg2\"><div class=\"servicesnav\"><a href=\"/2/shared/bsp/hi/services/ticker/html/default.stm\">News Ticker\r\n",
      "</a></div></td>\r\n",
      "<td class=\"serviceoption2\"><img src=\"/shared/img/o.gif\" width=\"5\" height=\"1\" alt=\"\"></td>\r\n",
      "<td bgcolor=\"#CCCCCC\"><img src=\"/shared/img/o.gif\" width=\"1\" height=\"1\" alt=\"\"></td>\r\n",
      "</tr>\r\n",
      "<tr>\r\n",
      "<td bgcolor=\"#CCCCCC\"><img src=\"/shared/img/o.gif\" width=\"1\" height=\"1\" alt=\"\"></td>\r\n",
      "<td class=\"servicebg3\"><img src=\"/shared/img/o.gif\" width=\"3\" height=\"1\" alt=\"\"></td>\r\n",
      "<td align=\"right\" class=\"servicebg3\"><div class=\"servicesnav\"><a href=\"/2/shared/bsp/hi/services/mobiles/html/default.stm\">Mobile/PDAs\r\n",
      "</a></div></td>\r\n",
      "<td class=\"serviceoption3\"><img src=\"/shared/img/o.gif\" width=\"5\" height=\"1\" alt=\"\"></td>\r\n",
      "<td bgcolor=\"#CCCCCC\"><img src=\"/shared/img/o.gif\" width=\"1\" height=\"1\" alt=\"\"></td>\r\n",
      "</tr>\r\n",
      "<tr>\r\n",
      "<td bgcolor=\"#CCCCCC\"><img src=\"/shared/img/o.gif\" width=\"1\" height=\"1\" alt=\"\"></td>\r\n",
      "<td align=\"center\" colspan=\"3\"><span class=\"lhsNavSeparator\">-------------\r\n",
      "</span></td>\r\n",
      "<td bgcolor=\"#CCCCCC\"><img src=\"/shared/img/o.gif\" width=\"1\" height=\"1\" alt=\"\"></td>\r\n",
      "</tr>\r\n",
      "</table>\r\n",
      "\r\n",
      "\r\n",
      "    <table width=\"100\" cellspacing=\"0\" cellpadding=\"0\" border=\"0\">\r\n",
      "    <tr>\r\n",
      "    <td bgcolor=\"#CCCCCC\"><img src=\"/shared/img/o.gif\" width=\"1\" height=\"1\" alt=\"\"></td>\r\n",
      "    <td><img src=\"/shared/img/o.gif\" width=\"3\" height=\"1\" alt=\"\"></td>\r\n",
      "    <td><img src=\"/shared/img/o.gif\" width=\"90\" height=\"1\" alt=\"\"></td>\r\n",
      "    <td><img src=\"/shared/img/o.gif\" width=\"5\" height=\"1\" alt=\"\"></td>\r\n",
      "    <td bgcolor=\"#CCCCCC\"><img src=\"/shared/img/o.gif\" width=\"1\" height=\"1\" alt=\"\"></td>\r\n",
      "    </tr>\r\n",
      "    <tr>\r\n",
      "    <td bgcolor=\"#CCCCCC\"><img src=\"/shared/img/o.gif\" width=\"1\" height=\"1\" alt=\"\"></td>\r\n",
      "    <td class=\"servicebg4\"><img src=\"/shared/img/o.gif\" width=\"3\" height=\"1\" alt=\"\"></td>\r\n",
      "    <td align=\"right\" class=\"servicebg4\"><div class=\"servicesnav\"><a href=\"/2/low/health/2284783.stm\">Text Only</a></div></td>\r\n",
      "    <td class=\"serviceoption4\"><img src=\"/shared/img/o.gif\" width=\"5\" height=\"1\" alt=\"\"></td>\r\n",
      "    <td bgcolor=\"#CCCCCC\"><img src=\"/shared/img/o.gif\" width=\"1\" height=\"1\" alt=\"\"></td>\r\n",
      "    </tr>\r\n",
      "    </table>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<table width=\"100\" cellspacing=\"0\" cellpadding=\"0\" border=\"0\">\r\n",
      "<tr>\r\n",
      "<td bgcolor=\"#CCCCCC\"><img src=\"/shared/img/o.gif\" width=\"1\" height=\"1\" alt=\"\"></td>\r\n",
      "<td class=\"servicebg5\"><img src=\"/shared/img/o.gif\" width=\"3\" height=\"1\" alt=\"\"></td>\r\n",
      "<td align=\"right\" class=\"servicebg5\"><div class=\"servicesnav\"><a href=\"/2/shared/bsp/hi/services/feedback/html/default.stm\">Feedback\r\n",
      "</a></div></td>\r\n",
      "<td class=\"serviceoption5\"><img src=\"/shared/img/o.gif\" width=\"5\" height=\"1\" alt=\"\"></td>\r\n",
      "<td bgcolor=\"#CCCCCC\"><img src=\"/shared/img/o.gif\" width=\"1\" height=\"1\" alt=\"\"></td>\r\n",
      "</tr>\r\n",
      "<tr>\r\n",
      "<td bgcolor=\"#CCCCCC\"><img src=\"/shared/img/o.gif\" width=\"1\" height=\"1\" alt=\"\"></td>\r\n",
      "<td class=\"servicebg6\"><img src=\"/shared/img/o.gif\" width=\"3\" height=\"1\" alt=\"\"></td>\r\n",
      "<td align=\"right\" class=\"servicebg6\"><div class=\"servicesnav\"><a href=\"/2/shared/bsp/hi/services/help/html/default.stm\">Help\r\n",
      "</a></div></td>\r\n",
      "<td class=\"serviceoption6\"><img src=\"/shared/img/o.gif\" width=\"5\" height=\"1\" alt=\"\"></td>\r\n",
      "<td bgcolor=\"#CCCCCC\"><img src=\"/shared/img/o.gif\" width=\"1\" height=\"1\" alt=\"\"></td>\r\n",
      "</tr>\r\n",
      "<tr bgcolor=\"#CCCCCC\">\r\n",
      "<td><img src=\"/shared/img/o.gif\" width=\"1\" height=\"1\" alt=\"\"></td>\r\n",
      "<td><img src=\"/shared/img/o.gif\" width=\"3\" height=\"1\" alt=\"\"></td>\r\n",
      "<td><img src=\"/shared/img/o.gif\" width=\"90\" height=\"1\" alt=\"\"></td>\r\n",
      "<td><img src=\"/shared/img/o.gif\" width=\"5\" height=\"1\" alt=\"\"></td>\r\n",
      "<td><img src=\"/shared/img/o.gif\" width=\"1\" height=\"1\" alt=\"\"></td>\r\n",
      "</tr>\r\n",
      "<tr>\r\n",
      "<td colspan=\"5\"><img src=\"/shared/img/o.gif\" width=\"1\" height=\"4\" alt=\"\"></td>\r\n",
      "</tr>\r\n",
      "</table>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<table width=\"100\" cellspacing=\"0\" cellpadding=\"0\" border=\"0\">\r\n",
      "<tr bgcolor=\"#6699CC\">\r\n",
      "<td><img src=\"/shared/img/o.gif\" width=\"1\" height=\"1\" alt=\"\"></td>\r\n",
      "<td><img src=\"/shared/img/o.gif\" width=\"3\" height=\"1\" alt=\"\"></td>\r\n",
      "<td><img src=\"/shared/img/o.gif\" width=\"90\" height=\"1\" alt=\"\"></td>\r\n",
      "<td><img src=\"/shared/img/o.gif\" width=\"5\" height=\"1\" alt=\"\"></td>\r\n",
      "<td><img src=\"/shared/img/o.gif\" width=\"1\" height=\"1\" alt=\"\"></td>\r\n",
      "</tr>\r\n",
      "<tr bgcolor=\"#336699\">\r\n",
      "<td colspan=\"3\" align=\"right\"><div class=\"servicehead\">EDITIONS\r\n",
      "</div></td>\r\n",
      "<td><img src=\"/shared/img/o.gif\" width=\"5\" height=\"1\" alt=\"\"></td>\r\n",
      "<td><img src=\"/shared/img/o.gif\" width=\"1\" height=\"1\" alt=\"\"></td>\r\n",
      "</tr>\r\n",
      "<tr>\r\n",
      "<td bgcolor=\"#CCCCCC\"><img src=\"/shared/img/o.gif\" width=\"1\" height=\"1\" alt=\"\"></td>\r\n",
      "<td><img src=\"/shared/img/o.gif\" width=\"3\" height=\"1\" alt=\"\"></td>\r\n",
      "<td align=\"right\"><div class=\"servicesnavtight\"><a href=\"/shared/hi/change_edition-news.stm\">Change to UK\r\n",
      "</a></div></td>\r\n",
      "<td><img src=\"/shared/img/o.gif\" width=\"5\" height=\"1\" alt=\"\"></td>\r\n",
      "<td bgcolor=\"#CCCCCC\"><img src=\"/shared/img/o.gif\" width=\"1\" height=\"1\" alt=\"\"></td>\r\n",
      "</tr>\r\n",
      "<tr bgcolor=\"#CCCCCC\">\r\n",
      "<td><img src=\"/shared/img/o.gif\" width=\"1\" height=\"1\" alt=\"\"></td>\r\n",
      "<td><img src=\"/shared/img/o.gif\" width=\"3\" height=\"1\" alt=\"\"></td>\r\n",
      "<td><img src=\"/shared/img/o.gif\" width=\"90\" height=\"1\" alt=\"\"></td>\r\n",
      "<td><img src=\"/shared/img/o.gif\" width=\"5\" height=\"1\" alt=\"\"></td>\r\n",
      "<td><img src=\"/shared/img/o.gif\" width=\"1\" height=\"1\" alt=\"\"></td>\r\n",
      "</tr>\r\n",
      "</table>\r\n",
      "</td>\r\n",
      "<td><img src=\"/shared/img/o.gif\" width=\"5\" height=\"200\" alt=\"\"></td>\r\n",
      "<td valign=\"TOP\">\r\n",
      "    \t\r\n",
      "\t\r\n",
      "\r\n",
      "                \r\n",
      "\r\n",
      "\r\n",
      "<font face=\"sans-serif\" size=\"1\"><span class=\"date\">Friday, 27 September, 2002, 11:51 GMT 12:51 UK\r\n",
      "</span></font>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\t\t\r\n",
      "\t\t\t<div class=\"headlinestory\"><b>Blondes 'to die out in 200 years'</b><br></div>\r\n",
      "\t\t\r\n",
      "\r\n",
      "\t\r\n",
      "\r\n",
      "\t\r\n",
      "                \r\n",
      "\t\r\n",
      "\t<div class=\"inlineimage\">\r\n",
      "\t\t<img height=\"180\" vspace=\"0\" border=\"0\" width=\"300\" alt=\" \" src=\"/media/images/38280000/jpg/_38280456_blonde300.jpg\">\r\n",
      "\t\t\r\n",
      "\t\t\t<div class=\"caption\"><font size=\"1\">Scientists believe the last blondes will be in Finland</font><br></div>\r\n",
      "\t\t\r\n",
      "\t</div>\r\n",
      "\t\r\n",
      "\r\n",
      "\t\r\n",
      "\r\n",
      "\t<font class=\"body\" face=\"sans-serif\" size=\"2\">\r\n",
      "\t<div class=\"bodytext\">\r\n",
      "\tThe last natural blondes will die out within 200 years, scientists believe. \r\n",
      "<P>\r\n",
      "A study by experts in Germany suggests people with blonde hair are an endangered species and will become extinct by 2202.\r\n",
      "<P>\r\n",
      "Researchers predict the last truly natural blonde will be born in Finland - the country with the highest proportion of blondes. \r\n",
      "<P>\r\n",
      "\r\n",
      "\r\n",
      "<!-- GENInlineBOX -->\r\n",
      "\r\n",
      "\t<table bgcolor=\"#FFFFCC\" class=\"boxbody\" cellspacing=\"0\" width=\"150\" border=\"0\" cellpadding=\"3\" align=\"right\">\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<!-- GENInlineQUOTE -->\r\n",
      "\r\n",
      "\t<tr><td><img src=\"/nol/shared/img/startquote.gif\" width=\"23\" height=\"18\" border=\"0\" valign=\"TOP\" alt=\"\"><br><div class=\"boxbody\">\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\tThe frequency of blondes may drop but they won't disappear\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\t</div><img align=\"RIGHT\" src=\"/nol/shared/img/endquote.gif\" width=\"23\" height=\"18\" border=\"0\" valign=\"ABSBOTTOM\" alt=\"\"><br clear=\"ALL\"></td></tr>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<!-- GENInlineNAME -->\r\n",
      "\r\n",
      "\t<tr><td bgcolor=\"cccc99\"><div class=\"boxhead\">\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\tProf Jonathan Rees, University of Edinburgh\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\t</div></td></tr>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\t</table>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\t\r\n",
      "But they say too few people now carry the gene for blondes to last beyond the next two centuries. \r\n",
      "<P>\r\n",
      "The problem is that blonde hair is caused by a recessive gene. \r\n",
      "<P>\r\n",
      "In order for a child to have blonde hair, it must have the gene on both sides of the family in the grandparents' generation. \r\n",
      "<P><B>Dyed rivals</B>\r\n",
      "<P>\r\n",
      "\r\n",
      "The researchers also believe that so-called bottle blondes may be to blame for the demise of their natural rivals. \r\n",
      "<P>\r\n",
      "They suggest that dyed-blondes are more attractive to men who choose them as partners over true blondes. \r\n",
      "<P>\r\n",
      "\r\n",
      "\r\n",
      "<!-- GENInlineIMAGE -->\r\n",
      "\r\n",
      "\t\t\r\n",
      "\t\t\t<table cellspacing=\"3\" align=\"RIGHT\" width=\"154\" border=\"0\" cellpadding=\"3\"><tr><td><font size=\"2\">\r\n",
      "\t\t\r\n",
      "\t\t<div class=\"inlineimage\">\r\n",
      "            <img alt=\"Tory MP Ann Widdecombe\" height=\"180\" vspace=\"0\" src=\"/media/images/38280000/jpg/_38280457_widders150.jpg\" border=\"0\" width=\"150\">\r\n",
      "            \r\n",
      "                <div class=\"caption\"><small>Bottle-blondes like Ann Widdecombe may be to blame</small><br></div>\r\n",
      "            \r\n",
      "\t\t</div>\r\n",
      "\t\t\r\n",
      "\t\t\t</font></td></tr></table>\r\n",
      "\t\t\r\n",
      "\r\n",
      "\t\r\n",
      "But Jonathan Rees, professor of dermatology at the University of Edinburgh said it was unlikely blondes would die out completely. \r\n",
      "<P>\r\n",
      "\"Genes don't die out unless there is a disadvantage of having that gene or by chance. They don't disappear,\" he told BBC News Online.\r\n",
      "<P>\r\n",
      "\"The only reason blondes would disappear is if having the gene was a disadvantage and I do not think that is the case. \r\n",
      "<P>\r\n",
      "\"The frequency of blondes may drop but they won't disappear.\"\r\n",
      "<P>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "</div>\r\n",
      "\t</font>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "</td>\r\n",
      "<td><img src=\"/shared/img/o.gif\" width=\"10\" height=\"1\" alt=\"\"></td>\r\n",
      "<td valign=\"TOP\"><font face=\"sans-serif\" size=\"2\">\r\n",
      "\t\r\n",
      "\t\r\n",
      "\r\n",
      "\t\r\n",
      "        \r\n",
      "\t<div class=\"rhslist\"><div class=\"rhshead\"><b>See also:</b><br><img src=\"/shared/img/999999.gif\" width=\"170\" vspace=\"2\" height=\"1\" alt=\"\"><br></div>\r\n",
      "\t\r\n",
      "\t\t<div class=\"aitem\">\r\n",
      "\t\t\t<span class=\"seealsodate\">28 Mar 01&nbsp;|&nbsp;Education</span>\r\n",
      "                        <div><a href=\"/2/hi/uk_news/education/1248103.stm\"><span>What is it about blondes?</span></a><br></div>\r\n",
      "\r\n",
      "\t\t</div>\r\n",
      "\t\r\n",
      "\t\t<div class=\"aitem\">\r\n",
      "\t\t\t<span class=\"seealsodate\">09 Apr 99&nbsp;|&nbsp;Health</span>\r\n",
      "                        <div><a href=\"/2/hi/health/315527.stm\"><span>Platinum blondes are labelled as dumb</span></a><br></div>\r\n",
      "\r\n",
      "\t\t</div>\r\n",
      "\t\r\n",
      "\t\t<div class=\"aitem\">\r\n",
      "\t\t\t<span class=\"seealsodate\">17 Apr 02&nbsp;|&nbsp;Health</span>\r\n",
      "                        <div><a href=\"/2/hi/health/1934496.stm\"><span>Hair dye cancer alert</span></a><br></div>\r\n",
      "\r\n",
      "\t\t</div>\r\n",
      "\t\r\n",
      "\t</div>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\t\r\n",
      "<div class=\"rhslist\">\r\n",
      "<div class=\"rhshead\"><b>Internet links:</b><br><img src=\"/shared/img/999999.gif\" width=\"170\" vspace=\"2\" height=\"1\" alt=\"\"><br></div>\r\n",
      "<div class=\"aitem\"><a href=\"http://www.ed.ac.uk/\">University of Edinburgh</a></div>\r\n",
      "<span class=\"disclaimer\"><br>The BBC is not responsible for the content of external internet sites</span><br>\r\n",
      "</div>\r\n",
      "\r\n",
      "\t\r\n",
      "            \r\n",
      "    \r\n",
      "<div class=\"rhslist\">\r\n",
      "\t<div class=\"rhshead\"><b>Top Health stories now:</b><br><img src=\"/shared/img/999999.gif\" width=\"170\" vspace=\"2\" height=\"1\" alt=\"\"><br></div>\r\n",
      "\t\r\n",
      "            <div class=\"bulletheadline\"><a href=\"/2/hi/health/2770999.stm\"><span>Heart risk link to big families</span></a><br></div>\r\n",
      "\r\n",
      "\t\r\n",
      "            <div class=\"bulletheadline\"><a href=\"/2/hi/health/2761469.stm\"><span>Back pain drug 'may aid diabetics'</span></a><br></div>\r\n",
      "\r\n",
      "\t\r\n",
      "            <div class=\"bulletheadline\"><a href=\"/2/hi/africa/2776719.stm\"><span>Congo Ebola outbreak confirmed</span></a><br></div>\r\n",
      "\r\n",
      "\t\r\n",
      "            <div class=\"bulletheadline\"><a href=\"/2/hi/health/2772499.stm\"><span>Vegetables ward off Alzheimer's</span></a><br></div>\r\n",
      "\r\n",
      "\t\r\n",
      "            <div class=\"bulletheadline\"><a href=\"/2/hi/middle_east/2774951.stm\"><span>Polio campaign launched in Iraq</span></a><br></div>\r\n",
      "\r\n",
      "\t\r\n",
      "            <div class=\"bulletheadline\"><a href=\"/2/hi/health/2760843.stm\"><span>Gene defect explains high blood pressure</span></a><br></div>\r\n",
      "\r\n",
      "\t\r\n",
      "            <div class=\"bulletheadline\"><a href=\"/2/hi/health/2772263.stm\"><span>Botox 'may cause new wrinkles'</span></a><br></div>\r\n",
      "\r\n",
      "\t\r\n",
      "            <div class=\"bulletheadline\"><a href=\"/2/hi/in_depth/sci_tech/2003/denver_2003/2769875.stm\"><span>Alien 'abductees' show real symptoms</span></a><br></div>\r\n",
      "\r\n",
      "\t\r\n",
      "</div>\r\n",
      "<img src=\"/shared/img/999999.gif\" width=\"170\" height=\"1\" border=\"0\" alt=\"\">\r\n",
      "<div class=\"promotextbold\">Links to more Health stories are at the foot of the page.<br><img src=\"/shared/img/999999.gif\" width=\"170\" height=\"1\" border=\"0\" vspace=\"4\" alt=\"\"><br>\r\n",
      "</div>\r\n",
      "\r\n",
      "\r\n",
      "    \r\n",
      "    \r\n",
      "\r\n",
      "\t\r\n",
      "</font></td>\r\n",
      "</tr>\r\n",
      "</table>\r\n",
      "<br>\r\n",
      "<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\">\r\n",
      "<tr>\r\n",
      "  <td width=\"115\"><img src=\"/shared/img/o.gif\" width=\"105\" height=\"1\" border=\"0\" alt=\"\"></td>\r\n",
      "  <td width=\"315\"><img border=\"0\" alt=\"\" src=\"/shared/img/999999.gif\" width=\"315\" height=\"1\"></td>\r\n",
      "</tr>\r\n",
      "<tr>\r\n",
      "  <td width=\"115\" height=\"18\"><img src=\"/shared/img/o.gif\" width=\"115\" height=\"1\" border=\"0\" alt=\"\"></td>\r\n",
      "  <td width=\"315\" align=\"left\"><a onClick=\"window.open('http://newsvote.bbc.co.uk/cgi-bin/emailthisstory/emailthisstory.pl','Mailer','status=no,scrollbars=yes,resizable=yes,width=370,height=445');\" href=\"http://newsvote.bbc.co.uk/cgi-bin/emailthisstory/emailthisstory.pl\" class=\"index\" target=\"Mailer\"><img border=\"0\" alt=\"\" src=\"/sol/shared/img/mail_icon.gif\" width=\"13\" height=\"9\"> <span class=\"blueText\"><b>E-mail this story to a friend</b></span></td>\r\n",
      "</tr>\r\n",
      "</table>\r\n",
      "\r\n",
      "    <table border=\"0\" cellpadding=\"0\" cellspacing=\"0\">\r\n",
      "  <tr>\r\n",
      "    <td width=\"115\"><img src=\"/shared/img/o.gif\" width=\"115\" height=\"1\" alt=\"\"></td>\r\n",
      "    <td width=\"315\"><img border=\"0\" alt=\"\" src=\"/shared/img/999999.gif\" width=\"315\" height=\"1\"></td>\r\n",
      "  </tr>\r\n",
      "  <tr>\r\n",
      "    <td><img src=\"/shared/img/o.gif\" width=\"115\" height=\"1\" alt=\"\"></td>\r\n",
      "    <td align=\"left\">\r\n",
      "\r\n",
      "<br clear=\"all\">\r\n",
      "<font size=\"2\">\r\n",
      " <span class=\"blackText\">\r\n",
      "<b>Links to more Health stories</b></span>\r\n",
      "<br>\r\n",
      "<form name=\"storyMenu\">\r\n",
      "<select name=\"storyLink\">\r\n",
      "<option value=\"#\">In This Section</option>\r\n",
      "\r\n",
      "        <OPTION value=\"/2/hi/health/2770999.stm\">Heart risk link to big families</OPTION>\r\n",
      "\r\n",
      "        <OPTION value=\"/2/hi/health/2761469.stm\">Back pain drug 'may aid diabetics'</OPTION>\r\n",
      "\r\n",
      "        <OPTION value=\"/2/hi/africa/2776719.stm\">Congo Ebola outbreak confirmed</OPTION>\r\n",
      "\r\n",
      "        <OPTION value=\"/2/hi/health/2772499.stm\">Vegetables ward off Alzheimer's</OPTION>\r\n",
      "\r\n",
      "        <OPTION value=\"/2/hi/middle_east/2774951.stm\">Polio campaign launched in Iraq</OPTION>\r\n",
      "\r\n",
      "        <OPTION value=\"/2/hi/health/2760843.stm\">Gene defect explains high blood pressure</OPTION>\r\n",
      "\r\n",
      "        <OPTION value=\"/2/hi/health/2772263.stm\">Botox 'may cause new wrinkles'</OPTION>\r\n",
      "\r\n",
      "        <OPTION value=\"/2/hi/in_depth/sci_tech/2003/denver_2003/2769875.stm\">Alien 'abductees' show real symptoms</OPTION>\r\n",
      "\r\n",
      "        <OPTION value=\"/2/hi/health/2760623.stm\">How sperm wriggle</OPTION>\r\n",
      "\r\n",
      "        <OPTION value=\"/2/hi/south_asia/2770437.stm\">Bollywood told to stub it out</OPTION>\r\n",
      "\r\n",
      "        <OPTION value=\"/2/hi/health/2772005.stm\">Fears over tuna health risk to babies</OPTION>\r\n",
      "\r\n",
      "        <OPTION value=\"/2/hi/health/2758249.stm\">Public can be taught to spot strokes</OPTION>\r\n",
      "\r\n",
      "</select>\r\n",
      "<INPUT VALUE=\"Go\" TYPE=\"BUTTON\" onClick=\"window.location = document.storyMenu.storyLink.options[document.storyMenu.storyLink.options.selectedIndex].value\">\r\n",
      "</form>\r\n",
      "</font>\r\n",
      "\r\n",
      "</td>\r\n",
      "  </tr>\r\n",
      "</table>\r\n",
      "\r\n",
      "    \r\n",
      "    \r\n",
      "\r\n",
      "<br clear=\"ALL\">\r\n",
      "<table width=\"610\" cellpadding=\"0\" cellspacing=\"0\" border=\"0\">\r\n",
      "<tr>\r\n",
      "<td><img border=\"0\" alt=\"\" src=\"/shared/img/o.gif\" width=\"100\" height=\"1\"></td>\r\n",
      "<td><img border=\"0\" alt=\"\" src=\"/shared/img/o.gif\" width=\"10\" height=\"1\"></td>\r\n",
      "<td><img border=\"0\" alt=\"\" src=\"/shared/img/o.gif\" width=\"500\" height=\"1\"></td>\r\n",
      "</tr>\r\n",
      "<tr>\r\n",
      "<td colspan=\"3\"><img vspace=\"4\" alt=\"\" src=\"/shared/img/000000.gif\" width=\"610\" height=\"1\"></td>\r\n",
      "</tr>\r\n",
      "<tr>\r\n",
      "<td align=\"right\" valign=\"top\"><a href=\"/2/shared/bsp/hi/services/copyright/html/default.stm\"><img src=\"/nol/shared/img/specials_nav/copyright_bbc.gif\" alt=\"&copy; BBC\" border=\"0\"></a></td>\r\n",
      "<td><img border=\"0\" alt=\"\" src=\"/shared/img/o.gif\" width=\"10\" height=\"1\"></td>\r\n",
      "<td>\r\n",
      "<font size=\"2\">\r\n",
      "<span class=\"footerarrow\">^^  </span>\r\n",
      "<span class=\"footer\"><a href=\"#top\">Back to top</a>\r\n",
      "<p>\r\n",
      "\r\n",
      "    <table width=\"500\" cellpadding=\"0\" cellspacing=\"0\" border=\"0\">\r\n",
      "<tr>\r\n",
      "<td><img border=\"0\" alt=\"\" src=\"/shared/img/o.gif\" width=\"500\" height=\"1\"></td>\r\n",
      "</tr>\r\n",
      "<tr>\r\n",
      "<td><b class=\"footer\"><span class=\"footerpiping\">\r\n",
      "<a href=\"/2/hi/default.stm\" class=\"index\">News Front Page</a>\r\n",
      " | \r\n",
      "\r\n",
      "\r\n",
      "<a href=\"/2/hi/africa/default.stm\" class=\"index\">Africa</a>\r\n",
      " | \r\n",
      "\r\n",
      "\r\n",
      "<a href=\"/2/hi/americas/default.stm\" class=\"index\">Americas</a>\r\n",
      " | \r\n",
      "\r\n",
      "\r\n",
      "<a href=\"/2/hi/asia-pacific/default.stm\" class=\"index\">Asia-Pacific</a>\r\n",
      " | \r\n",
      "\r\n",
      "\r\n",
      "<a href=\"/2/hi/europe/default.stm\" class=\"index\">Europe</a>\r\n",
      " | \r\n",
      "\r\n",
      "\r\n",
      "<a href=\"/2/hi/middle_east/default.stm\" class=\"index\">Middle East</a>\r\n",
      " | \r\n",
      "<br>\r\n",
      "\r\n",
      "<a href=\"/2/hi/south_asia/default.stm\" class=\"index\">South Asia</a>\r\n",
      " | \r\n",
      "\r\n",
      "\r\n",
      "<a href=\"/2/hi/uk_news/default.stm\" class=\"index\">UK</a>\r\n",
      " | \r\n",
      "\r\n",
      "\r\n",
      "<a href=\"/2/hi/business/default.stm\" class=\"index\">Business</a>\r\n",
      " | \r\n",
      "\r\n",
      "\r\n",
      "<a href=\"/2/hi/entertainment/default.stm\" class=\"index\">Entertainment</a>\r\n",
      " | \r\n",
      "\r\n",
      "\r\n",
      "<a href=\"/2/hi/science/nature/default.stm\" class=\"index\">Science/Nature</a>\r\n",
      " | \r\n",
      "<br>\r\n",
      "\r\n",
      "<a href=\"/2/hi/technology/default.stm\" class=\"index\">Technology</a>\r\n",
      " | \r\n",
      "\r\n",
      "\r\n",
      "<a href=\"/2/hi/health/default.stm\" class=\"index\">Health</a>\r\n",
      " | \r\n",
      "\r\n",
      "\r\n",
      "<a href=\"/2/hi/talking_point/default.stm\" class=\"index\">Talking Point</a>\r\n",
      " | \r\n",
      "\r\n",
      "<a href=\"/2/shared/bsp/hi/country_profiles/html/default.stm\" class=\"index\">Country Profiles</a>\r\n",
      "\r\n",
      " | \r\n",
      "\r\n",
      "<a href=\"/2/shared/bsp/hi/in_depth/html/default.stm\" class=\"index\">In Depth</a>\r\n",
      "\r\n",
      " | \r\n",
      "<br>\r\n",
      "\r\n",
      "<a href=\"/2/hi/programmes/default.stm\" class=\"index\">Programmes</a>\r\n",
      "\r\n",
      "\r\n",
      "</span></b></td>\r\n",
      "</tr>\r\n",
      "</table>\r\n",
      "    \r\n",
      "    \r\n",
      "</span></font></td>\r\n",
      "</tr>\r\n",
      "</table>\r\n",
      "    <table width=\"610\" cellpadding=\"0\" cellspacing=\"0\" border=\"0\">\r\n",
      "    <tr>\r\n",
      "    <td><img src=\"/shared/img/o.gif\" width=\"110\" height=\"1\" alt=\"\"></td>\r\n",
      "    <!-- black line row -->\r\n",
      "    <td class=\"footer\" align=\"left\" valign=\"top\" width=\"600\" bgcolor=\"#FFFFFF\"><b><span style=\"color : #999999;\">----------------------------------------------------------------------------------\r\n",
      "</span>\r\n",
      "    <br>\r\n",
      "    <a class=\"index\" href=\"/sport2/\">To BBC Sport&gt;&gt;\r\n",
      "</a> <span style=\"color : #999999;\">|\r\n",
      "</span> <a class=\"index\" href=\"http://www.bbc.co.uk/weather/\">To BBC Weather&gt;&gt;\r\n",
      "</a> <span style=\"color : #999999;\">|\r\n",
      "</span> <a class=\"index\" href=\"http://www.bbc.co.uk/worldservice/index.shtml\">To BBC World Service&gt;&gt;\r\n",
      "</a>\r\n",
      "    <br>\r\n",
      "    <b><span style=\"color : #999999;\">----------------------------------------------------------------------------------\r\n",
      "</span>\r\n",
      "    <br>\r\n",
      "    <a class=\"footer\" href=\"/2/shared/bsp/hi/services/copyright/html/default.stm\"><span style=\"font-size : 10px\">&copy; MMIII\r\n",
      "</span></a> <font size=\"1\">|\r\n",
      "</font> <a class=\"footer\" href=\"/2/shared/bsp/hi/services/help/html/sources.stm\"><span style=\"font-size : 10px\">News Sources\r\n",
      "</span></a> <font size=\"1\">|\r\n",
      "</font> <a class=\"footer\" href=\"http://www.bbc.co.uk/privacy/\"><span style=\"font-size : 10px\">Privacy\r\n",
      "</span></a></b>\r\n",
      "    <br><br><br>\r\n",
      "    </td>\r\n",
      "    </tr>\r\n",
      "    </table>\r\n",
      "<!-- START RedMeasure V4 - Java v1.1  $Revision: 1.9 $ -->\r\n",
      "<!-- COPYRIGHT 2000 Red Sheriff Limited -->\r\n",
      "\r\n",
      "<script language=\"JavaScript\"><!--\r\n",
      "var pCid=\"uk_bbc_0\";\r\n",
      "var w0=1;\r\n",
      "var refR=escape(document.referrer);\r\n",
      "if (refR.length>=252) refR=refR.substring(0,252)+\"...\";\r\n",
      "//--></script>\r\n",
      "<script language=\"JavaScript1.1\"><!--\r\n",
      "var w0=0;\r\n",
      "//--></script>\r\n",
      "<script language=\"JavaScript1.1\" src=\"http://server-uk.imrworldwide.com/a1.js\">\r\n",
      "</script>\r\n",
      "<script language=\"JavaScript\"><!--\r\n",
      "if(w0){\r\n",
      "var imgN='<img src=\"http://server-uk.imrworldwide.com/cgi-bin/count?ref='+\r\n",
      "\trefR+'&cid='+pCid+'\" width=1 height=1>';\r\n",
      "if(navigator.userAgent.indexOf('Mac')!=-1){document.write(imgN);\r\n",
      "}else{\r\n",
      "\tdocument.write('<applet code=\"Measure.class\" '+\r\n",
      "\t'codebase=\"http://server-uk.imrworldwide.com/\"'+'width=1 height=2>'+\r\n",
      "\t'<param name=\"ref\" value=\"'+refR+'\">'+'<param name=\"cid\" value=\"'+pCid+\r\n",
      "\t'\"><textflow>'+imgN+'</textflow></applet>');\r\n",
      "\t}\r\n",
      "}\r\n",
      "document.write(\"<COMMENT>\");\r\n",
      "//-->\r\n",
      "</script>\r\n",
      "<noscript>\r\n",
      "<img src=\"http://server-uk.imrworldwide.com/cgi-bin/count?cid=uk_bbc_0\" width=1 height=1>\r\n",
      "</noscript>\r\n",
      "</COMMENT>\r\n",
      "\r\n",
      "<!-- END RedMeasure V4 -->\r\n",
      "\r\n",
      "<script>\r\n",
      "\tvar si = document.location+\"\";\r\n",
      "\tvar tsi = si.replace(\".stm\",\"\").substr(si.length-11, si.length);\r\n",
      "\tif (!tsi.match(/\\d\\d\\d\\d\\d\\d\\d/)) {tsi = 0;}\r\n",
      "\tdocument.write('<img src=\"http://stats.bbc.co.uk/o.gif?~RS~s~RS~News~RS~t~RS~HighWeb_Legacy~RS~i~RS~' + tsi + '~RS~p~RS~0~RS~u~RS~/2/hi/health/2284783.stm~RS~r~RS~(none)~RS~a~RS~International~RS~q~RS~~RS~z~RS~14~RS~\">');\r\n",
      "</script>\r\n",
      "<noscript>\r\n",
      "\t<img src=\"http://stats.bbc.co.uk/o.gif?~RS~s~RS~News~RS~t~RS~HighWeb_Legacy~RS~i~RS~0~RS~p~RS~0~RS~u~RS~/2/hi/health/2284783.stm~RS~r~RS~(none)~RS~a~RS~International~RS~q~RS~~RS~z~RS~14~RS~\">\r\n",
      "</noscript>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<br>\r\n",
      "<link type=\"text/css\" rel=\"stylesheet\" href=\"/nol/shared/stylesheets/uki_globalstylesheet.css\">\r\n",
      "\r\n",
      "</body>\r\n",
      "</html>\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Making HTML from web available as text\n",
    "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
    "html = request.urlopen(url).read().decode('utf8')\n",
    "html[:60]\n",
    "\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NEWS',\n",
       " '|',\n",
       " 'Health',\n",
       " '|',\n",
       " 'Blondes',\n",
       " \"'to\",\n",
       " 'die',\n",
       " 'out',\n",
       " 'in',\n",
       " '200',\n",
       " \"years'\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, time to get text from this mess using BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "raw = BeautifulSoup(html).get_text()\n",
    "tokens = word_tokenize(raw)\n",
    "tokens[1:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 5 of 5 matches:\n",
      "hey say too few people now carry the gene for blondes to last beyond the next \n",
      "blonde hair is caused by a recessive gene . In order for a child to have blond\n",
      " have blonde hair , it must have the gene on both sides of the family in the g\n",
      "ere is a disadvantage of having that gene or by chance . They do n't disappear\n",
      "des would disappear is if having the gene was a disadvantage and I do not thin\n"
     ]
    }
   ],
   "source": [
    "# Trimming to get only the wanted text\n",
    "tokens = tokens[110:390]\n",
    "text = nltk.Text(tokens)\n",
    "text.concordance('gene') # Occurence of the word 'gene'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Search Engine Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only textual description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing RSS Feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Language Log'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'More literary troubles for Xi Jinping'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'<p><a href=\"https://www.aboluowang.com/2019/0102/1226427.html\">This ar'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'article',\n",
       " '(',\n",
       " 'in',\n",
       " 'Chinese',\n",
       " ')',\n",
       " 'describes',\n",
       " 'how',\n",
       " 'China',\n",
       " \"'s\",\n",
       " 'netizens',\n",
       " '(',\n",
       " 'wǎngyǒu',\n",
       " '网友',\n",
       " ')',\n",
       " 'are',\n",
       " 'ridiculing',\n",
       " 'President',\n",
       " 'Xi',\n",
       " 'for',\n",
       " 'inappropriately',\n",
       " 'quoting',\n",
       " 'a',\n",
       " 'poem',\n",
       " 'by',\n",
       " 'Kong',\n",
       " 'Rong',\n",
       " '孔融',\n",
       " '(',\n",
       " '153-208',\n",
       " ')',\n",
       " ',',\n",
       " 'a',\n",
       " '20th',\n",
       " 'generation',\n",
       " 'descendant',\n",
       " 'of',\n",
       " 'Confucius',\n",
       " ',',\n",
       " 'in',\n",
       " 'his',\n",
       " 'New',\n",
       " 'Year',\n",
       " \"'s\",\n",
       " 'address',\n",
       " 'to',\n",
       " 'the',\n",
       " 'nation',\n",
       " '.',\n",
       " 'The',\n",
       " 'first',\n",
       " 'lines',\n",
       " 'of',\n",
       " 'the',\n",
       " 'poem',\n",
       " 'are',\n",
       " ':',\n",
       " 'suìyuè',\n",
       " 'bù',\n",
       " 'jū',\n",
       " 'shíjié',\n",
       " 'rú',\n",
       " 'liú',\n",
       " '歲月不居',\n",
       " '時節如流',\n",
       " 'The',\n",
       " 'years',\n",
       " 'do',\n",
       " 'not',\n",
       " 'stand',\n",
       " 'still',\n",
       " ',',\n",
       " 'Time',\n",
       " 'flows',\n",
       " 'on',\n",
       " 'like',\n",
       " 'a',\n",
       " 'river',\n",
       " '.',\n",
       " 'Fair',\n",
       " 'enough',\n",
       " ',',\n",
       " 'in',\n",
       " 'and',\n",
       " 'of',\n",
       " 'itself',\n",
       " '.',\n",
       " 'The',\n",
       " 'problem',\n",
       " 'lies',\n",
       " 'in',\n",
       " 'what',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'the',\n",
       " 'poem',\n",
       " 'is',\n",
       " 'about',\n",
       " '.',\n",
       " 'If',\n",
       " 'you',\n",
       " 'put',\n",
       " 'those',\n",
       " 'two',\n",
       " 'lines',\n",
       " '(',\n",
       " 'in',\n",
       " 'Chinese',\n",
       " ')',\n",
       " 'into',\n",
       " 'Google',\n",
       " ',',\n",
       " 'you',\n",
       " 'will',\n",
       " 'get',\n",
       " 'many',\n",
       " 'critical',\n",
       " 'posts',\n",
       " 'and',\n",
       " 'reports',\n",
       " 'which',\n",
       " 'point',\n",
       " 'out',\n",
       " 'that',\n",
       " 'later',\n",
       " 'in',\n",
       " 'the',\n",
       " 'poem',\n",
       " 'the',\n",
       " 'following',\n",
       " 'lines',\n",
       " 'occur',\n",
       " ':',\n",
       " 'hǎinèi',\n",
       " 'zhīshì',\n",
       " 'língluò',\n",
       " 'dài',\n",
       " 'jǐn',\n",
       " '海內知識',\n",
       " '零落殆盡',\n",
       " 'Acquaintances',\n",
       " 'within',\n",
       " 'the',\n",
       " 'seas',\n",
       " ',',\n",
       " 'Scattered',\n",
       " 'and',\n",
       " 'nearly',\n",
       " 'exhausted',\n",
       " '.',\n",
       " 'By',\n",
       " '``',\n",
       " 'zhīshì',\n",
       " '知識',\n",
       " \"''\",\n",
       " ',',\n",
       " 'Kong',\n",
       " 'Rong',\n",
       " 'in',\n",
       " 'all',\n",
       " 'likelihood',\n",
       " 'was',\n",
       " 'referring',\n",
       " 'to',\n",
       " 'his',\n",
       " '``',\n",
       " 'acquaintances',\n",
       " \"''\",\n",
       " '(',\n",
       " 'his',\n",
       " 'friends',\n",
       " ')',\n",
       " ',',\n",
       " 'but',\n",
       " 'Xi',\n",
       " \"'s\",\n",
       " 'critics',\n",
       " 'emphasize',\n",
       " 'that',\n",
       " 'it',\n",
       " 'can',\n",
       " 'also',\n",
       " 'mean',\n",
       " '``',\n",
       " 'knowledge',\n",
       " \"''\",\n",
       " 'and',\n",
       " '``',\n",
       " 'intellectuals',\n",
       " \"''\",\n",
       " '.',\n",
       " 'Thus',\n",
       " 'they',\n",
       " 'interpret',\n",
       " 'these',\n",
       " 'lines',\n",
       " 'to',\n",
       " 'mean',\n",
       " 'that',\n",
       " 'knowledge',\n",
       " '``',\n",
       " 'within',\n",
       " 'the',\n",
       " 'seas',\n",
       " \"''\",\n",
       " '(',\n",
       " 'i.e.',\n",
       " ',',\n",
       " 'in',\n",
       " 'China',\n",
       " ')',\n",
       " 'is',\n",
       " 'desolate',\n",
       " 'and',\n",
       " 'nearly',\n",
       " 'depleted',\n",
       " '.',\n",
       " 'This',\n",
       " ',',\n",
       " 'they',\n",
       " 'imply',\n",
       " ',',\n",
       " 'is',\n",
       " 'the',\n",
       " 'direct',\n",
       " 'result',\n",
       " 'of',\n",
       " 'Xi',\n",
       " \"'s\",\n",
       " 'repressive',\n",
       " 'policies',\n",
       " 'exerting',\n",
       " 'draconian',\n",
       " 'control',\n",
       " 'over',\n",
       " 'the',\n",
       " 'internet',\n",
       " 'and',\n",
       " 'harsh',\n",
       " 'restrictions',\n",
       " 'on',\n",
       " 'intellectual',\n",
       " 'discourse',\n",
       " '.',\n",
       " 'Even',\n",
       " 'if',\n",
       " ',',\n",
       " 'putting',\n",
       " 'the',\n",
       " 'best',\n",
       " 'possible',\n",
       " 'light',\n",
       " 'on',\n",
       " 'the',\n",
       " 'matter',\n",
       " ',',\n",
       " 'we',\n",
       " 'insist',\n",
       " 'that',\n",
       " 'Kong',\n",
       " 'Rong',\n",
       " 'was',\n",
       " 'talking',\n",
       " 'about',\n",
       " 'friends',\n",
       " 'and',\n",
       " 'acquaintances',\n",
       " ',',\n",
       " 'not',\n",
       " 'knowledge',\n",
       " 'and',\n",
       " 'information',\n",
       " ',',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'still',\n",
       " 'not',\n",
       " 'a',\n",
       " 'very',\n",
       " 'felicitous',\n",
       " 'and',\n",
       " 'auspicious',\n",
       " 'thing',\n",
       " 'to',\n",
       " 'be',\n",
       " 'talking',\n",
       " 'about',\n",
       " 'their',\n",
       " 'desolation',\n",
       " 'and',\n",
       " 'depletion',\n",
       " 'on',\n",
       " 'New',\n",
       " 'Year',\n",
       " \"'s\",\n",
       " 'Day',\n",
       " '.',\n",
       " 'Xi',\n",
       " 'JInping',\n",
       " \"'s\",\n",
       " 'semi-learned',\n",
       " 'speechwriters',\n",
       " 'really',\n",
       " 'should',\n",
       " 'stop',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'make',\n",
       " 'him',\n",
       " 'sound',\n",
       " 'like',\n",
       " 'a',\n",
       " 'scholar',\n",
       " ',',\n",
       " 'because',\n",
       " 'he',\n",
       " 'keeps',\n",
       " 'mutilating',\n",
       " 'or',\n",
       " 'misusing',\n",
       " 'the',\n",
       " 'lines',\n",
       " 'that',\n",
       " 'they',\n",
       " 'feed',\n",
       " 'him',\n",
       " '.',\n",
       " 'As',\n",
       " 'numerous',\n",
       " 'commenters',\n",
       " 'to',\n",
       " 'previous',\n",
       " 'posts',\n",
       " 'in',\n",
       " 'this',\n",
       " 'series',\n",
       " 'have',\n",
       " 'pointed',\n",
       " 'out',\n",
       " ',',\n",
       " 'they',\n",
       " 'should',\n",
       " 'write',\n",
       " 'plain',\n",
       " ',',\n",
       " 'simple',\n",
       " ',',\n",
       " 'straightforward',\n",
       " ',',\n",
       " 'non-allusive',\n",
       " ',',\n",
       " 'easy',\n",
       " ',',\n",
       " 'vernacular',\n",
       " 'Mandarin',\n",
       " '.',\n",
       " 'The',\n",
       " 'chances',\n",
       " 'that',\n",
       " 'Xi',\n",
       " 'would',\n",
       " 'make',\n",
       " 'a',\n",
       " 'fool',\n",
       " 'of',\n",
       " 'himself',\n",
       " 'with',\n",
       " 'that',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'language',\n",
       " 'would',\n",
       " 'be',\n",
       " 'greatly',\n",
       " 'decreased',\n",
       " '.',\n",
       " 'What',\n",
       " \"'s\",\n",
       " 'the',\n",
       " 'point',\n",
       " 'of',\n",
       " 'making',\n",
       " 'him',\n",
       " 'sound',\n",
       " 'like',\n",
       " 'a',\n",
       " 'traditional',\n",
       " 'literatus',\n",
       " 'anyway',\n",
       " '?',\n",
       " 'That',\n",
       " 'would',\n",
       " 'be',\n",
       " 'like',\n",
       " 'speechwriters',\n",
       " 'for',\n",
       " 'US',\n",
       " 'presidents',\n",
       " 'writing',\n",
       " 'lines',\n",
       " 'full',\n",
       " 'of',\n",
       " 'Latin',\n",
       " 'and',\n",
       " 'Greek',\n",
       " '.',\n",
       " 'Instead',\n",
       " 'of',\n",
       " 'impressing',\n",
       " 'the',\n",
       " 'American',\n",
       " 'people',\n",
       " ',',\n",
       " 'it',\n",
       " 'would',\n",
       " 'only',\n",
       " 'make',\n",
       " 'them',\n",
       " 'think',\n",
       " 'that',\n",
       " 'their',\n",
       " 'president',\n",
       " 'was',\n",
       " 'a',\n",
       " 'pompous',\n",
       " 'ass',\n",
       " '.',\n",
       " 'Readings',\n",
       " \"''\",\n",
       " 'Annals',\n",
       " 'of',\n",
       " 'literary',\n",
       " 'vs.',\n",
       " 'vernacular',\n",
       " ',',\n",
       " 'part',\n",
       " '2',\n",
       " \"''\",\n",
       " '(',\n",
       " '9/4/16',\n",
       " ')',\n",
       " \"''\",\n",
       " 'Latin',\n",
       " 'Caesar',\n",
       " '–',\n",
       " '>',\n",
       " 'Tibetan',\n",
       " 'Gesar',\n",
       " '–',\n",
       " '>',\n",
       " 'Xi',\n",
       " 'Jinpingian',\n",
       " 'Sager',\n",
       " \"''\",\n",
       " '(',\n",
       " '3/20/18',\n",
       " ')',\n",
       " \"''\",\n",
       " 'Pinyin',\n",
       " 'for',\n",
       " 'the',\n",
       " 'Prez',\n",
       " \"''\",\n",
       " '(',\n",
       " '10/25/18',\n",
       " ')',\n",
       " \"''\",\n",
       " 'Peking',\n",
       " 'University',\n",
       " 'president',\n",
       " 'misreads',\n",
       " 'an',\n",
       " 'unobscure',\n",
       " 'character',\n",
       " ':',\n",
       " 'monumental',\n",
       " 'implications',\n",
       " \"''\",\n",
       " '(',\n",
       " '5/5/18',\n",
       " ')',\n",
       " \"''\",\n",
       " 'Xi',\n",
       " 'Jinping',\n",
       " \"'s\",\n",
       " 'reading',\n",
       " 'errors',\n",
       " 'multiply',\n",
       " \"''\",\n",
       " '(',\n",
       " '12/28/18',\n",
       " ')',\n",
       " '[',\n",
       " 'H.t',\n",
       " '.',\n",
       " 'Daan',\n",
       " 'Pan',\n",
       " ';',\n",
       " 'thanks',\n",
       " 'to',\n",
       " 'Zeyao',\n",
       " 'Wu',\n",
       " ',',\n",
       " 'Grace',\n",
       " 'Wu',\n",
       " ',',\n",
       " 'and',\n",
       " 'Melvin',\n",
       " 'Lee',\n",
       " ']']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This article (in Chinese) describes how China's netizens (wǎngyǒu 网友) are ridiculing President Xi for inappropriately quoting a poem by Kong Rong 孔融 (153-208), a 20th generation descendant of Confucius, in his New Year's address to the nation.\n",
      "The first lines of the poem are:\n",
      "suìyuè bù jū\n",
      "shíjié rú liú\n",
      "歲月不居\n",
      "時節如流\n",
      "The years do not stand still,\n",
      "Time flows on like a river.\n",
      "\n",
      "Fair enough, in and of itself.  The problem lies in what the rest of the poem is about.  If you put those two lines (in Chinese) into Google, you will get many critical posts and reports which point out that later in the poem the following lines occur:\n",
      "hǎinèi zhīshì\n",
      "língluò dài jǐn\n",
      "海內知識\n",
      "零落殆盡\n",
      "Acquaintances within the seas,\n",
      "Scattered and nearly exhausted.\n",
      "By \"zhīshì 知識\", Kong Rong in all likelihood was referring to his \"acquaintances\" (his friends), but Xi's critics emphasize that it can also mean \"knowledge\" and \"intellectuals\".  Thus they interpret these lines to mean that knowledge \"within the seas\" (i.e., in China) is desolate and nearly depleted.  This, they imply, is the direct result of Xi's repressive policies exerting draconian control over the internet and harsh restrictions on intellectual discourse.\n",
      "Even if, putting the best possible light on the matter, we insist that Kong Rong was talking about friends and acquaintances, not knowledge and information, it's still not a very felicitous and auspicious thing to be talking about their desolation and depletion on New Year's Day.\n",
      "Xi JInping's semi-learned speechwriters really should stop trying to make him sound like a scholar, because he keeps mutilating or misusing the lines that they feed him.  As numerous commenters to previous posts in this series have pointed out, they should write plain, simple, straightforward, non-allusive, easy, vernacular Mandarin.  The chances that Xi would make a fool of himself with that kind of language would be greatly decreased.  What's the point of making him sound like a traditional literatus anyway?  That would be like speechwriters for US presidents writing lines full of Latin and Greek.  Instead of impressing the American people, it would only make them think that their president was a pompous ass.\n",
      "Readings\n",
      "\"Annals of literary vs. vernacular, part 2\" (9/4/16)\n",
      "\"Latin Caesar –> Tibetan Gesar –> Xi Jinpingian Sager\" (3/20/18)\n",
      "\"Pinyin for the Prez\" (10/25/18)\n",
      "\"Peking University president misreads an unobscure character: monumental implications\" (5/5/18)\n",
      "\"Xi Jinping's reading errors multiply\" (12/28/18)\n",
      "[H.t. Daan Pan; thanks to Zeyao Wu, Grace Wu, and Melvin Lee]\n"
     ]
    }
   ],
   "source": [
    "# Accessing the content of a blog\n",
    "import feedparser\n",
    "llog = feedparser.parse(\"http://languagelog.ldc.upenn.edu/nll/?feed=atom\")\n",
    "llog['feed']['title']\n",
    "\n",
    "len(llog.entries)\n",
    "\n",
    "post = llog.entries[2]\n",
    "\n",
    "post.title\n",
    "\n",
    "content = post.content[0].value\n",
    "content [:70]\n",
    "\n",
    "raw = BeautifulSoup(content).get_text()\n",
    "word_tokenize(raw)\n",
    "\n",
    "print(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Local Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \n",
      "ToDo List 01/12/2018 (Saturnday):\n",
      "[DONE]  ++   [Task3 - NLTK Book] Chapter 0. Preface\n",
      "[DONE]  ++   [Task3 - NLTK Book] Chapter 1.Language Processing and Python\n",
      "[DONE]  ++   [Task3 - NLTK Book] Chapter 1.Language Processing and Python - Exercises\n",
      "[DONE]  +++  [Task4 - Google ML Crash Course] Prerequisites and Prework\n",
      "[DONE]  +++  [Task4 - Google ML Crash Course] Introduction to Machine Learning\n",
      "[DONE]  ++   [Task5 - Tensor Flow by Udemy] Section 1. Introduction\n",
      "[DONE]  ++   [Task12 - MIT 6.5191 - Intro to Deep Learning] Lecture 1 - Introduction to Deep Learning\n",
      "\n",
      "                                [---- EVENT TODAY: New Year ----]                                \n",
      "<\n",
      "\n",
      "_____________________________________________ Week 1 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 02/12/2018 (Sunday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 03/12/2018 (Monday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 04/12/2018 (Tuesday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 05/12/2018 (Wednesday):\n",
      "[DONE]  ++   [Task1 - Penguin Russian Course] Chapter 4. Doing Things - Verbs; Personal Pronouns\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 06/12/2018 (Thursday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 07/12/2018 (Friday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 08/12/2018 (Saturnday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "_____________________________________________ Week 2 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 09/12/2018 (Sunday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 10/12/2018 (Monday):\n",
      "[DONE]  +++  [Task4 - Google ML Crash Course] Descending into ML\n",
      "[DONE]  ++   [Task5 - Tensor Flow by Udemy] Section 2. Installation and Setup\n",
      "[DONE]  +++  [Task6 - Artificial Intelligence] Chapter 1. Introduction\n",
      "[DONE]  +++  [Task10 - MIT 6.096 - Introduction to C++] Lecture 1\n",
      "[DONE]  ++   [Task12 - MIT 6.5191 - Intro to Deep Learning] Lecture 2 - Deep Sequence Modeling\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 11/12/2018 (Tuesday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 12/12/2018 (Wednesday):\n",
      "[DONE]  ++++ [Task2 - Speech and Language Processing] Chapter 1. Introduction\n",
      "[DONE]  ++   [Task3 - NLTK Book] Chapter 2. Accessing Text Corpora and Lexical Resources\n",
      "[DONE]  +++  [Task10 - MIT 6.096 - Introduction to C++] Lecture 2\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 13/12/2018 (Thursday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 14/12/2018 (Friday):\n",
      "[DONE]  +++  [Task6 - Artificial Intelligence] Chapter 1. Introduction - Exercises\n",
      "[DONE]  +++  [Task10 - MIT 6.096 - Introduction to C++] Problem Set 1\n",
      "[DONE]  +++  [Task13 - Introdução à Linguistica I] Capítulo 1. Linguagem, Língua, Linguística\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 15/12/2018 (Saturnday):\n",
      "[DONE]  ++++ [Task2 - Speech and Language Processing] Chapter 1. Introduction - Exercises\n",
      "[DONE]  +++  [Task4 - Google ML Crash Course] Reducing Loss\n",
      "[DONE]  +++  [Task7 - Principles of Neuroscience] Chapter 1\n",
      "<\n",
      "\n",
      "_____________________________________________ Week 3 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 16/12/2018 (Sunday):\n",
      "[DONE]  +++  [Task10 - MIT 6.096 - Introduction to C++] Lecture 3\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 17/12/2018 (Monday):\n",
      "[DONE]  ++   [Task11 - MIT 6.034 - AI] Introduction and scope\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 18/12/2018 (Tuesday):\n",
      "[DONE]  ++++ [Task2 - Speech and Language Processing] Chapter 2. Regular Expressions and Automata\n",
      "[DONE]  ++++ [Task15 - NN and DL] Chapter 1. Using neural nets to recognize handwritten digits\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 19/12/2018 (Wednesday):\n",
      "[DONE]  ++   [Task5 - Tensor Flow by Udemy] Section 3. What is Machine Learning?\n",
      "[DONE]  ++   [Task12 - MIT 6.5191 - Intro to Deep Learning] Lecture 3 - Deep Computer Vision\n",
      "\n",
      "                                  [---- EVENT TODAY: Beach ----]                                  \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 20/12/2018 (Thursday):\n",
      "[DONE]  +++  [Task14 - Introducao a Linguistica II] Capítulo 1. Fonética\n",
      "[DONE]  ++   [Task18 - Books 2019] Book 1 - A Morte de Stalin\n",
      "\n",
      "                                  [---- EVENT TODAY: Beach ----]                                  \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 21/12/2018 (Friday):\n",
      "[DONE]  ++++ [Task2 - Speech and Language Processing] Chapter 2. Regular Expressions and Automata - Exercises\n",
      "\n",
      "                                  [---- EVENT TODAY: Beach ----]                                  \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 22/12/2018 (Saturnday):\n",
      "[LATE!] +++  [Task13 - Introdução à Linguistica I] Capítulo 1. Linguagem, Língua, Linguística - Exercícios\n",
      "<\n",
      "\n",
      "_____________________________________________ Week 4 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 23/12/2018 (Sunday):\n",
      "[DONE]  +++  [Task10 - MIT 6.096 - Introduction to C++] Lecture 4\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 24/12/2018 (Monday):\n",
      "[DONE]  ++++ [Task2 - Speech and Language Processing] Chapter 3. N-Grams\n",
      "[DONE]  +++  [Task7 - Principles of Neuroscience] Chapter 2\n",
      "[DONE]  ++   [Task11 - MIT 6.034 - AI] Reasoning: goal trees and problem solving\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 25/12/2018 (Tuesday):\n",
      "[DONE]  +    [Task9 - Take Off in German] Chapter 1\n",
      "\n",
      "                                [---- EVENT TODAY: Christmas ----]                                \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 26/12/2018 (Wednesday):\n",
      "[DONE]  +++  [Task4 - Google ML Crash Course] First Steps with TF\n",
      "[DONE]  +++  [Task10 - MIT 6.096 - Introduction to C++] Lecture 5\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 27/12/2018 (Thursday):\n",
      "[LATE!] ++++ [Task2 - Speech and Language Processing] Chapter 3. N-Grams - Exercises\n",
      "[DONE]  +++  [Task14 - Introducao a Linguistica II] Capítulo 1. Fonética - Exercícios\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 28/12/2018 (Friday):\n",
      "[DONE]  ++   [Task5 - Tensor Flow by Udemy] Section 4. Crash Course Overview\n",
      "[DONE]  +++  [Task6 - Artificial Intelligence] Chapter 2. Intelligent Agents\n",
      "[DONE]  ++   [Task12 - MIT 6.5191 - Intro to Deep Learning] Lecture 4 - Deep Generative Models\n",
      "[DONE]  +++  [Task13 - Introdução à Linguistica I] Capítulo 2. A Comunicação Humana\n",
      "[DONE]  ++++ [Task15 - NN and DL] Chapter 2. How the backpropagation algorithm works, Part 1\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 29/12/2018 (Saturnday):\n",
      "[LATE!] +++  [Task6 - Artificial Intelligence] Chapter 2. Intelligent Agents - Exercises\n",
      "\n",
      "                                 [---- EVENT TODAY: Chacara ----]                                 \n",
      "<\n",
      "\n",
      "_____________________________________________ Week 5 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 30/12/2018 (Sunday):\n",
      "[DONE]  ++   [Task1 - Penguin Russian Course] Chapter 5. Asking Questions; The Prepositional Case\n",
      "[DONE]  ++   [Task1 - Penguin Russian Course] Chapter 6. Possession; Going Places; The Accusative\n",
      "[DONE]  ++++ [Task2 - Speech and Language Processing] Chapter 4. Naive Bayes Classification and Sentiment\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 31/12/2018 (Monday):\n",
      "[LATE!] +++  [Task4 - Google ML Crash Course] Generalization\n",
      "[DONE]  ++   [Task11 - MIT 6.034 - AI] Reasoning: goal trees and rule-based expert systems\n",
      "[LATE!] ++   [Task18 - Books 2019] Book 2 - O Demônio do Meio-Dia\n",
      "\n",
      "                               [---- EVENT TODAY: End of Year ----]                               \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 01/01/2019 (Tuesday):\n",
      "[DONE]  +++  [Task10 - MIT 6.096 - Introduction to C++] Lecture 6\n",
      "[DONE]  +++  [Task17 - At Least one Paper per Week] Paper 1 - Deep Learning\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 02/01/2019 (Wednesday):\n",
      "[DONE]  ++++ [Task2 - Speech and Language Processing] Chapter 4. Naive Bayes Classification and Sentiment - Exercises\n",
      "[DONE]  ++   [Task19 - Udemy Excel] Section 1 - Introduction\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 03/01/2019 (Thursday):\n",
      "[LATE!] ++   [Task3 - NLTK Book] Chapter 3. Processing Raw Text\n",
      "[DONE]  +++  [Task10 - MIT 6.096 - Introduction to C++] Problem Set 2\n",
      "[LATE!] +++  [Task14 - Introducao a Linguistica II] Capítulo 2. Fonologia\n",
      "[LATE!] ++++ [Task15 - NN and DL] Chapter 2. How the backpropagation algorithm works, Part 2\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 04/01/2019 (Friday):\n",
      "[LATE!] +++  [Task4 - Google ML Crash Course] Training and Test Sets\n",
      "[LATE!] +++  [Task6 - Artificial Intelligence] Chapter 3. Resolution of Problems by Means of Searching\n",
      "[DONE]  +++  [Task10 - MIT 6.096 - Introduction to C++] http://www.cplusplus.com/doc/tutorial/pointers/\n",
      "[LATE!] +++  [Task10 - MIT 6.096 - Introduction to C++] Lecture 7\n",
      "[LATE!] +++  [Task13 - Introdução à Linguistica I] Capítulo 2. A Comunicação Humana - Exercícios\n",
      "[DONE]  ++   [Task19 - Udemy Excel] Section 2 - Gettting off from zero\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 05/01/2019 (Saturnday):\n",
      "[DONE]  ++++ [Task2 - Speech and Language Processing] Chapter 5. Logistic Regression\n",
      "        ++++ [Task15 - NN and DL] Chapter 3. Improving the way neural networks learn, Part 1\n",
      "<\n",
      "\n",
      "_____________________________________________ Week 6 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 06/01/2019 (Sunday):\n",
      "[DONE]  ++   [Task5 - Tensor Flow by Udemy] Section 5. Introduction to Neural Networks\n",
      "        ++   [Task12 - MIT 6.5191 - Intro to Deep Learning] Lecture 5 - Deep Reinforcement Learning\n",
      "        ++++ [Task15 - NN and DL] Chapter 3. Improving the way neural networks learn, Part 2\n",
      "        ++   [Task19 - Udemy Excel] Section 3 - Fundaments\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 1 - Preliminary Algebra\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 07/01/2019 (Monday):\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 4\n",
      "        +++  [Task10 - MIT 6.096 - Introduction to C++] Lecture 8\n",
      "        ++   [Task11 - MIT 6.034 - AI] Mega-Recitaiton 1: Rule-Based Systems\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 2 - Computing Machinery and Intelligency\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 08/01/2019 (Tuesday):\n",
      "[DONE]  ++   [Task1 - Penguin Russian Course] Chapter 7. Describing Things: Adjectives\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 5. Logistic Regression - Exercises\n",
      "        ++   [Task3 - NLTK Book] Chapter 3. Processing Raw Text - Exercises\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 5\n",
      "[DONE]  +    [Task9 - Take Off in German] Chapter 1 - Test\n",
      "        ++   [Task19 - Udemy Excel] Section 4 - Important Tools\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 09/01/2019 (Wednesday):\n",
      "        +++  [Task4 - Google ML Crash Course] Validation\n",
      "        ++   [Task18 - Books 2019] Book 3 - Harry Potter and the Sorcerer's Stone\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 10/01/2019 (Thursday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 3. Resolution of Problems by Means of Searching - Exercises\n",
      "        +++  [Task10 - MIT 6.096 - Introduction to C++] Problem Set 3\n",
      "        +++  [Task14 - Introducao a Linguistica II] Capítulo 2. Fonologia - Exercícios\n",
      "        ++   [Task19 - Udemy Excel] Section 5 - Learning More\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 11/01/2019 (Friday):\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 6. Vector Semantics\n",
      "[DONE]  +++  [Task13 - Introdução à Linguistica I] Capítulo 3. Teoria dos Signos\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 1 - Preliminary Algebra Exercises\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 12/01/2019 (Saturnday):\n",
      "        +++  [Task8 - Noções de Probabilidade e Estatística] Capítulo 1. Introdução à Análise Exploratória de Dados\n",
      "        ++   [Task19 - Udemy Excel] Section 6 - Accelerating\n",
      "<\n",
      "\n",
      "_____________________________________________ Week 7 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 13/01/2019 (Sunday):\n",
      "        ++   [Task3 - NLTK Book] Chapter 4. Writing Structured Programs\n",
      "        +++  [Task10 - MIT 6.096 - Introduction to C++] Lecture 9\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 3 - Brain-Machine Interfaces: From Basic Science to Neuroprotheses and Neuroheabilitation\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 14/01/2019 (Monday):\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 6. Vector Semantics - Exercises\n",
      "        +++  [Task4 - Google ML Crash Course] Representation\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 6\n",
      "[DONE]  ++   [Task11 - MIT 6.034 - AI] Search: depth-first, hill climbing, beam\n",
      "        ++++ [Task15 - NN and DL] Chapter 4. A visual proof that neural nets can compute any function, Part 1\n",
      "        ++   [Task19 - Udemy Excel] Section 7 - More Tools\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 15/01/2019 (Tuesday):\n",
      "        ++   [Task5 - Tensor Flow by Udemy] Section 6. TensorFlow Basics\n",
      "        +++  [Task8 - Noções de Probabilidade e Estatística] Capítulo 1. Introdução à Análise Exploratória de Dados - Exercises\n",
      "        ++   [Task12 - MIT 6.5191 - Intro to Deep Learning] Lecture 6 - Limitations and New Frontiers\n",
      "        ++++ [Task15 - NN and DL] Chapter 4. A visual proof that neural nets can compute any function, Part 2\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 16/01/2019 (Wednesday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 4. Beyond Classic Search\n",
      "        +++  [Task10 - MIT 6.096 - Introduction to C++] Lecture 10\n",
      "        ++   [Task19 - Udemy Excel] Section 8 - User Orientation\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 2 - Preliminary Calculus\n",
      "\n",
      "                              [---- EVENT TODAY: AASDAP start ----]                              \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 17/01/2019 (Thursday):\n",
      "        ++   [Task1 - Penguin Russian Course] Chapter 8. Plurals; Spelling Rules; Buying Things\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 7. Neural Nets and Neural Language Models\n",
      "        +++  [Task14 - Introducao a Linguistica II] Capítulo 3. Morfologia\n",
      "        ++   [Task16 - AASDAP Papers] Aurelie's Master - Part 1/3\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 18/01/2019 (Friday):\n",
      "        ++   [Task3 - NLTK Book] Chapter 4. Writing Structured Programs - Exercises\n",
      "        +++  [Task8 - Noções de Probabilidade e Estatística] Capítulo 2. Probabilidades\n",
      "        +++  [Task13 - Introdução à Linguistica I] Capítulo 3. Teoria dos Signos - Exercícios\n",
      "        ++   [Task19 - Udemy Excel] Section 9 - Advanced Formulas\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 19/01/2019 (Saturnday):\n",
      "        +++  [Task4 - Google ML Crash Course] Feature Crosses\n",
      "        +++  [Task10 - MIT 6.096 - Introduction to C++] Problem Set 4\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 4 - Towards AI-Complete Question Answering\n",
      "        ++   [Task18 - Books 2019] Book 4 - Harry Potter and the Chamber of Secrets\n",
      "<\n",
      "\n",
      "_____________________________________________ Week 8 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 20/01/2019 (Sunday):\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 7. Neural Nets and Neural Language Models - Exercises\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 7\n",
      "        ++   [Task19 - Udemy Excel] Section 10 - Dynamic Tools\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 21/01/2019 (Monday):\n",
      "        +++  [Task8 - Noções de Probabilidade e Estatística] Capítulo 2.  Probabilidades - Exercises\n",
      "        ++   [Task11 - MIT 6.034 - AI] Problem set 0 due\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 2 - Preliminary Calculus Exercises\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 22/01/2019 (Tuesday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 4. Beyond Classic Search - Exercises\n",
      "        +    [Task9 - Take Off in German] Chapter 2\n",
      "        ++   [Task19 - Udemy Excel] Section 11\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 23/01/2019 (Wednesday):\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 8. Part-of-Speech Tagging\n",
      "        ++   [Task3 - NLTK Book] Chapter 5. Categorizing and Tagging Words\n",
      "        ++++ [Task15 - NN and DL] Chapter 5. Why are deep neural networks hard to train?, Part 1\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 24/01/2019 (Thursday):\n",
      "        +++  [Task4 - Google ML Crash Course] Regularization: Simplicity\n",
      "        ++   [Task5 - Tensor Flow by Udemy] Section 7. Convolutional Neural Networks\n",
      "        +++  [Task8 - Noções de Probabilidade e Estatística] Capítulo 3. Variáveis Aleatórias Discretas\n",
      "        ++   [Task12 - MIT 6.5191 - Intro to Deep Learning] Lecture 7 - Issues in Image Classification (Google Guest)\n",
      "        +++  [Task14 - Introducao a Linguistica II] Capítulo 3. Morfologia - Exercícios\n",
      "        ++++ [Task15 - NN and DL] Chapter 5. Why are deep neural networks hard to train?, Part 2\n",
      "        ++   [Task19 - Udemy Excel] Section 12\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 25/01/2019 (Friday):\n",
      "[DONE]  +++  [Task13 - Introdução à Linguistica I] Capítulo 4. A Língua como Objeto da Linguística\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 5\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 26/01/2019 (Saturnday):\n",
      "        ++   [Task1 - Penguin Russian Course] Chapter 9. Numbers; The Genitive Case\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 8. Part-of-Speech Tagging - Exercises\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 8\n",
      "        ++   [Task19 - Udemy Excel] Section 13\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 3 - Complex Numbers and Hyperbolic Functions\n",
      "<\n",
      "\n",
      "_____________________________________________ Week 9 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 27/01/2019 (Sunday):\n",
      "        +++  [Task8 - Noções de Probabilidade e Estatística] Capítulo 3. Variáveis Aleatórias Discretas - Exercises\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 28/01/2019 (Monday):\n",
      "        ++   [Task3 - NLTK Book] Chapter 5. Categorizing and Tagging Words - Exercises\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 5. Competitive Search\n",
      "        ++   [Task11 - MIT 6.034 - AI] Search: optimal, branch and bound, A*\n",
      "        ++   [Task19 - Udemy Excel] Section 14\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 29/01/2019 (Tuesday):\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 9. Sequence Processing with Recurrent Networks\n",
      "        +++  [Task4 - Google ML Crash Course] Logistic Regression\n",
      "        ++   [Task18 - Books 2019] Book 5 - Harry Potter and the Prisoner of Azkaban\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 30/01/2019 (Wednesday):\n",
      "        +++  [Task8 - Noções de Probabilidade e Estatística] Capítulo 4. Medidas Resumo\n",
      "        ++   [Task16 - AASDAP Papers] Aurelie's Master - Part 2/3\n",
      "        ++   [Task19 - Udemy Excel] Section 15\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 31/01/2019 (Thursday):\n",
      "        +++  [Task14 - Introducao a Linguistica II] Capítulo 4. Sintaxe: Explorando a Estrutura da Sentença\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 6\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 3 - Complex Numbers and Hyperbolic Functions Exercises\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 01/02/2019 (Friday):\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 9. Sequence Processing with Recurrent Networks - Exercises\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 9\n",
      "        +++  [Task13 - Introdução à Linguistica I] Capítulo 4. A Língua como Objeto da Linguística - Exercícios\n",
      "        ++++ [Task15 - NN and DL] Chapter 6. Deep learning, Part 1\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 02/02/2019 (Saturnday):\n",
      "        ++   [Task3 - NLTK Book] Chapter 6. Learning to Classify Text\n",
      "        ++   [Task5 - Tensor Flow by Udemy] Section 8. Recurrent Neural Networks\n",
      "        +++  [Task8 - Noções de Probabilidade e Estatística] Capítulo 4. Medidas Resumo - Exercises\n",
      "        ++   [Task12 - MIT 6.5191 - Intro to Deep Learning] Lecture 8 - Faster ML Development with Tensorflow (Google Guest)\n",
      "        ++++ [Task15 - NN and DL] Chapter 6. Deep learning, Part 2\n",
      "<\n",
      "\n",
      "_____________________________________________ Week 10 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 03/02/2019 (Sunday):\n",
      "        +++  [Task4 - Google ML Crash Course] Classification\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 5. Competitive Search - Exercises\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 04/02/2019 (Monday):\n",
      "        ++   [Task1 - Penguin Russian Course] Chapter 10. 'To Have'; More on the Genitive\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 10. Formal Grammars of English\n",
      "        ++   [Task11 - MIT 6.034 - AI] Search: games, minimax, and alpha-beta\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 05/02/2019 (Tuesday):\n",
      "        +++  [Task8 - Noções de Probabilidade e Estatística] Capítulo 5. Variáveis Bidimensionais\n",
      "        +    [Task9 - Take Off in German] Chapter 2 - Test\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 4 - Series and Limits\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 06/02/2019 (Wednesday):\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 7\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 07/02/2019 (Thursday):\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 10. Formal Grammars of English - Exercises\n",
      "        ++   [Task3 - NLTK Book] Chapter 6. Learning to Classify Text - Exercises\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 10\n",
      "        +++  [Task14 - Introducao a Linguistica II] Capítulo 4. Sintaxe: Explorando a Estrutura da Sentença - Exercícios\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 08/02/2019 (Friday):\n",
      "        +++  [Task4 - Google ML Crash Course] Regularisation: Sparsity\n",
      "        +++  [Task8 - Noções de Probabilidade e Estatística] Capítulo 5. Variáveis Bidimensionais - Exercises\n",
      "[DONE]  +++  [Task13 - Introdução à Linguistica I] Capítulo 5. A Competência Linguística\n",
      "        ++   [Task18 - Books 2019] Book 6 - Harry Potter and the Goblet of Fire\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 09/02/2019 (Saturnday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 6. Problems of Restriction Satisfaction\n",
      "<\n",
      "\n",
      "_____________________________________________ Week 11 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 10/02/2019 (Sunday):\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 11. Syntactic Parsing\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 4 - Series and Limits Exercises\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 11/02/2019 (Monday):\n",
      "        ++   [Task5 - Tensor Flow by Udemy] Section 9. Miscellaneous Topics\n",
      "        +++  [Task8 - Noções de Probabilidade e Estatística] Capítulo 6. Variáveis Aleatórias Contínuas\n",
      "        ++   [Task11 - MIT 6.034 - AI] Problem set 1 due\n",
      "        ++   [Task12 - MIT 6.5191 - Intro to Deep Learning] Lecture 9 - Deep Learning, A Personal Perspective (NVIDIA Guest)\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 12/02/2019 (Tuesday):\n",
      "        ++   [Task3 - NLTK Book] Chapter 7. Extracting Information from Text\n",
      "        ++   [Task16 - AASDAP Papers] Aurelie's Master - Part 3/3\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 8\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 13/02/2019 (Wednesday):\n",
      "        ++   [Task1 - Penguin Russian Course] Chapter 11. The Past; Reflexive Verbs\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 11. Syntactic Parsing - Exercises\n",
      "        +++  [Task4 - Google ML Crash Course] Introduction to Neural Nets\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 11\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 14/02/2019 (Thursday):\n",
      "        +++  [Task8 - Noções de Probabilidade e Estatística] Capítulo 6. Variáveis Aleatórias Contínuas - Exercises\n",
      "        +++  [Task14 - Introducao a Linguistica II] Capítulo 5. Semântica Lexical\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 15/02/2019 (Friday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 6. Problems of Restriction Satisfaction - Exercises\n",
      "        +++  [Task13 - Introdução à Linguistica I] Capítulo 5. A Competência Linguística - Exercícios\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 5 - Partial Differentiation\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 16/02/2019 (Saturnday):\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 12. Statistical Parsing\n",
      "<\n",
      "\n",
      "_____________________________________________ Week 12 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 17/02/2019 (Sunday):\n",
      "        ++   [Task3 - NLTK Book] Chapter 7. Extracting Information from Text - Exercises\n",
      "        +++  [Task8 - Noções de Probabilidade e Estatística] Capítulo 7. Inferência Estatística - Estimação\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 18/02/2019 (Monday):\n",
      "        +++  [Task4 - Google ML Crash Course] Training Neural Nets\n",
      "        ++   [Task11 - MIT 6.034 - AI] Mega-Recitaiton 2: Basic Search, Optimal Search\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 9\n",
      "        ++   [Task18 - Books 2019] Book 7 - Harry Potter and the Order of the Phoenix\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 19/02/2019 (Tuesday):\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 12. Statistical Parsing - Exercises\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 12\n",
      "        +    [Task9 - Take Off in German] Chapter 3\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 20/02/2019 (Wednesday):\n",
      "        ++   [Task5 - Tensor Flow by Udemy] Section 10. AutoEncoders\n",
      "        +++  [Task8 - Noções de Probabilidade e Estatística] Capítulo 7. Inferência Estatística - Estimação - Exercises\n",
      "        ++   [Task12 - MIT 6.5191 - Intro to Deep Learning] Lecture 10 - Beyond Deep Learning (IBM Guest)\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 5 - Partial Differentiation Exercises\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 21/02/2019 (Thursday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 7. Logic Agents\n",
      "        +++  [Task14 - Introducao a Linguistica II] Capítulo 5. Semântica Lexical - Exercícios\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 22/02/2019 (Friday):\n",
      "        ++   [Task1 - Penguin Russian Course] Chapter 12. The Future; Aspect. The Dative Case\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 13. Dependency Parsing\n",
      "        ++   [Task3 - NLTK Book] Chapter 8. Analysing Sentence Structure\n",
      "[DONE]  +++  [Task13 - Introdução à Linguistica I] Capítulo 6. A Variação Linguística\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 23/02/2019 (Saturnday):\n",
      "        +++  [Task4 - Google ML Crash Course] Multi-Class Neural Nets\n",
      "        +++  [Task8 - Noções de Probabilidade e Estatística] Capítulo 8. Inferência Estatística - Testes de Hipóteses\n",
      "<\n",
      "\n",
      "_____________________________________________ Week 13 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 24/02/2019 (Sunday):\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 10\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 25/02/2019 (Monday):\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 13. Dependency Parsing - Exercises\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 13\n",
      "        ++   [Task11 - MIT 6.034 - AI] Quiz 1\n",
      "        ++   [Task16 - AASDAP Papers] (1999) Real Time Control of a Robot Arm Using Simultaneously Recorded Neurons in the Motor Cortex - K C John et al\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 6 - Multiple Integrals\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 26/02/2019 (Tuesday):\n",
      "        +++  [Task8 - Noções de Probabilidade e Estatística] Capítulo 8. Inferência Estatística - Testes de Hipóteses - Exercises\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 27/02/2019 (Wednesday):\n",
      "        ++   [Task3 - NLTK Book] Chapter 8. Analysing Sentence Structure - Exercises\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 7. Logic Agents - Exercises\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 28/02/2019 (Thursday):\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 14. The Representation of Sentence Meaning\n",
      "        +++  [Task4 - Google ML Crash Course] Embeddings\n",
      "        +++  [Task14 - Introducao a Linguistica II] Capítulo 6. Semântica Formal\n",
      "        ++   [Task18 - Books 2019] Book 8 - Harry Potter and the Half-Blood Prince\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 01/03/2019 (Friday):\n",
      "        ++   [Task5 - Tensor Flow by Udemy] Section 11. Reinforcement Learning with OpenAI Gym\n",
      "        +++  [Task8 - Noções de Probabilidade e Estatística] Capítulo 9. Tópicos Especiais\n",
      "        ++   [Task12 - MIT 6.5191 - Intro to Deep Learning] Lecture 11 - Computer Vision Meets Social Networks (Tencent Guest)\n",
      "        +++  [Task13 - Introdução à Linguistica I] Capítulo 6. A Variação Linguística - Exercícios\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 02/03/2019 (Saturnday):\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 11\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 6 - Multiple Integrals Exercises\n",
      "<\n",
      "\n",
      "_____________________________________________ Week 14 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 03/03/2019 (Sunday):\n",
      "        ++   [Task1 - Penguin Russian Course] Chapter 13. Aspect in the Past; Use of Tenses\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 14. The Representation of Sentence Meaning - Exercises\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 14\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 04/03/2019 (Monday):\n",
      "        ++   [Task3 - NLTK Book] Chapter 9. Building Feature Based Grammars\n",
      "        +++  [Task8 - Noções de Probabilidade e Estatística] Capítulo 9. Tópicos Especiais - Exercises\n",
      "        ++   [Task11 - MIT 6.034 - AI] Constraints: interpreting line drawings\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 05/03/2019 (Tuesday):\n",
      "        +++  [Task4 - Google ML Crash Course] Production ML Systems\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 8. First Order Logic\n",
      "        +    [Task9 - Take Off in German] Chapter 3 - Test\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 06/03/2019 (Wednesday):\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 15. Computational Semantics\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 07/03/2019 (Thursday):\n",
      "        +++  [Task14 - Introducao a Linguistica II] Capítulo 6. Semântica Formal - Exercícios\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 7 - Vector Algebra\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 08/03/2019 (Friday):\n",
      "        +++  [Task13 - Introdução à Linguistica I] Capítulo 7. A Mudança Linguística\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 12\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 09/03/2019 (Saturnday):\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 15. Computational Semantics - Exercises\n",
      "        ++   [Task3 - NLTK Book] Chapter 9. Building Feature Based Grammars - Exercises\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 15\n",
      "<\n",
      "\n",
      "_____________________________________________ Week 15 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 10/03/2019 (Sunday):\n",
      "        +++  [Task4 - Google ML Crash Course] Static vs. Dynamic Training\n",
      "        ++   [Task5 - Tensor Flow by Udemy] Section 12. GAN - Generative Adversarial Networks\n",
      "        ++   [Task16 - AASDAP Papers] (2006) - Brain Machine Interfaces _ Past, Present and Future - A L Mikhail, A L N Miguel\n",
      "        ++   [Task18 - Books 2019] Book 9 - Harry Potter and the Deathly Hallows\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 11/03/2019 (Monday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 8. First Order Logic - Exercises\n",
      "        ++   [Task11 - MIT 6.034 - AI] Constraints: search, domain reduction\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 12/03/2019 (Tuesday):\n",
      "        ++   [Task1 - Penguin Russian Course] Chapter 14. Aspect in the Future; Impersonal Contructions\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 16. Semantic Parsing\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 7 - Vector Algebra  Exercises\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 13/03/2019 (Wednesday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 14/03/2019 (Thursday):\n",
      "        ++   [Task3 - NLTK Book] Chapter 10. Analysing the Meaning of Sentences\n",
      "        +++  [Task14 - Introducao a Linguistica II] Capítulo 7. Pragmática\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 13\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 15/03/2019 (Friday):\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 16. Semantic Parsing - Exercises\n",
      "        +++  [Task4 - Google ML Crash Course] Static vs. Dynamic Inference\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 16\n",
      "        +++  [Task13 - Introdução à Linguistica I] Capítulo 7. A Mudança Linguística - Exercícios\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 16/03/2019 (Saturnday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "_____________________________________________ Week 16 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 17/03/2019 (Sunday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 9. Inference in First Order Logic\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 8 - Matrices and Vector Spaces\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 18/03/2019 (Monday):\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 17. Information Extraction\n",
      "        ++   [Task11 - MIT 6.034 - AI] Constraints: visual object recognition\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 19/03/2019 (Tuesday):\n",
      "        ++   [Task3 - NLTK Book] Chapter 10. Analysing the Meaning of Sentences - Exercises\n",
      "        +    [Task9 - Take Off in German] Chapter 4\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 20/03/2019 (Wednesday):\n",
      "        +++  [Task4 - Google ML Crash Course] Data Dependencies\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 14\n",
      "        ++   [Task18 - Books 2019] Book 10 - Sobrevivendo no Inferno - MCs Racionais\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 21/03/2019 (Thursday):\n",
      "        ++   [Task1 - Penguin Russian Course] Chapter 15. Requests and the Imperative\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 17. Information Extraction - Exercises\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 17\n",
      "        +++  [Task14 - Introducao a Linguistica II] Capítulo 7. Pragmática - Exercícios\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 22/03/2019 (Friday):\n",
      "        +++  [Task13 - Introdução à Linguistica I] Capítulo 8. A Linguagem em Uso\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 8 - Matrices and Vector Spaces Exercises\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 23/03/2019 (Saturnday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 9. Inference in First Order Logic - Exercises\n",
      "        ++   [Task16 - AASDAP Papers] (2010) A Novel Verticalized Reducation Device for Spinal Cord Injuries _ The WalkTrainer, from Design to Clinical Trials - Y Stauffer et al\n",
      "<\n",
      "\n",
      "_____________________________________________ Week 17 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 24/03/2019 (Sunday):\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 18. Semantic Role Labeling and Arguments Structure\n",
      "        ++   [Task3 - NLTK Book] Chapter 11. Managing Linguistic Data\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 25/03/2019 (Monday):\n",
      "        +++  [Task4 - Google ML Crash Course] Fairness\n",
      "        ++   [Task11 - MIT 6.034 - AI] Mega-Recitaiton 3: Games, Minimax, Alpha-Beta\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 26/03/2019 (Tuesday):\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 15\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 27/03/2019 (Wednesday):\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 18. Semantic Role Labeling and Arguments Structure - Exercises\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 18\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 9 - Normal Modes\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 28/03/2019 (Thursday):\n",
      "        +++  [Task14 - Introducao a Linguistica II] Capítulo 8. Estudos do Discurso\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 29/03/2019 (Friday):\n",
      "        ++   [Task3 - NLTK Book] Chapter 11. Managing Linguistic Data - Exercises\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 10. Classic Planning\n",
      "        +++  [Task13 - Introdução à Linguistica I] Capítulo 8. A Linguagem em Uso - Exercícios\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 30/03/2019 (Saturnday):\n",
      "        ++   [Task1 - Penguin Russian Course] Chapter 16. The Instrumental Case\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 19. Lexicon for Sentiment, Affect and Connotation\n",
      "        ++   [Task18 - Books 2019] Book 11 - Sapiens - Yuval Noah Harari\n",
      "<\n",
      "\n",
      "_____________________________________________ Week 18 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 31/03/2019 (Sunday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 01/04/2019 (Monday):\n",
      "        ++   [Task11 - MIT 6.034 - AI] Problem set 2 due\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 16\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 9 - Normal Modes Exercises\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 02/04/2019 (Tuesday):\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 19. Lexicon for Sentiment, Affect and Connotation - Exercises\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 19\n",
      "        +    [Task9 - Take Off in German] Chapter 4 - Test\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 03/04/2019 (Wednesday):\n",
      "        ++   [Task3 - NLTK Book] Chapter 12. Afterword Facing the Language Challenge\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 04/04/2019 (Thursday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 10. Classic Planning - Exercises\n",
      "        +++  [Task14 - Introducao a Linguistica II] Capítulo 8. Estudos do Discurso - Exercícios\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 05/04/2019 (Friday):\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 20. Coreference Resolution and Entity Linking\n",
      "        +++  [Task13 - Introdução à Linguistica I] Capítulo 9. A Abordagem do Texto\n",
      "        ++   [Task16 - AASDAP Papers] (2011) Future Developments in Brain Machine Interface Research - A L Mikhail\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 06/04/2019 (Saturnday):\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 10 - Vector Calculus\n",
      "<\n",
      "\n",
      "_____________________________________________ Week 19 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 07/04/2019 (Sunday):\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 17\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 08/04/2019 (Monday):\n",
      "        ++   [Task1 - Penguin Russian Course] Chapter 17. Time, Date, Age; Ordinal Numbers\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 20. Coreference Resolution and Entity Linking - Exercises\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 20\n",
      "        ++   [Task11 - MIT 6.034 - AI] Introduction to learning, nearest neighbors\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 09/04/2019 (Tuesday):\n",
      "        ++   [Task18 - Books 2019] Book 12 - A Mulher do Oficial Nazista\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 10/04/2019 (Wednesday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 11. Planning and Action in the Real World\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 11/04/2019 (Thursday):\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 21. Discourse Coherence\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 10 - Vector Calculus Exercises\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 12/04/2019 (Friday):\n",
      "        +++  [Task13 - Introdução à Linguistica I] Capítulo 9. A Abordagem do Texto - Exercícios\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 13/04/2019 (Saturnday):\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 18\n",
      "<\n",
      "\n",
      "_____________________________________________ Week 20 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 14/04/2019 (Sunday):\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 21. Discourse Coherence - Exercises\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 21\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 15/04/2019 (Monday):\n",
      "        ++   [Task11 - MIT 6.034 - AI] Learning: identification trees, disorder\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 16/04/2019 (Tuesday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 11. Planning and Action in the Real World - Exercises\n",
      "        +    [Task9 - Take Off in German] Chapter 5\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 11 - Line, Surface and Volume Integrals\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 17/04/2019 (Wednesday):\n",
      "        ++   [Task1 - Penguin Russian Course] Chapter 18. The Comparative; Suprlatives; Relative Clauses\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 22. Machine Translation\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 18/04/2019 (Thursday):\n",
      "        ++   [Task16 - AASDAP Papers] (2013) A Brain Machine Interface Enables Bimanual Arm Movemtnts in Monkeys - J I Peter et al\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 19/04/2019 (Friday):\n",
      "        +++  [Task13 - Introdução à Linguistica I] Capítulo 10. A Aquisição da Linguagem\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 19\n",
      "        ++   [Task18 - Books 2019] Book 13 - Contos Completos - Fernando Pessoa\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 20/04/2019 (Saturnday):\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 22. Machine Translation - Exercises\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 22\n",
      "<\n",
      "\n",
      "_____________________________________________ Week 21 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 21/04/2019 (Sunday):\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 11 - Line, Surface and Volume Integrals Exercises\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 22/04/2019 (Monday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 12. Knowledge Representation\n",
      "        ++   [Task11 - MIT 6.034 - AI] Quiz 2\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 23/04/2019 (Tuesday):\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 23. Question Answering\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 24/04/2019 (Wednesday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 25/04/2019 (Thursday):\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 20\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 26/04/2019 (Friday):\n",
      "        ++   [Task1 - Penguin Russian Course] Chapter 19. The Conditional; Obligation; Prefixes\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 23. Question Answering - Exercises\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 23\n",
      "        +++  [Task13 - Introdução à Linguistica I] Capítulo 10. A Aquisição da Linguagem - Exercícios\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 12 - Fourier Series\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 27/04/2019 (Saturnday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "_____________________________________________ Week 22 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 28/04/2019 (Sunday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 12. Knowledge Representation - Exercises\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 29/04/2019 (Monday):\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 24. Dialog Systems and Chatbots\n",
      "        ++   [Task11 - MIT 6.034 - AI] Learning: neural nets, back propagation\n",
      "        ++   [Task18 - Books 2019] Book 14 - The Power - Naomi Alderman\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 30/04/2019 (Tuesday):\n",
      "        +    [Task9 - Take Off in German] Chapter 5 - Test\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 01/05/2019 (Wednesday):\n",
      "        ++   [Task16 - AASDAP Papers] (2016) Assimilation of Virtual Legs and Perception of Floor Texture by COmplete Paraplegic PAtients Receiving Artificial Tactile Feedback - S Solaiman et al\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 21\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 12 - Fourier Series Exercises\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 02/05/2019 (Thursday):\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 24. Dialog Systems and Chatbots - Exercises\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 24\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 03/05/2019 (Friday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 04/05/2019 (Saturnday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 13. Quantifying Uncertainty\n",
      "<\n",
      "\n",
      "_____________________________________________ Week 23 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 05/05/2019 (Sunday):\n",
      "        ++   [Task1 - Penguin Russian Course] Chapter 20. Verbs of Motion; Going, Running, Bringing\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 25. Advanced Dialog Systems\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 06/05/2019 (Monday):\n",
      "        ++   [Task11 - MIT 6.034 - AI] Mega-Recitaiton 4: Neural Nets\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 13 - Integral Transforms\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 07/05/2019 (Tuesday):\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 22\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 08/05/2019 (Wednesday):\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 25. Advanced Dialog Systems - Exercises\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 25\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 09/05/2019 (Thursday):\n",
      "        ++   [Task18 - Books 2019] Book 15 - Superintelligence - Nick Bostrom\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 10/05/2019 (Friday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 13. Quantifying Uncertainty - Exercises\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 11/05/2019 (Saturnday):\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 26. Speech Recognition and Synthesis\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 13 - Integral Transforms Exercises\n",
      "<\n",
      "\n",
      "_____________________________________________ Week 24 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 12/05/2019 (Sunday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 13/05/2019 (Monday):\n",
      "        ++   [Task11 - MIT 6.034 - AI] Problem set 3 due\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 23\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 14/05/2019 (Tuesday):\n",
      "        ++   [Task1 - Penguin Russian Course] Chapter 21. Possession\n",
      "        ++++ [Task2 - Speech and Language Processing] Chapter 26. Speech Recognition and Synthesis - Exercises\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 26\n",
      "        +    [Task9 - Take Off in German] Chapter 6\n",
      "        ++   [Task16 - AASDAP Papers] (2016) Long Trm Training with a Brain Machine Interfce Based Gait Protocol Induces Partial Neurological Recovery in Paraplegic Patients - R C D Ana\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 15/05/2019 (Wednesday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 16/05/2019 (Thursday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 14. Probabilistic Thinking\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 14 - First Order Ordinary Differential Equations\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 17/05/2019 (Friday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 18/05/2019 (Saturnday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "_____________________________________________ Week 25 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 19/05/2019 (Sunday):\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 24\n",
      "        ++   [Task18 - Books 2019] Book 16 - Sculpting in Time - Andrey Tarkovsky\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 20/05/2019 (Monday):\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 27\n",
      "        ++   [Task11 - MIT 6.034 - AI] Learning: genetic algorithms\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 21/05/2019 (Tuesday):\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 14 - First Order Ordinary Differential Equations Exercises\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 22/05/2019 (Wednesday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 14. Probabilistic Thinking - Exercises\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 23/05/2019 (Thursday):\n",
      "        ++   [Task1 - Penguin Russian Course] Chapter 22. Fun with Numbers\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 24/05/2019 (Friday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 25/05/2019 (Saturnday):\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 25\n",
      "<\n",
      "\n",
      "_____________________________________________ Week 26 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 26/05/2019 (Sunday):\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 28\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 15 - Higher-Order Ordinary Differential Equations\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 27/05/2019 (Monday):\n",
      "        ++   [Task11 - MIT 6.034 - AI] Learning: sparse spaces, phonology\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 28/05/2019 (Tuesday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 15. Temporal Probabilistic Thinking\n",
      "        +    [Task9 - Take Off in German] Chapter 6 - Test\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 29/05/2019 (Wednesday):\n",
      "        ++   [Task18 - Books 2019] Book 17 - Duas Narrativas Fantásticas - Fiodor Dostoyevsky\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 30/05/2019 (Thursday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 31/05/2019 (Friday):\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 26\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 15 - Higher-Order Ordinary Differential Equations Exercises\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 01/06/2019 (Saturnday):\n",
      "        ++   [Task1 - Penguin Russian Course] Chapter 23. Time Expressions\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 29\n",
      "<\n",
      "\n",
      "_____________________________________________ Week 27 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 02/06/2019 (Sunday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 03/06/2019 (Monday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 15. Temporal Probabilistic Thinking - Exercises\n",
      "        ++   [Task11 - MIT 6.034 - AI] Learning: near misses, felicity conditions\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 04/06/2019 (Tuesday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 05/06/2019 (Wednesday):\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 16 - Series Solutions of Ordinary Differential Equations\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 06/06/2019 (Thursday):\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 27\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 07/06/2019 (Friday):\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 30\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 08/06/2019 (Saturnday):\n",
      "        ++   [Task18 - Books 2019] Book 18 - Justica - Michael J. Sandel\n",
      "<\n",
      "\n",
      "_____________________________________________ Week 28 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 09/06/2019 (Sunday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 16. Simple Decisions Taking\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 10/06/2019 (Monday):\n",
      "        ++   [Task1 - Penguin Russian Course] Chapter 24. Negation; Place\n",
      "        ++   [Task11 - MIT 6.034 - AI] Learning: support vector machines\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 16 - Series Solutions of Ordinary Differential Equations Exercises\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 11/06/2019 (Tuesday):\n",
      "        +    [Task9 - Take Off in German] Chapter 7\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 12/06/2019 (Wednesday):\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 28\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 13/06/2019 (Thursday):\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 31\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 14/06/2019 (Friday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 15/06/2019 (Saturnday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 16. Simple Decisions Taking - Exercises\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 17 - Eigenfunction Methods for Differential Equations\n",
      "<\n",
      "\n",
      "_____________________________________________ Week 29 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 16/06/2019 (Sunday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 17/06/2019 (Monday):\n",
      "        ++   [Task11 - MIT 6.034 - AI] Mega-Recitaiton 5: Support Vector Machines\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 18/06/2019 (Tuesday):\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 29\n",
      "        ++   [Task18 - Books 2019] Book 19 - L'Insoutenable Legerete de L'etre - Milan Kundera\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 19/06/2019 (Wednesday):\n",
      "        ++   [Task1 - Penguin Russian Course] Chapter 25. Diminutives; Proper Names; Politness\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 32\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 20/06/2019 (Thursday):\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 17 - Eigenfunction Methods for Differential Equations Exercises\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 21/06/2019 (Friday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 17. Complex Decisions Taking\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 22/06/2019 (Saturnday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "_____________________________________________ Week 30 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 23/06/2019 (Sunday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 24/06/2019 (Monday):\n",
      "        ++   [Task11 - MIT 6.034 - AI] Problem set 4 due\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 30\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 25/06/2019 (Tuesday):\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 33\n",
      "        +    [Task9 - Take Off in German] Chapter 7 - Test\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 18 - Special Functions\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 26/06/2019 (Wednesday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 27/06/2019 (Thursday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 17. Complex Decisions Taking - Exercises\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 28/06/2019 (Friday):\n",
      "        ++   [Task1 - Penguin Russian Course] Chapter 26. Indefinite Pronouns; Word Order; Writting Letters\n",
      "        ++   [Task18 - Books 2019] Book 20 - Der Kleine Prinz - Antoine de Saint-Exupery\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 29/06/2019 (Saturnday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "_____________________________________________ Week 31 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 30/06/2019 (Sunday):\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 31\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 18 - Special Functions Exercises\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 01/07/2019 (Monday):\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 34\n",
      "        ++   [Task11 - MIT 6.034 - AI] Quiz 3\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 02/07/2019 (Tuesday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 03/07/2019 (Wednesday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 18. Learning from Examples\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 04/07/2019 (Thursday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 05/07/2019 (Friday):\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 19 - Quantum Operators\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 06/07/2019 (Saturnday):\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 32\n",
      "<\n",
      "\n",
      "_____________________________________________ Week 32 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 07/07/2019 (Sunday):\n",
      "        ++   [Task1 - Penguin Russian Course] Chapter 27. Participles: Types and Stress\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 35\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 08/07/2019 (Monday):\n",
      "        ++   [Task11 - MIT 6.034 - AI] Learning: boosting\n",
      "        ++   [Task18 - Books 2019] Book 21 - Guerra e Paz I - Liev Tolstoi\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 09/07/2019 (Tuesday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 18. Learning from Examples - Exercises\n",
      "        +    [Task9 - Take Off in German] Chapter 8\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 10/07/2019 (Wednesday):\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 19 - Quantum Operators Exercises\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 11/07/2019 (Thursday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 12/07/2019 (Friday):\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 33\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 13/07/2019 (Saturnday):\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 36\n",
      "<\n",
      "\n",
      "_____________________________________________ Week 33 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 14/07/2019 (Sunday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 15/07/2019 (Monday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 19. Knowledge in Learning\n",
      "        ++   [Task11 - MIT 6.034 - AI] Mega-Recitaiton 6: Boosting\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 20 - Partial Differential Equations: General and Particular Solutions\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 16/07/2019 (Tuesday):\n",
      "        ++   [Task1 - Penguin Russian Course] Chapter 28. Verbal Adverbs\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 17/07/2019 (Wednesday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 18/07/2019 (Thursday):\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 34\n",
      "        ++   [Task18 - Books 2019] Book 22 - Guerra e Paz II - Liev Tolstoi\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 19/07/2019 (Friday):\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 37\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 20/07/2019 (Saturnday):\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 20 - Partial Differential Equations: General and Particular Solutions Exercises\n",
      "<\n",
      "\n",
      "_____________________________________________ Week 34 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 21/07/2019 (Sunday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 19. Knowledge in Learning - Exercises\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 22/07/2019 (Monday):\n",
      "        ++   [Task11 - MIT 6.034 - AI] Representations: classes, trajectories, transitions\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 23/07/2019 (Tuesday):\n",
      "        +    [Task9 - Take Off in German] Chapter 8 - Test\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 24/07/2019 (Wednesday):\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 35\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 25/07/2019 (Thursday):\n",
      "        ++   [Task1 - Penguin Russian Course] Chapter 29. 'Bookish' Style; Active Participles; Punctuation; Short-Form Adjectives\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 38\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 21 - Partial Differential Equations: Separation of Variables and Other Methods\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 26/07/2019 (Friday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 27/07/2019 (Saturnday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 20. Learning of Probabilistic Models\n",
      "<\n",
      "\n",
      "_____________________________________________ Week 35 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 28/07/2019 (Sunday):\n",
      "        ++   [Task18 - Books 2019] Book 23 - Topographie de la Terreur\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 29/07/2019 (Monday):\n",
      "        ++   [Task11 - MIT 6.034 - AI] Architectures: GPS, SOAR, Subsumption, Society of Mind\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 30/07/2019 (Tuesday):\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 36\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 21 - Partial Differential Equations: Separation of Variables and Other Methods Exercises\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 31/07/2019 (Wednesday):\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 39\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 01/08/2019 (Thursday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 02/08/2019 (Friday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 20. Learning of Probabilistic Models - Exercises\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 03/08/2019 (Saturnday):\n",
      "        ++   [Task1 - Penguin Russian Course] Chapter 30. Abbreviations; Names of Russian Letters; Participles\n",
      "<\n",
      "\n",
      "_____________________________________________ Week 36 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 04/08/2019 (Sunday):\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 22 - Calculus of Variations\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 05/08/2019 (Monday):\n",
      "        ++   [Task11 - MIT 6.034 - AI] The AI business\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 37\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 06/08/2019 (Tuesday):\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 40\n",
      "        +    [Task9 - Take Off in German] Chapter 9\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 07/08/2019 (Wednesday):\n",
      "        ++   [Task18 - Books 2019] Book 24\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 08/08/2019 (Thursday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 21. Learning by Reinforcement\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 09/08/2019 (Friday):\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 22 - Calculus of Variations Exercises\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 10/08/2019 (Saturnday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "_____________________________________________ Week 37 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 11/08/2019 (Sunday):\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 38\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 12/08/2019 (Monday):\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 41\n",
      "        ++   [Task11 - MIT 6.034 - AI] Mega-Recitaiton 7: Near Misses, Arch Learning\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 13/08/2019 (Tuesday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 14/08/2019 (Wednesday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 21. Learning by Reinforcement - Exercises\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 23 - Integral Equations\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 15/08/2019 (Thursday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 16/08/2019 (Friday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 17/08/2019 (Saturnday):\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 39\n",
      "        ++   [Task18 - Books 2019] Book 25\n",
      "<\n",
      "\n",
      "_____________________________________________ Week 38 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 18/08/2019 (Sunday):\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 42\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 19/08/2019 (Monday):\n",
      "        ++   [Task11 - MIT 6.034 - AI] Probabilistic inference I\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 23 - Integral Equations Exercises\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 20/08/2019 (Tuesday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 22. Natural Language Processing\n",
      "        +    [Task9 - Take Off in German] Chapter 9 - Test\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 21/08/2019 (Wednesday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 22/08/2019 (Thursday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 23/08/2019 (Friday):\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 40\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 24/08/2019 (Saturnday):\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 43\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 24 - Complex Variables\n",
      "<\n",
      "\n",
      "_____________________________________________ Week 39 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 25/08/2019 (Sunday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 26/08/2019 (Monday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 22. Natural Language Processing - Exercises\n",
      "        ++   [Task11 - MIT 6.034 - AI] Quiz 4\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 27/08/2019 (Tuesday):\n",
      "        ++   [Task18 - Books 2019] Book 26\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 28/08/2019 (Wednesday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 29/08/2019 (Thursday):\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 41\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 24 - Complex Variables Exercises\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 30/08/2019 (Friday):\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 44\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 31/08/2019 (Saturnday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "_____________________________________________ Week 40 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 01/09/2019 (Sunday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 23. Natural Language for Communication\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 02/09/2019 (Monday):\n",
      "        ++   [Task11 - MIT 6.034 - AI] Probabilistic inference II\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 03/09/2019 (Tuesday):\n",
      "        +    [Task9 - Take Off in German] Chapter 10\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 25 - Application of Complex Variables\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 04/09/2019 (Wednesday):\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 42\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 05/09/2019 (Thursday):\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 45\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 06/09/2019 (Friday):\n",
      "        ++   [Task18 - Books 2019] Book 27\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 07/09/2019 (Saturnday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 23. Natural Language for Communication - Exercises\n",
      "<\n",
      "\n",
      "_____________________________________________ Week 41 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 08/09/2019 (Sunday):\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 25 - Application of Complex Variables Exercises\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 09/09/2019 (Monday):\n",
      "        ++   [Task11 - MIT 6.034 - AI] Model merging, cross-modal coupling, course summary\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 10/09/2019 (Tuesday):\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 43\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 11/09/2019 (Wednesday):\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 46\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 12/09/2019 (Thursday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 13/09/2019 (Friday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 24. Perception\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 26 - Tensors\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 14/09/2019 (Saturnday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "_____________________________________________ Week 42 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 15/09/2019 (Sunday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 16/09/2019 (Monday):\n",
      "        ++   [Task11 - MIT 6.034 - AI] Problem set 5 due\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 44\n",
      "        ++   [Task18 - Books 2019] Book 28\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 17/09/2019 (Tuesday):\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 47\n",
      "        +    [Task9 - Take Off in German] Chapter 10 - Test\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 18/09/2019 (Wednesday):\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 26 - Tensors Exercises\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 19/09/2019 (Thursday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 24. Perception - Exercises\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 20/09/2019 (Friday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 21/09/2019 (Saturnday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "_____________________________________________ Week 43 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 22/09/2019 (Sunday):\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 45\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 23/09/2019 (Monday):\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 48\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 27 - Numerical Methods\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 24/09/2019 (Tuesday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 25/09/2019 (Wednesday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 25. Robotics\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 26/09/2019 (Thursday):\n",
      "        ++   [Task18 - Books 2019] Book 29\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 27/09/2019 (Friday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 28/09/2019 (Saturnday):\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 46\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 27 - Numerical Methods Exercises\n",
      "<\n",
      "\n",
      "_____________________________________________ Week 44 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 29/09/2019 (Sunday):\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 49\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 30/09/2019 (Monday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 01/10/2019 (Tuesday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 25. Robotics - Exercises\n",
      "        +    [Task9 - Take Off in German] Chapter 11\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 02/10/2019 (Wednesday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 03/10/2019 (Thursday):\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 28 - Group Theory\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 04/10/2019 (Friday):\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 47\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 05/10/2019 (Saturnday):\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 50\n",
      "<\n",
      "\n",
      "_____________________________________________ Week 45 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 06/10/2019 (Sunday):\n",
      "        ++   [Task18 - Books 2019] Book 30\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 07/10/2019 (Monday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 26. Philosofical Fundaments\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 08/10/2019 (Tuesday):\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 28 - Group Theory Exercises\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 09/10/2019 (Wednesday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 10/10/2019 (Thursday):\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 48\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 11/10/2019 (Friday):\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 51\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 12/10/2019 (Saturnday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "_____________________________________________ Week 46 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 13/10/2019 (Sunday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 26. Philosofical Fundaments - Exercises\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 29 - Representation Theory\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 14/10/2019 (Monday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 15/10/2019 (Tuesday):\n",
      "        +    [Task9 - Take Off in German] Chapter 11 - Test\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 16/10/2019 (Wednesday):\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 49\n",
      "        ++   [Task18 - Books 2019] Book 31\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 17/10/2019 (Thursday):\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 52\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 18/10/2019 (Friday):\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 29 - Representation Theory Exercises\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 19/10/2019 (Saturnday):\n",
      "        +++  [Task6 - Artificial Intelligence] Chapter 27. AI, Present and Future\n",
      "<\n",
      "\n",
      "_____________________________________________ Week 47 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 20/10/2019 (Sunday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 21/10/2019 (Monday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 22/10/2019 (Tuesday):\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 50\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 23/10/2019 (Wednesday):\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 53\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 30 - Probability\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 24/10/2019 (Thursday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 25/10/2019 (Friday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 26/10/2019 (Saturnday):\n",
      "        ++   [Task18 - Books 2019] Book 32\n",
      "<\n",
      "\n",
      "_____________________________________________ Week 48 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 27/10/2019 (Sunday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 28/10/2019 (Monday):\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 51\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 30 - Probability Exercises\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 29/10/2019 (Tuesday):\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 54\n",
      "        +    [Task9 - Take Off in German] Chapter 12\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 30/10/2019 (Wednesday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 31/10/2019 (Thursday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 01/11/2019 (Friday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 02/11/2019 (Saturnday):\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 31 - Statistics\n",
      "<\n",
      "\n",
      "_____________________________________________ Week 49 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 03/11/2019 (Sunday):\n",
      "        +++  [Task17 - At Least one Paper per Week] Paper 52\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 04/11/2019 (Monday):\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 55\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 05/11/2019 (Tuesday):\n",
      "        ++   [Task18 - Books 2019] Book 33\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 06/11/2019 (Wednesday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 07/11/2019 (Thursday):\n",
      "        +++  [Task20 - Mathematical Methods for Physics and Engineering] Chapter 31 - Statistics Exercises\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 08/11/2019 (Friday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 09/11/2019 (Saturnday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "_____________________________________________ Week 50 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 10/11/2019 (Sunday):\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 56\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 11/11/2019 (Monday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 12/11/2019 (Tuesday):\n",
      "        +    [Task9 - Take Off in German] Chapter 12 - Test\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 13/11/2019 (Wednesday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 14/11/2019 (Thursday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 15/11/2019 (Friday):\n",
      "        ++   [Task18 - Books 2019] Book 34\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 16/11/2019 (Saturnday):\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 57\n",
      "<\n",
      "\n",
      "_____________________________________________ Week 51 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 17/11/2019 (Sunday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 18/11/2019 (Monday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 19/11/2019 (Tuesday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 20/11/2019 (Wednesday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 21/11/2019 (Thursday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 22/11/2019 (Friday):\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 58\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 23/11/2019 (Saturnday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "_____________________________________________ Week 52 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 24/11/2019 (Sunday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 25/11/2019 (Monday):\n",
      "        ++   [Task18 - Books 2019] Book 35\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 26/11/2019 (Tuesday):\n",
      "        +    [Task9 - Take Off in German] Chapter 13\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 27/11/2019 (Wednesday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 28/11/2019 (Thursday):\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 59\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 29/11/2019 (Friday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 30/11/2019 (Saturnday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "_____________________________________________ Week 53 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 01/12/2019 (Sunday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 02/12/2019 (Monday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 03/12/2019 (Tuesday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 04/12/2019 (Wednesday):\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 60\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 05/12/2019 (Thursday):\n",
      "        ++   [Task18 - Books 2019] Book 36\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 06/12/2019 (Friday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 07/12/2019 (Saturnday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "_____________________________________________ Week 54 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 08/12/2019 (Sunday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 09/12/2019 (Monday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 10/12/2019 (Tuesday):\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 61\n",
      "        +    [Task9 - Take Off in German] Chapter 13 - Test\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 11/12/2019 (Wednesday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 12/12/2019 (Thursday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 13/12/2019 (Friday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 14/12/2019 (Saturnday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "_____________________________________________ Week 55 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 15/12/2019 (Sunday):\n",
      "        ++   [Task18 - Books 2019] Book 37\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 16/12/2019 (Monday):\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 62\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 17/12/2019 (Tuesday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 18/12/2019 (Wednesday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 19/12/2019 (Thursday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 20/12/2019 (Friday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 21/12/2019 (Saturnday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "_____________________________________________ Week 56 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 22/12/2019 (Sunday):\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 63\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 23/12/2019 (Monday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 24/12/2019 (Tuesday):\n",
      "        +    [Task9 - Take Off in German] Chapter 14\n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 25/12/2019 (Wednesday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 26/12/2019 (Thursday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 27/12/2019 (Friday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 28/12/2019 (Saturnday):\n",
      "        +++  [Task7 - Principles of Neuroscience] Chapter 64\n",
      "<\n",
      "\n",
      "_____________________________________________ Week 57 _____________________________________________\n",
      "\n",
      "\n",
      "> \n",
      "ToDo List 29/12/2019 (Sunday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "> \n",
      "ToDo List 30/12/2019 (Monday):\n",
      "\n",
      "                            [---- Rest (OR MOVE TASKS FOR TODAY) ----]                            \n",
      "<\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open('C:\\\\Users\\\\seidi\\\\Documents\\\\(mapped) Important\\\\DailyToDo.txt', 'r')\n",
    "raw = f.read() # creates a string with the contents of the whole \n",
    "\n",
    "print(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Text from PDF, MSWord and other Binary Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only textual description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The NLP Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [NLTK Chapter 4] NLP Pipeline.PNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('C:\\\\Users\\\\seidi\\\\Documents\\\\(mapped) Important\\\\DailyToDo.txt', 'r')\n",
    "raw = f.read()\n",
    "\n",
    "type(raw) # Raw is a string\n",
    "\n",
    "tokens = word_tokenize(raw)\n",
    "type(tokens) # Tokens is a list\n",
    "\n",
    "# Normalizing and Sorting lists produces a list\n",
    "words = [w.lower() for w in tokens]\n",
    "type(words) \n",
    "\n",
    "vocab = sorted(set(words))\n",
    "type(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strings: Text Processing at the Lowest Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Operations with Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Monty Python'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\"Monty Python's Flying Circus\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\"Monty Python's Flying Circus\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monty = 'Monty Python' # Creating a simple string\n",
    "circus = \"Monty Python's Flying Circus\"   # How to eal with a quotemark\n",
    "circus2 = 'Monty Python\\'s Flying Circus' # How to eal with a quotemark\n",
    "\n",
    "monty\n",
    "\n",
    "circus\n",
    "\n",
    "circus2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shall I compare thee to a Summer's day?Thou are more lovely and more temperate:\n",
      "Shall I compare thee to a Summer's day?Thou are more lovely and more temperate:\n"
     ]
    }
   ],
   "source": [
    "# How to write multiple lines string\n",
    "\n",
    "# Method 1: use \\\n",
    "couplet = \"Shall I compare thee to a Summer's day?\"\\\n",
    "          \"Thou are more lovely and more temperate:\"\n",
    "\n",
    "print(couplet)\n",
    "\n",
    "# Method 2: use ()\n",
    "couplet = (\"Shall I compare thee to a Summer's day?\"\n",
    "          \"Thou are more lovely and more temperate:\")\n",
    "\n",
    "print(couplet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Shall I compare thee to a Summer's day?\\n          Thou are more lovely and more temperate:\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to add newline to print\n",
    "\n",
    "# Use eiter \"\"\" \"\"\" or ''' '''\n",
    "couplet = \"\"\"Shall I compare thee to a Summer's day?\n",
    "          Thou are more lovely and more temperate:\"\"\"\n",
    "\n",
    "couplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "veryveryvery\n",
      "veryveryvery\n"
     ]
    }
   ],
   "source": [
    "# Concatenate and multiply string\n",
    "\n",
    "print('very' + 'very' + 'very')\n",
    "\n",
    "print('very'*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            very\n",
      "          veryvery\n",
      "        veryveryvery\n",
      "      veryveryveryvery\n",
      "    veryveryveryveryvery\n",
      "  veryveryveryveryveryvery\n",
      "veryveryveryveryveryveryvery\n",
      "  veryveryveryveryveryvery\n",
      "    veryveryveryveryvery\n",
      "      veryveryveryvery\n",
      "        veryveryvery\n",
      "          veryvery\n",
      "            very\n"
     ]
    }
   ],
   "source": [
    "a = [1, 2, 3, 4, 5, 6, 7, 6, 5, 4, 3, 2, 1]\n",
    "b = [' ' * 2 * (7 - i) + 'very' * i for i in a]\n",
    "for line in b:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monty Python\n"
     ]
    }
   ],
   "source": [
    "print(monty) # Show without quotation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monty Python Holy Grail\n",
      "Monty Python and the Holy Grail\n"
     ]
    }
   ],
   "source": [
    "# Concatenating in print\n",
    "grail = \"Holy Grail\"\n",
    "print(monty, grail)\n",
    "\n",
    "print(monty, 'and the', grail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Individual Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'M'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monty[0]\n",
    "monty[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'y'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting from end\n",
    "monty[-1] # last character\n",
    "monty[-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('e', 117092), ('t', 87996), ('a', 77916), ('o', 69326), ('n', 65617)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['e', 't', 'a', 'o', 'n', 'i', 's', 'h', 'r', 'l']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# counting just lowercase letters from moby dick\n",
    "from nltk.corpus import gutenberg\n",
    "raw = gutenberg.raw('melville-moby_dick.txt')\n",
    "fdist = nltk.FreqDist(ch.lower() for ch in raw if ch.isalpha())\n",
    "fdist.most_common(5)\n",
    "[char for (char, count) in fdist.most_common(10)]\n",
    "\n",
    "# This can be used to identify the language of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pyth'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Monty'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Slicing\n",
    "monty[6:10] # from indexes 6 to 9\n",
    "monty[-12:-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Monty Pyt'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "' Python'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Omiting first value makes slicing start form index 0\n",
    "monty[:9]\n",
    "\n",
    "# Omiting second value makes slicing ends on last index\n",
    "monty[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 'thing'\n"
     ]
    }
   ],
   "source": [
    "# To find a substring in a string, just use in\n",
    "phrase = 'And now for something completely different'\n",
    "if 'thing' in phrase:\n",
    "    print(\"found 'thing'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To find index of a substring, use find()\n",
    "monty.find('Python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See Table 3.2 for some useful string commands    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class str in module builtins:\n",
      "\n",
      "class str(object)\n",
      " |  str(object='') -> str\n",
      " |  str(bytes_or_buffer[, encoding[, errors]]) -> str\n",
      " |  \n",
      " |  Create a new string object from the given object. If encoding or\n",
      " |  errors is specified, then the object must expose a data buffer\n",
      " |  that will be decoded using the given encoding and error handler.\n",
      " |  Otherwise, returns the result of object.__str__() (if defined)\n",
      " |  or repr(object).\n",
      " |  encoding defaults to sys.getdefaultencoding().\n",
      " |  errors defaults to 'strict'.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __add__(self, value, /)\n",
      " |      Return self+value.\n",
      " |  \n",
      " |  __contains__(self, key, /)\n",
      " |      Return key in self.\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __format__(...)\n",
      " |      S.__format__(format_spec) -> str\n",
      " |      \n",
      " |      Return a formatted version of S as described by format_spec.\n",
      " |  \n",
      " |  __ge__(self, value, /)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __getitem__(self, key, /)\n",
      " |      Return self[key].\n",
      " |  \n",
      " |  __getnewargs__(...)\n",
      " |  \n",
      " |  __gt__(self, value, /)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __hash__(self, /)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __le__(self, value, /)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  __lt__(self, value, /)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __mod__(self, value, /)\n",
      " |      Return self%value.\n",
      " |  \n",
      " |  __mul__(self, value, /)\n",
      " |      Return self*value.\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __rmod__(self, value, /)\n",
      " |      Return value%self.\n",
      " |  \n",
      " |  __rmul__(self, value, /)\n",
      " |      Return value*self.\n",
      " |  \n",
      " |  __sizeof__(...)\n",
      " |      S.__sizeof__() -> size of S in memory, in bytes\n",
      " |  \n",
      " |  __str__(self, /)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  capitalize(...)\n",
      " |      S.capitalize() -> str\n",
      " |      \n",
      " |      Return a capitalized version of S, i.e. make the first character\n",
      " |      have upper case and the rest lower case.\n",
      " |  \n",
      " |  casefold(...)\n",
      " |      S.casefold() -> str\n",
      " |      \n",
      " |      Return a version of S suitable for caseless comparisons.\n",
      " |  \n",
      " |  center(...)\n",
      " |      S.center(width[, fillchar]) -> str\n",
      " |      \n",
      " |      Return S centered in a string of length width. Padding is\n",
      " |      done using the specified fill character (default is a space)\n",
      " |  \n",
      " |  count(...)\n",
      " |      S.count(sub[, start[, end]]) -> int\n",
      " |      \n",
      " |      Return the number of non-overlapping occurrences of substring sub in\n",
      " |      string S[start:end].  Optional arguments start and end are\n",
      " |      interpreted as in slice notation.\n",
      " |  \n",
      " |  encode(...)\n",
      " |      S.encode(encoding='utf-8', errors='strict') -> bytes\n",
      " |      \n",
      " |      Encode S using the codec registered for encoding. Default encoding\n",
      " |      is 'utf-8'. errors may be given to set a different error\n",
      " |      handling scheme. Default is 'strict' meaning that encoding errors raise\n",
      " |      a UnicodeEncodeError. Other possible values are 'ignore', 'replace' and\n",
      " |      'xmlcharrefreplace' as well as any other name registered with\n",
      " |      codecs.register_error that can handle UnicodeEncodeErrors.\n",
      " |  \n",
      " |  endswith(...)\n",
      " |      S.endswith(suffix[, start[, end]]) -> bool\n",
      " |      \n",
      " |      Return True if S ends with the specified suffix, False otherwise.\n",
      " |      With optional start, test S beginning at that position.\n",
      " |      With optional end, stop comparing S at that position.\n",
      " |      suffix can also be a tuple of strings to try.\n",
      " |  \n",
      " |  expandtabs(...)\n",
      " |      S.expandtabs(tabsize=8) -> str\n",
      " |      \n",
      " |      Return a copy of S where all tab characters are expanded using spaces.\n",
      " |      If tabsize is not given, a tab size of 8 characters is assumed.\n",
      " |  \n",
      " |  find(...)\n",
      " |      S.find(sub[, start[, end]]) -> int\n",
      " |      \n",
      " |      Return the lowest index in S where substring sub is found,\n",
      " |      such that sub is contained within S[start:end].  Optional\n",
      " |      arguments start and end are interpreted as in slice notation.\n",
      " |      \n",
      " |      Return -1 on failure.\n",
      " |  \n",
      " |  format(...)\n",
      " |      S.format(*args, **kwargs) -> str\n",
      " |      \n",
      " |      Return a formatted version of S, using substitutions from args and kwargs.\n",
      " |      The substitutions are identified by braces ('{' and '}').\n",
      " |  \n",
      " |  format_map(...)\n",
      " |      S.format_map(mapping) -> str\n",
      " |      \n",
      " |      Return a formatted version of S, using substitutions from mapping.\n",
      " |      The substitutions are identified by braces ('{' and '}').\n",
      " |  \n",
      " |  index(...)\n",
      " |      S.index(sub[, start[, end]]) -> int\n",
      " |      \n",
      " |      Return the lowest index in S where substring sub is found, \n",
      " |      such that sub is contained within S[start:end].  Optional\n",
      " |      arguments start and end are interpreted as in slice notation.\n",
      " |      \n",
      " |      Raises ValueError when the substring is not found.\n",
      " |  \n",
      " |  isalnum(...)\n",
      " |      S.isalnum() -> bool\n",
      " |      \n",
      " |      Return True if all characters in S are alphanumeric\n",
      " |      and there is at least one character in S, False otherwise.\n",
      " |  \n",
      " |  isalpha(...)\n",
      " |      S.isalpha() -> bool\n",
      " |      \n",
      " |      Return True if all characters in S are alphabetic\n",
      " |      and there is at least one character in S, False otherwise.\n",
      " |  \n",
      " |  isdecimal(...)\n",
      " |      S.isdecimal() -> bool\n",
      " |      \n",
      " |      Return True if there are only decimal characters in S,\n",
      " |      False otherwise.\n",
      " |  \n",
      " |  isdigit(...)\n",
      " |      S.isdigit() -> bool\n",
      " |      \n",
      " |      Return True if all characters in S are digits\n",
      " |      and there is at least one character in S, False otherwise.\n",
      " |  \n",
      " |  isidentifier(...)\n",
      " |      S.isidentifier() -> bool\n",
      " |      \n",
      " |      Return True if S is a valid identifier according\n",
      " |      to the language definition.\n",
      " |      \n",
      " |      Use keyword.iskeyword() to test for reserved identifiers\n",
      " |      such as \"def\" and \"class\".\n",
      " |  \n",
      " |  islower(...)\n",
      " |      S.islower() -> bool\n",
      " |      \n",
      " |      Return True if all cased characters in S are lowercase and there is\n",
      " |      at least one cased character in S, False otherwise.\n",
      " |  \n",
      " |  isnumeric(...)\n",
      " |      S.isnumeric() -> bool\n",
      " |      \n",
      " |      Return True if there are only numeric characters in S,\n",
      " |      False otherwise.\n",
      " |  \n",
      " |  isprintable(...)\n",
      " |      S.isprintable() -> bool\n",
      " |      \n",
      " |      Return True if all characters in S are considered\n",
      " |      printable in repr() or S is empty, False otherwise.\n",
      " |  \n",
      " |  isspace(...)\n",
      " |      S.isspace() -> bool\n",
      " |      \n",
      " |      Return True if all characters in S are whitespace\n",
      " |      and there is at least one character in S, False otherwise.\n",
      " |  \n",
      " |  istitle(...)\n",
      " |      S.istitle() -> bool\n",
      " |      \n",
      " |      Return True if S is a titlecased string and there is at least one\n",
      " |      character in S, i.e. upper- and titlecase characters may only\n",
      " |      follow uncased characters and lowercase characters only cased ones.\n",
      " |      Return False otherwise.\n",
      " |  \n",
      " |  isupper(...)\n",
      " |      S.isupper() -> bool\n",
      " |      \n",
      " |      Return True if all cased characters in S are uppercase and there is\n",
      " |      at least one cased character in S, False otherwise.\n",
      " |  \n",
      " |  join(...)\n",
      " |      S.join(iterable) -> str\n",
      " |      \n",
      " |      Return a string which is the concatenation of the strings in the\n",
      " |      iterable.  The separator between elements is S.\n",
      " |  \n",
      " |  ljust(...)\n",
      " |      S.ljust(width[, fillchar]) -> str\n",
      " |      \n",
      " |      Return S left-justified in a Unicode string of length width. Padding is\n",
      " |      done using the specified fill character (default is a space).\n",
      " |  \n",
      " |  lower(...)\n",
      " |      S.lower() -> str\n",
      " |      \n",
      " |      Return a copy of the string S converted to lowercase.\n",
      " |  \n",
      " |  lstrip(...)\n",
      " |      S.lstrip([chars]) -> str\n",
      " |      \n",
      " |      Return a copy of the string S with leading whitespace removed.\n",
      " |      If chars is given and not None, remove characters in chars instead.\n",
      " |  \n",
      " |  partition(...)\n",
      " |      S.partition(sep) -> (head, sep, tail)\n",
      " |      \n",
      " |      Search for the separator sep in S, and return the part before it,\n",
      " |      the separator itself, and the part after it.  If the separator is not\n",
      " |      found, return S and two empty strings.\n",
      " |  \n",
      " |  replace(...)\n",
      " |      S.replace(old, new[, count]) -> str\n",
      " |      \n",
      " |      Return a copy of S with all occurrences of substring\n",
      " |      old replaced by new.  If the optional argument count is\n",
      " |      given, only the first count occurrences are replaced.\n",
      " |  \n",
      " |  rfind(...)\n",
      " |      S.rfind(sub[, start[, end]]) -> int\n",
      " |      \n",
      " |      Return the highest index in S where substring sub is found,\n",
      " |      such that sub is contained within S[start:end].  Optional\n",
      " |      arguments start and end are interpreted as in slice notation.\n",
      " |      \n",
      " |      Return -1 on failure.\n",
      " |  \n",
      " |  rindex(...)\n",
      " |      S.rindex(sub[, start[, end]]) -> int\n",
      " |      \n",
      " |      Return the highest index in S where substring sub is found,\n",
      " |      such that sub is contained within S[start:end].  Optional\n",
      " |      arguments start and end are interpreted as in slice notation.\n",
      " |      \n",
      " |      Raises ValueError when the substring is not found.\n",
      " |  \n",
      " |  rjust(...)\n",
      " |      S.rjust(width[, fillchar]) -> str\n",
      " |      \n",
      " |      Return S right-justified in a string of length width. Padding is\n",
      " |      done using the specified fill character (default is a space).\n",
      " |  \n",
      " |  rpartition(...)\n",
      " |      S.rpartition(sep) -> (head, sep, tail)\n",
      " |      \n",
      " |      Search for the separator sep in S, starting at the end of S, and return\n",
      " |      the part before it, the separator itself, and the part after it.  If the\n",
      " |      separator is not found, return two empty strings and S.\n",
      " |  \n",
      " |  rsplit(...)\n",
      " |      S.rsplit(sep=None, maxsplit=-1) -> list of strings\n",
      " |      \n",
      " |      Return a list of the words in S, using sep as the\n",
      " |      delimiter string, starting at the end of the string and\n",
      " |      working to the front.  If maxsplit is given, at most maxsplit\n",
      " |      splits are done. If sep is not specified, any whitespace string\n",
      " |      is a separator.\n",
      " |  \n",
      " |  rstrip(...)\n",
      " |      S.rstrip([chars]) -> str\n",
      " |      \n",
      " |      Return a copy of the string S with trailing whitespace removed.\n",
      " |      If chars is given and not None, remove characters in chars instead.\n",
      " |  \n",
      " |  split(...)\n",
      " |      S.split(sep=None, maxsplit=-1) -> list of strings\n",
      " |      \n",
      " |      Return a list of the words in S, using sep as the\n",
      " |      delimiter string.  If maxsplit is given, at most maxsplit\n",
      " |      splits are done. If sep is not specified or is None, any\n",
      " |      whitespace string is a separator and empty strings are\n",
      " |      removed from the result.\n",
      " |  \n",
      " |  splitlines(...)\n",
      " |      S.splitlines([keepends]) -> list of strings\n",
      " |      \n",
      " |      Return a list of the lines in S, breaking at line boundaries.\n",
      " |      Line breaks are not included in the resulting list unless keepends\n",
      " |      is given and true.\n",
      " |  \n",
      " |  startswith(...)\n",
      " |      S.startswith(prefix[, start[, end]]) -> bool\n",
      " |      \n",
      " |      Return True if S starts with the specified prefix, False otherwise.\n",
      " |      With optional start, test S beginning at that position.\n",
      " |      With optional end, stop comparing S at that position.\n",
      " |      prefix can also be a tuple of strings to try.\n",
      " |  \n",
      " |  strip(...)\n",
      " |      S.strip([chars]) -> str\n",
      " |      \n",
      " |      Return a copy of the string S with leading and trailing\n",
      " |      whitespace removed.\n",
      " |      If chars is given and not None, remove characters in chars instead.\n",
      " |  \n",
      " |  swapcase(...)\n",
      " |      S.swapcase() -> str\n",
      " |      \n",
      " |      Return a copy of S with uppercase characters converted to lowercase\n",
      " |      and vice versa.\n",
      " |  \n",
      " |  title(...)\n",
      " |      S.title() -> str\n",
      " |      \n",
      " |      Return a titlecased version of S, i.e. words start with title case\n",
      " |      characters, all remaining cased characters have lower case.\n",
      " |  \n",
      " |  translate(...)\n",
      " |      S.translate(table) -> str\n",
      " |      \n",
      " |      Return a copy of the string S in which each character has been mapped\n",
      " |      through the given translation table. The table must implement\n",
      " |      lookup/indexing via __getitem__, for instance a dictionary or list,\n",
      " |      mapping Unicode ordinals to Unicode ordinals, strings, or None. If\n",
      " |      this operation raises LookupError, the character is left untouched.\n",
      " |      Characters mapped to None are deleted.\n",
      " |  \n",
      " |  upper(...)\n",
      " |      S.upper() -> str\n",
      " |      \n",
      " |      Return a copy of S converted to uppercase.\n",
      " |  \n",
      " |  zfill(...)\n",
      " |      S.zfill(width) -> str\n",
      " |      \n",
      " |      Pad a numeric string S with zeros on the left, to fill a field\n",
      " |      of the specified width. The string S is never truncated.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  maketrans(x, y=None, z=None, /)\n",
      " |      Return a translation table usable for str.translate().\n",
      " |      \n",
      " |      If there is only one argument, it must be a dictionary mapping Unicode\n",
      " |      ordinals (integers) or characters to Unicode ordinals, strings or None.\n",
      " |      Character keys will be then converted to ordinals.\n",
      " |      If there are two arguments, they must be strings of equal length, and\n",
      " |      in the resulting dictionary, each character in x will be mapped to the\n",
      " |      character at the same position in y. If there is a third argument, it\n",
      " |      must be a string, whose characters will be mapped to None in the result.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prompt info on string\n",
    "help(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing with Unicode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an introductory text on the book, so you can understand what is Unicode and decoding/encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting encoded text from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruska Biblioteka Państwowa. Jej dawne zbiory znane pod nazwą\n",
      "\"Berlinka\" to skarb kultury i sztuki niemieckiej. Przewiezione przez\n",
      "Niemców pod koniec II wojny światowej na Dolny Śląsk, zostały\n",
      "odnalezione po 1945 r. na terytorium Polski. Trafiły do Biblioteki\n",
      "Jagiellońskiej w Krakowie, obejmują ponad 500 tys. zabytkowych\n",
      "archiwaliów, m.in. manuskrypty Goethego, Mozarta, Beethovena, Bacha.\n"
     ]
    }
   ],
   "source": [
    "# Opening a polish file encoded in latin-2\n",
    "path = nltk.data.find('corpora/unicode_samples/polish-lat2.txt')\n",
    "\n",
    "f = open(path, encoding='latin2') # Decode form latin-2 to Unicode\n",
    "\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    print(line) # Encode form Unicode to print format (UTF-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Pruska Biblioteka Pa\\\\u0144stwowa. Jej dawne zbiory znane pod nazw\\\\u0105'\n",
      "b'\"Berlinka\" to skarb kultury i sztuki niemieckiej. Przewiezione przez'\n",
      "b'Niemc\\\\xf3w pod koniec II wojny \\\\u015bwiatowej na Dolny \\\\u015al\\\\u0105sk, zosta\\\\u0142y'\n",
      "b'odnalezione po 1945 r. na terytorium Polski. Trafi\\\\u0142y do Biblioteki'\n",
      "b'Jagiello\\\\u0144skiej w Krakowie, obejmuj\\\\u0105 ponad 500 tys. zabytkowych'\n",
      "b'archiwali\\\\xf3w, m.in. manuskrypty Goethego, Mozarta, Beethovena, Bacha.'\n"
     ]
    }
   ],
   "source": [
    "# Printing all non-ASCII characters in code point representation (\\xXX, \\uXXXX)\n",
    "f = open(path, encoding='latin2')\n",
    "for line in f:\n",
    "     line = line.strip()\n",
    "     print(line.encode('unicode_escape'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'0x144'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'ń'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "b'\\xc5\\x84'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding integer representaiton of a character\n",
    "ord('ń')\n",
    "\n",
    "# Define the same character from it's escape sequence\n",
    "hex(324)\n",
    "nacute = '\\u0144' # From UTF-8 representaiton, used in python\n",
    "nacute\n",
    "\n",
    "# bytes representation\n",
    "nacute.encode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Niemc\\\\xf3w pod koniec II wojny \\\\u015bwiatowej na Dolny \\\\u015al\\\\u0105sk, zosta\\\\u0142y\\\\n'\n",
      "b'\\xc3\\xb3' U+00f3 LATIN SMALL LETTER O WITH ACUTE\n",
      "b'\\xc5\\x9b' U+015b LATIN SMALL LETTER S WITH ACUTE\n",
      "b'\\xc5\\x9a' U+015a LATIN CAPITAL LETTER S WITH ACUTE\n",
      "b'\\xc4\\x85' U+0105 LATIN SMALL LETTER A WITH OGONEK\n",
      "b'\\xc5\\x82' U+0142 LATIN SMALL LETTER L WITH STROKE\n"
     ]
    }
   ],
   "source": [
    "# Find non-ASCII character in the third line of the polish text and print\n",
    "#UTF-8 bytes representation, code point in unicode convention and Unicode name\n",
    "\n",
    "import unicodedata\n",
    "lines = open(path, encoding='latin2').readlines()\n",
    "line = lines[2]\n",
    "print(line.encode('unicode_escape'))\n",
    "\n",
    "for c in line:\n",
    "    if ord(c) > 127:\n",
    "         print('{} U+{:04x} {}'.format(c.encode('utf8'), ord(c), unicodedata.name(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'niemców pod koniec ii wojny światowej na dolny śląsk, zostały\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "b'niemc\\\\xf3w pod koniec ii wojny \\\\u015bwiatowej na dolny \\\\u015bl\\\\u0105sk, zosta\\\\u0142y\\\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'światowej'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using python's methods and re to work with Unicode characters\n",
    "line.find('zosta\\u0142y')\n",
    "line = line.lower()\n",
    "line\n",
    "\n",
    "line.encode('unicode_escape')\n",
    "import re # Will be explained later. Regular Expression\n",
    "m = re.search('\\u015b\\w*', line)\n",
    "m.group()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['niemców',\n",
       " 'pod',\n",
       " 'koniec',\n",
       " 'ii',\n",
       " 'wojny',\n",
       " 'światowej',\n",
       " 'na',\n",
       " 'dolny',\n",
       " 'śląsk',\n",
       " ',',\n",
       " 'zostały']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word_tokenizer can work directly with Unicode\n",
    "word_tokenize(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Expressions for Detecting Word Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing re and getting a word list from Words Corpus\n",
    "import re\n",
    "wordlist = [w for w in nltk.corpus.words.words('en') if w.islower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Basic Meta-Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abaissed',\n",
       " 'abandoned',\n",
       " 'abased',\n",
       " 'abashed',\n",
       " 'abatised',\n",
       " 'abed',\n",
       " 'aborted',\n",
       " 'abridged',\n",
       " 'abscessed',\n",
       " 'absconded',\n",
       " 'absorbed',\n",
       " 'abstracted',\n",
       " 'abstricted',\n",
       " 'accelerated',\n",
       " 'accepted',\n",
       " 'accidented',\n",
       " 'accoladed',\n",
       " 'accolated',\n",
       " 'accomplished',\n",
       " 'accosted',\n",
       " 'accredited',\n",
       " 'accursed',\n",
       " 'accused',\n",
       " 'accustomed',\n",
       " 'acetated',\n",
       " 'acheweed',\n",
       " 'aciculated',\n",
       " 'aciliated',\n",
       " 'acknowledged',\n",
       " 'acorned',\n",
       " 'acquainted',\n",
       " 'acquired',\n",
       " 'acquisited',\n",
       " 'acred',\n",
       " 'aculeated',\n",
       " 'addebted',\n",
       " 'added',\n",
       " 'addicted',\n",
       " 'addlebrained',\n",
       " 'addleheaded',\n",
       " 'addlepated',\n",
       " 'addorsed',\n",
       " 'adempted',\n",
       " 'adfected',\n",
       " 'adjoined',\n",
       " 'admired',\n",
       " 'admitted',\n",
       " 'adnexed',\n",
       " 'adopted',\n",
       " 'adossed',\n",
       " 'adreamed',\n",
       " 'adscripted',\n",
       " 'aduncated',\n",
       " 'advanced',\n",
       " 'advised',\n",
       " 'aeried',\n",
       " 'aethered',\n",
       " 'afeared',\n",
       " 'affected',\n",
       " 'affectioned',\n",
       " 'affined',\n",
       " 'afflicted',\n",
       " 'affricated',\n",
       " 'affrighted',\n",
       " 'affronted',\n",
       " 'aforenamed',\n",
       " 'afterfeed',\n",
       " 'aftershafted',\n",
       " 'afterthoughted',\n",
       " 'afterwitted',\n",
       " 'agazed',\n",
       " 'aged',\n",
       " 'agglomerated',\n",
       " 'aggrieved',\n",
       " 'agminated',\n",
       " 'agnamed',\n",
       " 'agonied',\n",
       " 'agreed',\n",
       " 'agueweed',\n",
       " 'ahungered',\n",
       " 'aiguilletted',\n",
       " 'ailweed',\n",
       " 'airbrained',\n",
       " 'airified',\n",
       " 'aiseweed',\n",
       " 'aisled',\n",
       " 'alarmed',\n",
       " 'alated',\n",
       " 'alimonied',\n",
       " 'aliped',\n",
       " 'alleyed',\n",
       " 'allied',\n",
       " 'alligatored',\n",
       " 'allseed',\n",
       " 'almsdeed',\n",
       " 'aloed',\n",
       " 'altared',\n",
       " 'alveolated',\n",
       " 'amazed',\n",
       " 'ameed',\n",
       " 'amiced',\n",
       " 'amphitheatered',\n",
       " 'ampullated',\n",
       " 'amused',\n",
       " 'anchored',\n",
       " 'angled',\n",
       " 'anguiped',\n",
       " 'anguished',\n",
       " 'angulated',\n",
       " 'angulinerved',\n",
       " 'anhungered',\n",
       " 'animated',\n",
       " 'aniseed',\n",
       " 'annodated',\n",
       " 'annulated',\n",
       " 'anomaliped',\n",
       " 'anserated',\n",
       " 'anteflected',\n",
       " 'anteflexed',\n",
       " 'antimoniated',\n",
       " 'antimoniureted',\n",
       " 'antimoniuretted',\n",
       " 'antiquated',\n",
       " 'antired',\n",
       " 'antiweed',\n",
       " 'antlered',\n",
       " 'apertured',\n",
       " 'apexed',\n",
       " 'apicifixed',\n",
       " 'apiculated',\n",
       " 'apocopated',\n",
       " 'apostrophied',\n",
       " 'appearanced',\n",
       " 'appellatived',\n",
       " 'appendaged',\n",
       " 'appendiculated',\n",
       " 'applied',\n",
       " 'appressed',\n",
       " 'aralkylated',\n",
       " 'arbored',\n",
       " 'arched',\n",
       " 'architraved',\n",
       " 'arcked',\n",
       " 'arcuated',\n",
       " 'ared',\n",
       " 'areolated',\n",
       " 'ariled',\n",
       " 'arillated',\n",
       " 'armchaired',\n",
       " 'armed',\n",
       " 'armied',\n",
       " 'armillated',\n",
       " 'armored',\n",
       " 'armoried',\n",
       " 'arpeggiated',\n",
       " 'arpeggioed',\n",
       " 'arrased',\n",
       " 'arrowed',\n",
       " 'arrowheaded',\n",
       " 'arrowweed',\n",
       " 'arseneted',\n",
       " 'arsenetted',\n",
       " 'arseniureted',\n",
       " 'articled',\n",
       " 'articulated',\n",
       " 'ashamed',\n",
       " 'ashlared',\n",
       " 'ashweed',\n",
       " 'aspersed',\n",
       " 'asphyxied',\n",
       " 'assented',\n",
       " 'assessed',\n",
       " 'assigned',\n",
       " 'assistanted',\n",
       " 'associated',\n",
       " 'assonanced',\n",
       " 'assorted',\n",
       " 'assumed',\n",
       " 'assured',\n",
       " 'asteriated',\n",
       " 'astonied',\n",
       " 'aswooned',\n",
       " 'atrophiated',\n",
       " 'atrophied',\n",
       " 'attached',\n",
       " 'attired',\n",
       " 'attrited',\n",
       " 'augmented',\n",
       " 'aurated',\n",
       " 'auricled',\n",
       " 'auriculated',\n",
       " 'authorized',\n",
       " 'autoinhibited',\n",
       " 'autosensitized',\n",
       " 'autosled',\n",
       " 'averted',\n",
       " 'avowed',\n",
       " 'awearied',\n",
       " 'awned',\n",
       " 'awninged',\n",
       " 'axed',\n",
       " 'axhammered',\n",
       " 'axised',\n",
       " 'axled',\n",
       " 'axseed',\n",
       " 'axweed',\n",
       " 'azoted',\n",
       " 'azured',\n",
       " 'babied',\n",
       " 'babished',\n",
       " 'babyfied',\n",
       " 'baccated',\n",
       " 'backboned',\n",
       " 'backed',\n",
       " 'backhanded',\n",
       " 'backwatered',\n",
       " 'baconweed',\n",
       " 'badgerweed',\n",
       " 'bagged',\n",
       " 'bagwigged',\n",
       " 'baked',\n",
       " 'balanced',\n",
       " 'balconied',\n",
       " 'baldachined',\n",
       " 'baldricked',\n",
       " 'balled',\n",
       " 'ballweed',\n",
       " 'balsamweed',\n",
       " 'balustered',\n",
       " 'balustraded',\n",
       " 'bandannaed',\n",
       " 'banded',\n",
       " 'bandoleered',\n",
       " 'bangled',\n",
       " 'banked',\n",
       " 'bankweed',\n",
       " 'bannered',\n",
       " 'barbated',\n",
       " 'barbed',\n",
       " 'barebacked',\n",
       " 'bareboned',\n",
       " 'barefaced',\n",
       " 'barefooted',\n",
       " 'barehanded',\n",
       " 'bareheaded',\n",
       " 'barelegged',\n",
       " 'barenecked',\n",
       " 'barmybrained',\n",
       " 'barred',\n",
       " 'barreled',\n",
       " 'bartizaned',\n",
       " 'basebred',\n",
       " 'based',\n",
       " 'basehearted',\n",
       " 'basifixed',\n",
       " 'basilweed',\n",
       " 'basined',\n",
       " 'basinerved',\n",
       " 'basqued',\n",
       " 'bastioned',\n",
       " 'bated',\n",
       " 'bathroomed',\n",
       " 'battered',\n",
       " 'batteried',\n",
       " 'battled',\n",
       " 'battlemented',\n",
       " 'bayed',\n",
       " 'bayoneted',\n",
       " 'beached',\n",
       " 'beaded',\n",
       " 'beaked',\n",
       " 'bealtared',\n",
       " 'beamed',\n",
       " 'beanweed',\n",
       " 'beaproned',\n",
       " 'bearded',\n",
       " 'beautied',\n",
       " 'beavered',\n",
       " 'beballed',\n",
       " 'bebannered',\n",
       " 'bebed',\n",
       " 'bebelted',\n",
       " 'bebled',\n",
       " 'bebothered',\n",
       " 'bebouldered',\n",
       " 'bebuttoned',\n",
       " 'becassocked',\n",
       " 'bechained',\n",
       " 'bechignoned',\n",
       " 'becircled',\n",
       " 'becoiffed',\n",
       " 'becombed',\n",
       " 'becousined',\n",
       " 'becrinolined',\n",
       " 'becuffed',\n",
       " 'becurtained',\n",
       " 'becushioned',\n",
       " 'bed',\n",
       " 'bedaggered',\n",
       " 'bedangled',\n",
       " 'bedded',\n",
       " 'bediademed',\n",
       " 'bediamonded',\n",
       " 'beedged',\n",
       " 'beefheaded',\n",
       " 'beeheaded',\n",
       " 'beeswinged',\n",
       " 'beetled',\n",
       " 'beetleheaded',\n",
       " 'beetleweed',\n",
       " 'beeweed',\n",
       " 'befamilied',\n",
       " 'befanned',\n",
       " 'befathered',\n",
       " 'beferned',\n",
       " 'befetished',\n",
       " 'befezzed',\n",
       " 'befilleted',\n",
       " 'befilmed',\n",
       " 'beforested',\n",
       " 'befountained',\n",
       " 'befrocked',\n",
       " 'befrogged',\n",
       " 'befurbelowed',\n",
       " 'befurred',\n",
       " 'begabled',\n",
       " 'begarlanded',\n",
       " 'begartered',\n",
       " 'beggarweed',\n",
       " 'beglobed',\n",
       " 'begoggled',\n",
       " 'begowned',\n",
       " 'behatted',\n",
       " 'behaviored',\n",
       " 'beheadlined',\n",
       " 'behooped',\n",
       " 'beinked',\n",
       " 'bekilted',\n",
       " 'beknived',\n",
       " 'beknotted',\n",
       " 'belaced',\n",
       " 'belated',\n",
       " 'belatticed',\n",
       " 'belavendered',\n",
       " 'beledgered',\n",
       " 'belfried',\n",
       " 'beliked',\n",
       " 'belimousined',\n",
       " 'belled',\n",
       " 'bellied',\n",
       " 'bellmouthed',\n",
       " 'bellweed',\n",
       " 'beloved',\n",
       " 'belozenged',\n",
       " 'belted',\n",
       " 'bemazed',\n",
       " 'bemedaled',\n",
       " 'bemedalled',\n",
       " 'bemitered',\n",
       " 'bemitred',\n",
       " 'bemused',\n",
       " 'bemuslined',\n",
       " 'bended',\n",
       " 'beneaped',\n",
       " 'beneficed',\n",
       " 'beneighbored',\n",
       " 'benempted',\n",
       " 'benighted',\n",
       " 'bennetweed',\n",
       " 'benumbed',\n",
       " 'benweed',\n",
       " 'benzoated',\n",
       " 'benzoinated',\n",
       " 'bepastured',\n",
       " 'bepatched',\n",
       " 'beperiwigged',\n",
       " 'bepewed',\n",
       " 'bepillared',\n",
       " 'bepistoled',\n",
       " 'beplaided',\n",
       " 'beplumed',\n",
       " 'beribanded',\n",
       " 'beribboned',\n",
       " 'beringed',\n",
       " 'beringleted',\n",
       " 'berobed',\n",
       " 'berouged',\n",
       " 'berried',\n",
       " 'berthed',\n",
       " 'beruffed',\n",
       " 'beruffled',\n",
       " 'beshawled',\n",
       " 'besieged',\n",
       " 'beslushed',\n",
       " 'besotted',\n",
       " 'bespecked',\n",
       " 'bespectacled',\n",
       " 'besped',\n",
       " 'bespeed',\n",
       " 'bespelled',\n",
       " 'bespurred',\n",
       " 'bestatued',\n",
       " 'bestayed',\n",
       " 'bestrapped',\n",
       " 'bestubbled',\n",
       " 'besweatered',\n",
       " 'betattered',\n",
       " 'betaxed',\n",
       " 'betowered',\n",
       " 'betrothed',\n",
       " 'betrousered',\n",
       " 'betted',\n",
       " 'betuckered',\n",
       " 'beturbaned',\n",
       " 'betusked',\n",
       " 'betutored',\n",
       " 'betwattled',\n",
       " 'beuniformed',\n",
       " 'beveled',\n",
       " 'bevelled',\n",
       " 'bevesseled',\n",
       " 'bevesselled',\n",
       " 'bevined',\n",
       " 'bevoiled',\n",
       " 'bewaitered',\n",
       " 'bewhiskered',\n",
       " 'bewigged',\n",
       " 'bewildered',\n",
       " 'bewinged',\n",
       " 'bewired',\n",
       " 'bewrathed',\n",
       " 'biangulated',\n",
       " 'biarcuated',\n",
       " 'biarticulated',\n",
       " 'bicarbureted',\n",
       " 'biciliated',\n",
       " 'bicolored',\n",
       " 'bicorned',\n",
       " 'bidented',\n",
       " 'bifanged',\n",
       " 'bifidated',\n",
       " 'biflected',\n",
       " 'biforked',\n",
       " 'biformed',\n",
       " 'bifronted',\n",
       " 'bifurcated',\n",
       " 'bigeminated',\n",
       " 'bighearted',\n",
       " 'bigmouthed',\n",
       " 'bigoted',\n",
       " 'bigwigged',\n",
       " 'bilamellated',\n",
       " 'bilaminated',\n",
       " 'billed',\n",
       " 'bilobated',\n",
       " 'bilobed',\n",
       " 'bilsted',\n",
       " 'bimaculated',\n",
       " 'bimotored',\n",
       " 'bindweed',\n",
       " 'bineweed',\n",
       " 'binominated',\n",
       " 'binucleated',\n",
       " 'biparted',\n",
       " 'bipectinated',\n",
       " 'biped',\n",
       " 'bipennated',\n",
       " 'bipinnated',\n",
       " 'bipinnatiparted',\n",
       " 'bipinnatisected',\n",
       " 'biradiated',\n",
       " 'birdmouthed',\n",
       " 'birdseed',\n",
       " 'birdweed',\n",
       " 'birostrated',\n",
       " 'birthbed',\n",
       " 'bisexed',\n",
       " 'bishopweed',\n",
       " 'bistered',\n",
       " 'bistipuled',\n",
       " 'bisubstituted',\n",
       " 'bitted',\n",
       " 'bitterhearted',\n",
       " 'bitterweed',\n",
       " 'bituberculated',\n",
       " 'bitumed',\n",
       " 'bivalved',\n",
       " 'bivaulted',\n",
       " 'bivocalized',\n",
       " 'blackhearted',\n",
       " 'blackseed',\n",
       " 'blackshirted',\n",
       " 'bladderseed',\n",
       " 'bladderweed',\n",
       " 'bladed',\n",
       " 'blakeberyed',\n",
       " 'blamed',\n",
       " 'blanked',\n",
       " 'blanketed',\n",
       " 'blanketweed',\n",
       " 'blasted',\n",
       " 'bleached',\n",
       " 'bleared',\n",
       " 'bleed',\n",
       " 'blended',\n",
       " 'blessed',\n",
       " 'blighted',\n",
       " 'blinded',\n",
       " 'blindfolded',\n",
       " 'blindweed',\n",
       " 'blinked',\n",
       " 'blinkered',\n",
       " 'blistered',\n",
       " 'blisterweed',\n",
       " 'blithehearted',\n",
       " 'bloated',\n",
       " 'blobbed',\n",
       " 'blocked',\n",
       " 'blockheaded',\n",
       " 'blooded',\n",
       " 'bloodied',\n",
       " 'bloodshed',\n",
       " 'bloodstained',\n",
       " 'bloodweed',\n",
       " 'blossomed',\n",
       " 'blotched',\n",
       " 'bloused',\n",
       " 'blowzed',\n",
       " 'bludgeoned',\n",
       " 'bluebelled',\n",
       " 'bluehearted',\n",
       " 'blueweed',\n",
       " 'blunderheaded',\n",
       " 'blunthearted',\n",
       " 'blurred',\n",
       " 'bobbed',\n",
       " 'bobsled',\n",
       " 'bobtailed',\n",
       " 'bodiced',\n",
       " 'bodied',\n",
       " 'boiled',\n",
       " 'boldhearted',\n",
       " 'bolectioned',\n",
       " 'boled',\n",
       " 'boleweed',\n",
       " 'bolled',\n",
       " 'bombed',\n",
       " 'bonded',\n",
       " 'boned',\n",
       " 'boneheaded',\n",
       " 'bonneted',\n",
       " 'booked',\n",
       " 'booted',\n",
       " 'bootied',\n",
       " 'boozed',\n",
       " 'bordered',\n",
       " 'bordured',\n",
       " 'bosomed',\n",
       " 'bossed',\n",
       " 'bosselated',\n",
       " 'botched',\n",
       " 'botherheaded',\n",
       " 'bothsided',\n",
       " 'bottled',\n",
       " 'bottomed',\n",
       " 'boughed',\n",
       " 'bounded',\n",
       " 'bountied',\n",
       " 'bowed',\n",
       " 'boweled',\n",
       " 'bowlegged',\n",
       " 'bowstringed',\n",
       " 'braced',\n",
       " 'braceleted',\n",
       " 'brackened',\n",
       " 'bracted',\n",
       " 'braided',\n",
       " 'brambled',\n",
       " 'branched',\n",
       " 'branded',\n",
       " 'brandied',\n",
       " 'brangled',\n",
       " 'bravehearted',\n",
       " 'brawned',\n",
       " 'brazenfaced',\n",
       " 'breasted',\n",
       " 'breastweed',\n",
       " 'breathed',\n",
       " 'brecciated',\n",
       " 'bred',\n",
       " 'breeched',\n",
       " 'breed',\n",
       " 'breviped',\n",
       " 'bridebed',\n",
       " 'brideweed',\n",
       " 'bridged',\n",
       " 'bridled',\n",
       " 'briered',\n",
       " 'brimmed',\n",
       " 'bristled',\n",
       " 'broadhearted',\n",
       " 'brocaded',\n",
       " 'brocked',\n",
       " 'brokenhearted',\n",
       " 'bromoiodized',\n",
       " 'bronzed',\n",
       " 'brooked',\n",
       " 'brookweed',\n",
       " 'broomweed',\n",
       " 'broozled',\n",
       " 'browed',\n",
       " 'brownweed',\n",
       " 'bruckled',\n",
       " 'brushed',\n",
       " 'buboed',\n",
       " 'bucked',\n",
       " 'buckled',\n",
       " 'buckskinned',\n",
       " 'buffed',\n",
       " 'bugled',\n",
       " 'bugleweed',\n",
       " 'bugseed',\n",
       " 'bugweed',\n",
       " 'bulbed',\n",
       " 'bulked',\n",
       " 'bulkheaded',\n",
       " 'bullated',\n",
       " 'bulldogged',\n",
       " 'bulleted',\n",
       " 'bulletheaded',\n",
       " 'bullheaded',\n",
       " 'bullweed',\n",
       " 'bummed',\n",
       " 'bundlerooted',\n",
       " 'bundweed',\n",
       " 'bunted',\n",
       " 'buried',\n",
       " 'burled',\n",
       " 'burned',\n",
       " 'burnoosed',\n",
       " 'burntweed',\n",
       " 'burred',\n",
       " 'burroweed',\n",
       " 'burseed',\n",
       " 'burweed',\n",
       " 'bushed',\n",
       " 'busied',\n",
       " 'busked',\n",
       " 'buskined',\n",
       " 'busted',\n",
       " 'bustled',\n",
       " 'busybodied',\n",
       " 'buttered',\n",
       " 'butterfingered',\n",
       " 'butterweed',\n",
       " 'butteryfingered',\n",
       " 'buttocked',\n",
       " 'buttoned',\n",
       " 'buttonweed',\n",
       " 'cabled',\n",
       " 'caboshed',\n",
       " 'caddiced',\n",
       " 'caddised',\n",
       " 'cadenced',\n",
       " 'cadweed',\n",
       " 'caftaned',\n",
       " 'caged',\n",
       " 'cairned',\n",
       " 'caissoned',\n",
       " 'calced',\n",
       " 'calcified',\n",
       " 'calcined',\n",
       " 'calculated',\n",
       " 'calibered',\n",
       " 'calicoed',\n",
       " 'caligated',\n",
       " 'calpacked',\n",
       " 'calved',\n",
       " 'calycled',\n",
       " 'calyculated',\n",
       " 'camailed',\n",
       " 'camerated',\n",
       " 'cammed',\n",
       " 'campanulated',\n",
       " 'campshed',\n",
       " 'camused',\n",
       " 'canaliculated',\n",
       " 'cancellated',\n",
       " 'cancered',\n",
       " 'cancerweed',\n",
       " 'candied',\n",
       " 'candlelighted',\n",
       " 'candlesticked',\n",
       " 'candyweed',\n",
       " 'canioned',\n",
       " 'cankered',\n",
       " 'cankerweed',\n",
       " 'canned',\n",
       " 'cannelated',\n",
       " 'cannelured',\n",
       " 'cannoned',\n",
       " 'cannulated',\n",
       " 'canted',\n",
       " 'cantilevered',\n",
       " 'cantoned',\n",
       " 'cantred',\n",
       " 'caped',\n",
       " 'capernoited',\n",
       " 'capeweed',\n",
       " 'capitaled',\n",
       " 'capitated',\n",
       " 'capped',\n",
       " 'capriped',\n",
       " 'capsulated',\n",
       " 'capuched',\n",
       " 'carapaced',\n",
       " 'carbolated',\n",
       " 'carboyed',\n",
       " 'carbuncled',\n",
       " 'carcaneted',\n",
       " 'carded',\n",
       " 'carinated',\n",
       " 'carkled',\n",
       " 'carnaged',\n",
       " 'carnationed',\n",
       " 'carpetweed',\n",
       " 'carried',\n",
       " 'carrotweed',\n",
       " 'carucated',\n",
       " 'carunculated',\n",
       " 'cased',\n",
       " 'casemated',\n",
       " 'casemented',\n",
       " 'caseweed',\n",
       " 'casqued',\n",
       " 'castellated',\n",
       " 'castled',\n",
       " 'castorized',\n",
       " 'catamited',\n",
       " 'cataracted',\n",
       " 'catarrhed',\n",
       " 'catchweed',\n",
       " 'catenated',\n",
       " 'caterpillared',\n",
       " 'catfaced',\n",
       " 'catfooted',\n",
       " 'cathedraled',\n",
       " 'caudated',\n",
       " 'caverned',\n",
       " 'cavitied',\n",
       " 'cayenned',\n",
       " 'cedared',\n",
       " 'ceilinged',\n",
       " 'celebrated',\n",
       " 'cellated',\n",
       " 'celled',\n",
       " 'cellulated',\n",
       " 'celluloided',\n",
       " 'centered',\n",
       " 'centriffed',\n",
       " 'centuried',\n",
       " 'cerated',\n",
       " 'cered',\n",
       " 'certified',\n",
       " 'chafeweed',\n",
       " 'chaffseed',\n",
       " 'chaffweed',\n",
       " 'chafted',\n",
       " 'chained',\n",
       " 'chaliced',\n",
       " 'chambered',\n",
       " 'chamberleted',\n",
       " 'chamberletted',\n",
       " 'chanceled',\n",
       " 'channeled',\n",
       " 'channelled',\n",
       " 'chaped',\n",
       " 'chapleted',\n",
       " 'chapournetted',\n",
       " 'chapped',\n",
       " 'charioted',\n",
       " 'charqued',\n",
       " 'chartered',\n",
       " 'chasmed',\n",
       " 'chasteweed',\n",
       " 'chasubled',\n",
       " 'checked',\n",
       " 'checkered',\n",
       " 'checkrowed',\n",
       " 'cheered',\n",
       " 'cheliped',\n",
       " 'cherried',\n",
       " 'chickenbreasted',\n",
       " 'chickenhearted',\n",
       " 'chickenweed',\n",
       " 'chickweed',\n",
       " 'chicqued',\n",
       " 'chiggerweed',\n",
       " 'chignoned',\n",
       " 'childbed',\n",
       " 'childed',\n",
       " 'chilled',\n",
       " 'chined',\n",
       " 'chinned',\n",
       " 'chipped',\n",
       " 'chiseled',\n",
       " 'chitinized',\n",
       " 'chokered',\n",
       " 'chokeweed',\n",
       " 'cholterheaded',\n",
       " 'chopped',\n",
       " 'choppered',\n",
       " 'chorded',\n",
       " 'chowderheaded',\n",
       " 'christened',\n",
       " 'chubbed',\n",
       " 'chuckleheaded',\n",
       " 'churchified',\n",
       " 'churled',\n",
       " 'ciliated',\n",
       " 'cingulated',\n",
       " 'cinnamoned',\n",
       " 'cinquefoiled',\n",
       " 'circled',\n",
       " 'circumscribed',\n",
       " 'circumstanced',\n",
       " 'cirrated',\n",
       " 'cirrhosed',\n",
       " 'cirriped',\n",
       " 'cisted',\n",
       " 'citied',\n",
       " 'citified',\n",
       " 'citrated',\n",
       " 'civilized',\n",
       " 'clammed',\n",
       " 'clammyweed',\n",
       " 'clanned',\n",
       " 'clapped',\n",
       " 'classed',\n",
       " 'classified',\n",
       " 'clavated',\n",
       " 'clavellated',\n",
       " 'clawed',\n",
       " 'claybrained',\n",
       " 'clayweed',\n",
       " 'cleaded',\n",
       " 'cleanhanded',\n",
       " 'cleanhearted',\n",
       " 'clearheaded',\n",
       " 'clearhearted',\n",
       " 'clearweed',\n",
       " 'cled',\n",
       " 'cleeked',\n",
       " 'clefted',\n",
       " 'clerestoried',\n",
       " 'cliented',\n",
       " 'cliffed',\n",
       " 'cliffweed',\n",
       " 'clipped',\n",
       " 'cloaked',\n",
       " 'clocked',\n",
       " 'clodpated',\n",
       " 'cloistered',\n",
       " 'closed',\n",
       " 'closefisted',\n",
       " 'closehanded',\n",
       " 'closehearted',\n",
       " 'closemouthed',\n",
       " 'clotweed',\n",
       " 'clouded',\n",
       " 'clouted',\n",
       " 'clovered',\n",
       " 'clubbed',\n",
       " 'clubfisted',\n",
       " 'clubfooted',\n",
       " 'clubweed',\n",
       " 'clustered',\n",
       " 'coaged',\n",
       " 'coaggregated',\n",
       " 'coated',\n",
       " 'coattailed',\n",
       " 'cobbed',\n",
       " 'cocashweed',\n",
       " 'cochleated',\n",
       " 'cockaded',\n",
       " 'cocked',\n",
       " 'cockeyed',\n",
       " 'cockled',\n",
       " 'cockneybred',\n",
       " 'cockscombed',\n",
       " 'cockweed',\n",
       " 'codheaded',\n",
       " 'coed',\n",
       " 'coelongated',\n",
       " 'coembedded',\n",
       " 'coequated',\n",
       " 'coexpanded',\n",
       " 'coffeeweed',\n",
       " 'cogged',\n",
       " 'coifed',\n",
       " 'coiled',\n",
       " 'coldhearted',\n",
       " 'coleseed',\n",
       " 'colicweed',\n",
       " 'collared',\n",
       " 'collected',\n",
       " 'collied',\n",
       " 'colloped',\n",
       " 'colonnaded',\n",
       " 'colored',\n",
       " 'columnated',\n",
       " 'columned',\n",
       " 'combed',\n",
       " 'combined',\n",
       " 'compacted',\n",
       " 'complected',\n",
       " 'complexioned',\n",
       " 'complicated',\n",
       " 'componed',\n",
       " 'componented',\n",
       " 'composed',\n",
       " 'compressed',\n",
       " 'comprised',\n",
       " 'compulsed',\n",
       " 'conamed',\n",
       " 'concamerated',\n",
       " 'concealed',\n",
       " 'conceded',\n",
       " 'conceited',\n",
       " 'concentrated',\n",
       " 'concerned',\n",
       " 'concerted',\n",
       " 'conched',\n",
       " 'conchyliated',\n",
       " 'condemned',\n",
       " 'condensed',\n",
       " 'conditioned',\n",
       " 'conduplicated',\n",
       " 'coned',\n",
       " 'confated',\n",
       " 'conferted',\n",
       " 'confined',\n",
       " 'confirmed',\n",
       " 'conflated',\n",
       " 'confounded',\n",
       " 'confused',\n",
       " 'congested',\n",
       " 'conjoined',\n",
       " 'conjugated',\n",
       " 'connected',\n",
       " 'conred',\n",
       " 'consecrated',\n",
       " 'considered',\n",
       " 'consolidated',\n",
       " 'constrained',\n",
       " 'constricted',\n",
       " 'consumpted',\n",
       " 'contagioned',\n",
       " 'contented',\n",
       " 'contextured',\n",
       " 'continued',\n",
       " 'contorted',\n",
       " 'contortioned',\n",
       " 'contracted',\n",
       " 'contractured',\n",
       " 'contusioned',\n",
       " 'converted',\n",
       " 'convexed',\n",
       " 'convinced',\n",
       " 'convoluted',\n",
       " 'coolheaded',\n",
       " 'coolweed',\n",
       " 'copied',\n",
       " 'copleased',\n",
       " 'copped',\n",
       " 'coppernosed',\n",
       " 'copperytailed',\n",
       " 'coppiced',\n",
       " 'coppled',\n",
       " 'copsewooded',\n",
       " 'copygraphed',\n",
       " 'coraled',\n",
       " 'corded',\n",
       " 'corduroyed',\n",
       " 'cored',\n",
       " 'coreflexed',\n",
       " 'corked',\n",
       " 'cornered',\n",
       " 'cornified',\n",
       " 'cornuated',\n",
       " 'cornuted',\n",
       " 'corollated',\n",
       " 'coronaled',\n",
       " 'coronated',\n",
       " 'coroneted',\n",
       " 'coronetted',\n",
       " 'corpusculated',\n",
       " 'corrected',\n",
       " 'correlated',\n",
       " 'corridored',\n",
       " ...]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Words which end in -ed\n",
    "[w for w in wordlist if re.search('ed$', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abjectly',\n",
       " 'adjuster',\n",
       " 'dejected',\n",
       " 'dejectly',\n",
       " 'injector',\n",
       " 'majestic',\n",
       " 'objectee',\n",
       " 'objector',\n",
       " 'rejecter',\n",
       " 'rejector',\n",
       " 'unjilted',\n",
       " 'unjolted',\n",
       " 'unjustly']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The . wildcard will math any character\n",
    "# ^ matches the start of the string and $ matches the end\n",
    "# Find all words with 8 letters, which 3rd letter is a j and 6th letter is a t\n",
    "[w for w in wordlist if re.search('^..j..t..$', w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranges and Closures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gold', 'golf', 'hold', 'hole']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [] represents a disjunction (anything matching a character or combination of\n",
    "#characters inside the brackets)\n",
    "# Words that could be produced pressing same keys on an old cellpone\n",
    "# By pressing 4653\n",
    "[w for w in wordlist if re.search('^[ghi][mno][jlk][def]$', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['miiiiiiiiiiiiinnnnnnnnnnneeeeeeeeee',\n",
       " 'miiiiiinnnnnnnnnneeeeeeee',\n",
       " 'mine',\n",
       " 'mmmmmmmmiiiiiiiiinnnnnnnnneeeeeeee']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'aaaaaaaaaaaaaaaaa',\n",
       " 'aaahhhh',\n",
       " 'ah',\n",
       " 'ahah',\n",
       " 'ahahah',\n",
       " 'ahh',\n",
       " 'ahhahahaha',\n",
       " 'ahhh',\n",
       " 'ahhhh',\n",
       " 'ahhhhhh',\n",
       " 'ahhhhhhhhhhhhhh',\n",
       " 'h',\n",
       " 'ha',\n",
       " 'haaa',\n",
       " 'hah',\n",
       " 'haha',\n",
       " 'hahaaa',\n",
       " 'hahah',\n",
       " 'hahaha',\n",
       " 'hahahaa',\n",
       " 'hahahah',\n",
       " 'hahahaha',\n",
       " 'hahahahaaa',\n",
       " 'hahahahahaha',\n",
       " 'hahahahahahaha',\n",
       " 'hahahahahahahahahahahahahahahaha',\n",
       " 'hahahhahah',\n",
       " 'hahhahahaha']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# + matches a single or more ocurrences of the previous character or\n",
    "#disjunction\n",
    "chat_words = sorted(set(w for w in nltk.corpus.nps_chat.words()))\n",
    "[w for w in chat_words if re.search('^m+i+n+e+$', w)]\n",
    "\n",
    "\n",
    "[w for w in chat_words if re.search('^[ha]+$', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.0085',\n",
       " '0.05',\n",
       " '0.1',\n",
       " '0.16',\n",
       " '0.2',\n",
       " '0.25',\n",
       " '0.28',\n",
       " '0.3',\n",
       " '0.4',\n",
       " '0.5',\n",
       " '0.50',\n",
       " '0.54',\n",
       " '0.56',\n",
       " '0.60',\n",
       " '0.7',\n",
       " '0.82',\n",
       " '0.84',\n",
       " '0.9',\n",
       " '0.95',\n",
       " '0.99',\n",
       " '1.01',\n",
       " '1.1',\n",
       " '1.125',\n",
       " '1.14',\n",
       " '1.1650',\n",
       " '1.17',\n",
       " '1.18',\n",
       " '1.19',\n",
       " '1.2',\n",
       " '1.20',\n",
       " '1.24',\n",
       " '1.25',\n",
       " '1.26',\n",
       " '1.28',\n",
       " '1.35',\n",
       " '1.39',\n",
       " '1.4',\n",
       " '1.457',\n",
       " '1.46',\n",
       " '1.49',\n",
       " '1.5',\n",
       " '1.50',\n",
       " '1.55',\n",
       " '1.56',\n",
       " '1.5755',\n",
       " '1.5805',\n",
       " '1.6',\n",
       " '1.61',\n",
       " '1.637',\n",
       " '1.64',\n",
       " '1.65',\n",
       " '1.7',\n",
       " '1.75',\n",
       " '1.76',\n",
       " '1.8',\n",
       " '1.82',\n",
       " '1.8415',\n",
       " '1.85',\n",
       " '1.8500',\n",
       " '1.9',\n",
       " '1.916',\n",
       " '1.92',\n",
       " '10.19',\n",
       " '10.2',\n",
       " '10.5',\n",
       " '107.03',\n",
       " '107.9',\n",
       " '109.73',\n",
       " '11.10',\n",
       " '11.5',\n",
       " '11.57',\n",
       " '11.6',\n",
       " '11.72',\n",
       " '11.95',\n",
       " '112.9',\n",
       " '113.2',\n",
       " '116.3',\n",
       " '116.4',\n",
       " '116.7',\n",
       " '116.9',\n",
       " '118.6',\n",
       " '12.09',\n",
       " '12.5',\n",
       " '12.52',\n",
       " '12.68',\n",
       " '12.7',\n",
       " '12.82',\n",
       " '12.97',\n",
       " '120.7',\n",
       " '1206.26',\n",
       " '121.6',\n",
       " '126.1',\n",
       " '126.15',\n",
       " '127.03',\n",
       " '129.91',\n",
       " '13.1',\n",
       " '13.15',\n",
       " '13.5',\n",
       " '13.50',\n",
       " '13.625',\n",
       " '13.65',\n",
       " '13.73',\n",
       " '13.8',\n",
       " '13.90',\n",
       " '130.6',\n",
       " '130.7',\n",
       " '131.01',\n",
       " '132.9',\n",
       " '133.7',\n",
       " '133.8',\n",
       " '14.00',\n",
       " '14.13',\n",
       " '14.26',\n",
       " '14.28',\n",
       " '14.43',\n",
       " '14.5',\n",
       " '14.53',\n",
       " '14.54',\n",
       " '14.6',\n",
       " '14.75',\n",
       " '14.99',\n",
       " '141.9',\n",
       " '142.84',\n",
       " '142.85',\n",
       " '143.08',\n",
       " '143.80',\n",
       " '143.93',\n",
       " '148.9',\n",
       " '149.9',\n",
       " '15.5',\n",
       " '150.00',\n",
       " '153.3',\n",
       " '154.2',\n",
       " '16.05',\n",
       " '16.09',\n",
       " '16.125',\n",
       " '16.2',\n",
       " '16.5',\n",
       " '16.68',\n",
       " '16.7',\n",
       " '16.9',\n",
       " '169.9',\n",
       " '17.3',\n",
       " '17.4',\n",
       " '17.5',\n",
       " '17.95',\n",
       " '1738.1',\n",
       " '176.1',\n",
       " '18.3',\n",
       " '18.6',\n",
       " '18.95',\n",
       " '185.9',\n",
       " '188.84',\n",
       " '19.3',\n",
       " '19.50',\n",
       " '19.6',\n",
       " '19.94',\n",
       " '19.95',\n",
       " '191.9',\n",
       " '2.07',\n",
       " '2.1',\n",
       " '2.15',\n",
       " '2.19',\n",
       " '2.2',\n",
       " '2.25',\n",
       " '2.29',\n",
       " '2.3',\n",
       " '2.30',\n",
       " '2.35',\n",
       " '2.375',\n",
       " '2.4',\n",
       " '2.42',\n",
       " '2.44',\n",
       " '2.46',\n",
       " '2.47',\n",
       " '2.5',\n",
       " '2.50',\n",
       " '2.6',\n",
       " '2.62',\n",
       " '2.65',\n",
       " '2.7',\n",
       " '2.75',\n",
       " '2.8',\n",
       " '2.80',\n",
       " '2.87',\n",
       " '2.875',\n",
       " '2.9',\n",
       " '2.95',\n",
       " '20.07',\n",
       " '20.5',\n",
       " '21.1',\n",
       " '21.9',\n",
       " '2141.7',\n",
       " '2160.1',\n",
       " '2163.2',\n",
       " '22.75',\n",
       " '220.45',\n",
       " '221.4',\n",
       " '225.6',\n",
       " '23.25',\n",
       " '23.4',\n",
       " '23.5',\n",
       " '23.72',\n",
       " '234.4',\n",
       " '236.74',\n",
       " '236.79',\n",
       " '24.95',\n",
       " '25.50',\n",
       " '25.6',\n",
       " '251.2',\n",
       " '26.2',\n",
       " '26.5',\n",
       " '26.8',\n",
       " '263.07',\n",
       " '2645.90',\n",
       " '2691.19',\n",
       " '27.1',\n",
       " '27.4',\n",
       " '273.5',\n",
       " '278.7',\n",
       " '28.25',\n",
       " '28.36',\n",
       " '28.4',\n",
       " '28.5',\n",
       " '28.53',\n",
       " '28.6',\n",
       " '29.3',\n",
       " '29.4',\n",
       " '29.9',\n",
       " '292.32',\n",
       " '3.01',\n",
       " '3.04',\n",
       " '3.1',\n",
       " '3.16',\n",
       " '3.18',\n",
       " '3.19',\n",
       " '3.2',\n",
       " '3.20',\n",
       " '3.23',\n",
       " '3.253',\n",
       " '3.28',\n",
       " '3.3',\n",
       " '3.35',\n",
       " '3.375',\n",
       " '3.4',\n",
       " '3.42',\n",
       " '3.43',\n",
       " '3.5',\n",
       " '3.55',\n",
       " '3.6',\n",
       " '3.61',\n",
       " '3.625',\n",
       " '3.7',\n",
       " '3.75',\n",
       " '3.8',\n",
       " '3.80',\n",
       " '3.9',\n",
       " '30.6',\n",
       " '30.9',\n",
       " '319.75',\n",
       " '32.8',\n",
       " '334.5',\n",
       " '34.625',\n",
       " '341.20',\n",
       " '3436.58',\n",
       " '35.2',\n",
       " '35.7',\n",
       " '352.7',\n",
       " '352.9',\n",
       " '35500.64',\n",
       " '35564.43',\n",
       " '36.9',\n",
       " '361.8',\n",
       " '3648.82',\n",
       " '37.3',\n",
       " '37.5',\n",
       " '372.14',\n",
       " '372.9',\n",
       " '374.19',\n",
       " '374.20',\n",
       " '377.60',\n",
       " '38.3',\n",
       " '38.375',\n",
       " '38.5',\n",
       " '38.875',\n",
       " '387.8',\n",
       " '4.1',\n",
       " '4.10',\n",
       " '4.2',\n",
       " '4.25',\n",
       " '4.3',\n",
       " '4.4',\n",
       " '4.5',\n",
       " '4.55',\n",
       " '4.6',\n",
       " '4.7',\n",
       " '4.75',\n",
       " '4.8',\n",
       " '4.875',\n",
       " '4.898',\n",
       " '4.9',\n",
       " '40.21',\n",
       " '41.60',\n",
       " '415.6',\n",
       " '415.8',\n",
       " '42.1',\n",
       " '42.5',\n",
       " '422.5',\n",
       " '43.875',\n",
       " '434.4',\n",
       " '436.01',\n",
       " '446.62',\n",
       " '449.04',\n",
       " '45.2',\n",
       " '45.3',\n",
       " '45.75',\n",
       " '456.64',\n",
       " '46.1',\n",
       " '47.1',\n",
       " '47.125',\n",
       " '47.5',\n",
       " '47.6',\n",
       " '49.9',\n",
       " '494.50',\n",
       " '497.34',\n",
       " '5.1',\n",
       " '5.2180',\n",
       " '5.276',\n",
       " '5.29',\n",
       " '5.3',\n",
       " '5.39',\n",
       " '5.4',\n",
       " '5.435',\n",
       " '5.5',\n",
       " '5.57',\n",
       " '5.6',\n",
       " '5.63',\n",
       " '5.7',\n",
       " '5.70',\n",
       " '5.8',\n",
       " '5.82',\n",
       " '5.9',\n",
       " '5.92',\n",
       " '50.1',\n",
       " '50.38',\n",
       " '50.45',\n",
       " '51.25',\n",
       " '51.6',\n",
       " '55.1',\n",
       " '566.54',\n",
       " '57.50',\n",
       " '57.6',\n",
       " '57.7',\n",
       " '58.64',\n",
       " '59.6',\n",
       " '59.9',\n",
       " '6.03',\n",
       " '6.1',\n",
       " '6.20',\n",
       " '6.21',\n",
       " '6.25',\n",
       " '6.4',\n",
       " '6.40',\n",
       " '6.44',\n",
       " '6.5',\n",
       " '6.50',\n",
       " '6.53',\n",
       " '6.6',\n",
       " '6.7',\n",
       " '6.70',\n",
       " '6.79',\n",
       " '6.84',\n",
       " '6.9',\n",
       " '60.36',\n",
       " '618.1',\n",
       " '62.1',\n",
       " '62.5',\n",
       " '62.625',\n",
       " '63.79',\n",
       " '630.9',\n",
       " '64.5',\n",
       " '66.5',\n",
       " '7.15',\n",
       " '7.2',\n",
       " '7.20',\n",
       " '7.272',\n",
       " '7.3',\n",
       " '7.4',\n",
       " '7.40',\n",
       " '7.422',\n",
       " '7.45',\n",
       " '7.458',\n",
       " '7.5',\n",
       " '7.50',\n",
       " '7.52',\n",
       " '7.55',\n",
       " '7.60',\n",
       " '7.62',\n",
       " '7.63',\n",
       " '7.65',\n",
       " '7.74',\n",
       " '7.78',\n",
       " '7.79',\n",
       " '7.8',\n",
       " '7.80',\n",
       " '7.84',\n",
       " '7.88',\n",
       " '7.90',\n",
       " '7.95',\n",
       " '70.2',\n",
       " '70.7',\n",
       " '705.6',\n",
       " '72.7',\n",
       " '734.9',\n",
       " '737.5',\n",
       " '77.56',\n",
       " '77.6',\n",
       " '77.70',\n",
       " '8.04',\n",
       " '8.06',\n",
       " '8.07',\n",
       " '8.1',\n",
       " '8.12',\n",
       " '8.14',\n",
       " '8.15',\n",
       " '8.19',\n",
       " '8.2',\n",
       " '8.22',\n",
       " '8.25',\n",
       " '8.30',\n",
       " '8.35',\n",
       " '8.45',\n",
       " '8.467',\n",
       " '8.47',\n",
       " '8.48',\n",
       " '8.5',\n",
       " '8.50',\n",
       " '8.53',\n",
       " '8.55',\n",
       " '8.56',\n",
       " '8.575',\n",
       " '8.60',\n",
       " '8.64',\n",
       " '8.65',\n",
       " '8.70',\n",
       " '8.75',\n",
       " '8.9',\n",
       " '80.50',\n",
       " '80.8',\n",
       " '81.8',\n",
       " '811.9',\n",
       " '83.4',\n",
       " '84.29',\n",
       " '84.9',\n",
       " '85.1',\n",
       " '85.7',\n",
       " '86.12',\n",
       " '87.5',\n",
       " '88.32',\n",
       " '89.7',\n",
       " '89.9',\n",
       " '9.3',\n",
       " '9.32',\n",
       " '9.37',\n",
       " '9.45',\n",
       " '9.5',\n",
       " '9.625',\n",
       " '9.75',\n",
       " '9.8',\n",
       " '9.82',\n",
       " '9.9',\n",
       " '92.9',\n",
       " '93.3',\n",
       " '93.9',\n",
       " '94.2',\n",
       " '94.8',\n",
       " '95.09',\n",
       " '96.4',\n",
       " '98.3',\n",
       " '99.1',\n",
       " '99.3']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['C$', 'US$']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['1614',\n",
       " '1637',\n",
       " '1787',\n",
       " '1901',\n",
       " '1903',\n",
       " '1917',\n",
       " '1925',\n",
       " '1929',\n",
       " '1933',\n",
       " '1934',\n",
       " '1948',\n",
       " '1953',\n",
       " '1955',\n",
       " '1956',\n",
       " '1961',\n",
       " '1965',\n",
       " '1966',\n",
       " '1967',\n",
       " '1968',\n",
       " '1969',\n",
       " '1970',\n",
       " '1971',\n",
       " '1972',\n",
       " '1973',\n",
       " '1975',\n",
       " '1976',\n",
       " '1977',\n",
       " '1979',\n",
       " '1980',\n",
       " '1981',\n",
       " '1982',\n",
       " '1983',\n",
       " '1984',\n",
       " '1985',\n",
       " '1986',\n",
       " '1987',\n",
       " '1988',\n",
       " '1989',\n",
       " '1990',\n",
       " '1991',\n",
       " '1992',\n",
       " '1993',\n",
       " '1994',\n",
       " '1995',\n",
       " '1996',\n",
       " '1997',\n",
       " '1998',\n",
       " '1999',\n",
       " '2000',\n",
       " '2005',\n",
       " '2009',\n",
       " '2017',\n",
       " '2019',\n",
       " '2029',\n",
       " '3057',\n",
       " '8300']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['10-day',\n",
       " '10-lap',\n",
       " '10-year',\n",
       " '100-share',\n",
       " '12-point',\n",
       " '12-year',\n",
       " '14-hour',\n",
       " '15-day',\n",
       " '150-point',\n",
       " '190-point',\n",
       " '20-point',\n",
       " '20-stock',\n",
       " '21-month',\n",
       " '237-seat',\n",
       " '240-page',\n",
       " '27-year',\n",
       " '30-day',\n",
       " '30-point',\n",
       " '30-share',\n",
       " '30-year',\n",
       " '300-day',\n",
       " '36-day',\n",
       " '36-store',\n",
       " '42-year',\n",
       " '50-state',\n",
       " '500-stock',\n",
       " '52-week',\n",
       " '69-point',\n",
       " '84-month',\n",
       " '87-store',\n",
       " '90-day']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['black-and-white',\n",
       " 'bread-and-butter',\n",
       " 'father-in-law',\n",
       " 'machine-gun-toting',\n",
       " 'savings-and-loan']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['62%-owned',\n",
       " 'Absorbed',\n",
       " 'According',\n",
       " 'Adopting',\n",
       " 'Advanced',\n",
       " 'Advancing',\n",
       " 'Alfred',\n",
       " 'Allied',\n",
       " 'Annualized',\n",
       " 'Anything',\n",
       " 'Arbitrage-related',\n",
       " 'Arbitraging',\n",
       " 'Asked',\n",
       " 'Assuming',\n",
       " 'Atlanta-based',\n",
       " 'Baking',\n",
       " 'Banking',\n",
       " 'Beginning',\n",
       " 'Beijing',\n",
       " 'Being',\n",
       " 'Bermuda-based',\n",
       " 'Betting',\n",
       " 'Boeing',\n",
       " 'Broadcasting',\n",
       " 'Bucking',\n",
       " 'Buying',\n",
       " 'Calif.-based',\n",
       " 'Change-ringing',\n",
       " 'Citing',\n",
       " 'Concerned',\n",
       " 'Confronted',\n",
       " 'Conn.based',\n",
       " 'Consolidated',\n",
       " 'Continued',\n",
       " 'Continuing',\n",
       " 'Declining',\n",
       " 'Defending',\n",
       " 'Depending',\n",
       " 'Designated',\n",
       " 'Determining',\n",
       " 'Developed',\n",
       " 'Died',\n",
       " 'During',\n",
       " 'Encouraged',\n",
       " 'Encouraging',\n",
       " 'English-speaking',\n",
       " 'Estimated',\n",
       " 'Everything',\n",
       " 'Excluding',\n",
       " 'Exxon-owned',\n",
       " 'Faulding',\n",
       " 'Fed',\n",
       " 'Feeding',\n",
       " 'Filling',\n",
       " 'Filmed',\n",
       " 'Financing',\n",
       " 'Following',\n",
       " 'Founded',\n",
       " 'Fracturing',\n",
       " 'Francisco-based',\n",
       " 'Fred',\n",
       " 'Funded',\n",
       " 'Funding',\n",
       " 'Generalized',\n",
       " 'Germany-based',\n",
       " 'Getting',\n",
       " 'Guaranteed',\n",
       " 'Having',\n",
       " 'Heating',\n",
       " 'Heightened',\n",
       " 'Holding',\n",
       " 'Housing',\n",
       " 'Illuminating',\n",
       " 'Indeed',\n",
       " 'Indexing',\n",
       " 'Irving',\n",
       " 'Jersey-based',\n",
       " 'Judging',\n",
       " 'Knowing',\n",
       " 'Learning',\n",
       " 'Legislating',\n",
       " 'Leming',\n",
       " 'Limited',\n",
       " 'London-based',\n",
       " 'Manfred',\n",
       " 'Manufacturing',\n",
       " 'Melamed',\n",
       " 'Miami-based',\n",
       " 'Mich.-based',\n",
       " 'Mining',\n",
       " 'Minneapolis-based',\n",
       " 'Mo.-based',\n",
       " 'Mortgage-Backed',\n",
       " 'Moving',\n",
       " 'Muzzling',\n",
       " 'N.J.-based',\n",
       " 'NBC-owned',\n",
       " 'NIH-appointed',\n",
       " 'Named',\n",
       " 'No-Smoking',\n",
       " 'Observing',\n",
       " 'Offering',\n",
       " 'Ohio-based',\n",
       " 'Orleans-based',\n",
       " 'Packaging',\n",
       " 'Performing',\n",
       " 'Philadelphia-based',\n",
       " 'Posted',\n",
       " 'Provided',\n",
       " 'Publishing',\n",
       " 'Purchasing',\n",
       " 'Rated',\n",
       " 'Reached',\n",
       " 'Red',\n",
       " 'Red-blooded',\n",
       " 'Reducing',\n",
       " 'Reed',\n",
       " 'Regarded',\n",
       " 'Rekindled',\n",
       " 'Related',\n",
       " 'Ringing',\n",
       " 'Rolling',\n",
       " 'Sacramento-based',\n",
       " 'Scoring',\n",
       " 'Seattle-based',\n",
       " 'Seed',\n",
       " 'Skilled',\n",
       " 'Smelting',\n",
       " 'Something',\n",
       " 'Spending',\n",
       " 'Standardized',\n",
       " 'Standing',\n",
       " 'Starting',\n",
       " 'Sterling',\n",
       " 'Taking',\n",
       " 'Texas-based',\n",
       " 'Toronto-based',\n",
       " 'Traded',\n",
       " 'Trading',\n",
       " 'Troubled',\n",
       " 'U.N.-supervised',\n",
       " 'U.S.-backed',\n",
       " 'United',\n",
       " 'Used',\n",
       " 'Varying',\n",
       " 'Washington-based',\n",
       " 'Whiting',\n",
       " 'Wilfred',\n",
       " 'Winning',\n",
       " 'Xiaoping',\n",
       " 'York-based',\n",
       " 'Zayed',\n",
       " 'abandoned',\n",
       " 'abating',\n",
       " 'abolishing',\n",
       " 'abortion-related',\n",
       " 'abounding',\n",
       " 'abridging',\n",
       " 'absorbed',\n",
       " 'acceded',\n",
       " 'accelerated',\n",
       " 'accepted',\n",
       " 'accepting',\n",
       " 'according',\n",
       " 'accounted',\n",
       " 'accounting',\n",
       " 'accrued',\n",
       " 'accumulated',\n",
       " 'accused',\n",
       " 'accusing',\n",
       " 'achieved',\n",
       " 'achieving',\n",
       " 'acknowledging',\n",
       " 'acquired',\n",
       " 'acquiring',\n",
       " 'acquisition-minded',\n",
       " 'acted',\n",
       " 'acting',\n",
       " 'adapted',\n",
       " 'adapting',\n",
       " 'added',\n",
       " 'adding',\n",
       " 'addressing',\n",
       " 'adjusted',\n",
       " 'adjusting',\n",
       " 'admitted',\n",
       " 'admitting',\n",
       " 'adopted',\n",
       " 'advanced',\n",
       " 'advancing',\n",
       " 'advertised',\n",
       " 'advertising',\n",
       " 'advised',\n",
       " 'advocated',\n",
       " 'advocating',\n",
       " 'affecting',\n",
       " 'afflicted',\n",
       " 'aggravated',\n",
       " 'agreed',\n",
       " 'agreeing',\n",
       " 'ailing',\n",
       " 'aimed',\n",
       " 'aiming',\n",
       " 'aired',\n",
       " 'airline-related',\n",
       " 'alarmed',\n",
       " 'alienated',\n",
       " 'alleged',\n",
       " 'alleging',\n",
       " 'allocated',\n",
       " 'allowed',\n",
       " 'altered',\n",
       " 'altering',\n",
       " 'amended',\n",
       " 'amending',\n",
       " 'amounted',\n",
       " 'amusing',\n",
       " 'angered',\n",
       " 'announced',\n",
       " 'annoyed',\n",
       " 'annualized',\n",
       " 'answered',\n",
       " 'anti-dumping',\n",
       " 'anticipated',\n",
       " 'anticipating',\n",
       " 'anything',\n",
       " 'apologizing',\n",
       " 'appealing',\n",
       " 'appeared',\n",
       " 'appearing',\n",
       " 'applied',\n",
       " 'appointed',\n",
       " 'approached',\n",
       " 'appropriated',\n",
       " 'approved',\n",
       " 'arched',\n",
       " 'argued',\n",
       " 'arguing',\n",
       " 'arising',\n",
       " 'armed',\n",
       " 'arranged',\n",
       " 'arrested',\n",
       " 'arrived',\n",
       " 'asbestos-related',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'assassinated',\n",
       " 'assembled',\n",
       " 'asserted',\n",
       " 'asserting',\n",
       " 'assessed',\n",
       " 'assigned',\n",
       " 'assisted',\n",
       " 'associated',\n",
       " 'assumed',\n",
       " 'assuming',\n",
       " 'assured',\n",
       " 'attached',\n",
       " 'attacking',\n",
       " 'attempted',\n",
       " 'attempting',\n",
       " 'attended',\n",
       " 'attending',\n",
       " 'attracted',\n",
       " 'attracting',\n",
       " 'attributed',\n",
       " 'auctioned',\n",
       " 'authorized',\n",
       " 'authorizing',\n",
       " 'automated',\n",
       " 'automotive-lighting',\n",
       " 'averaged',\n",
       " 'averted',\n",
       " 'avoiding',\n",
       " 'awarded',\n",
       " 'awarding',\n",
       " 'backed',\n",
       " 'backing',\n",
       " 'balanced',\n",
       " 'bald-faced',\n",
       " 'balkanized',\n",
       " 'balked',\n",
       " 'balloting',\n",
       " 'bank-backed',\n",
       " 'banking',\n",
       " 'banned',\n",
       " 'banning',\n",
       " 'barking',\n",
       " 'barred',\n",
       " 'based',\n",
       " 'battered',\n",
       " 'battery-operated',\n",
       " 'batting',\n",
       " 'bearing',\n",
       " 'becoming',\n",
       " 'bedding',\n",
       " 'befuddled',\n",
       " 'beginning',\n",
       " 'behaving',\n",
       " 'beheading',\n",
       " 'being',\n",
       " 'beleaguered',\n",
       " 'believed',\n",
       " 'bell-ringing',\n",
       " 'belonging',\n",
       " 'benefited',\n",
       " 'best-selling',\n",
       " 'betting',\n",
       " 'bickering',\n",
       " 'bidding',\n",
       " 'billed',\n",
       " 'billing',\n",
       " 'blamed',\n",
       " 'bled',\n",
       " 'blessing',\n",
       " 'blighted',\n",
       " 'blocked',\n",
       " 'blurred',\n",
       " 'boarding',\n",
       " 'bolstered',\n",
       " 'bombarding',\n",
       " 'booked',\n",
       " 'booming',\n",
       " 'boosted',\n",
       " 'boosting',\n",
       " 'borrowed',\n",
       " 'borrowing',\n",
       " 'botched',\n",
       " 'bothered',\n",
       " 'bounced',\n",
       " 'bowed',\n",
       " 'breaking',\n",
       " 'breathed',\n",
       " 'breathtaking',\n",
       " 'breed',\n",
       " 'bribed',\n",
       " 'bribing',\n",
       " 'briefing',\n",
       " 'brightened',\n",
       " 'bring',\n",
       " 'bringing',\n",
       " 'broad-based',\n",
       " 'broadcasting',\n",
       " 'broadened',\n",
       " 'brokering',\n",
       " 'brushed',\n",
       " 'budding',\n",
       " 'building',\n",
       " 'bundling',\n",
       " 'buoyed',\n",
       " 'burned',\n",
       " 'buying',\n",
       " 'calculated',\n",
       " 'called',\n",
       " 'calling',\n",
       " 'campaigning',\n",
       " 'cancer-causing',\n",
       " 'capitalized',\n",
       " 'capped',\n",
       " 'captivating',\n",
       " 'cared',\n",
       " 'carried',\n",
       " 'carrying',\n",
       " 'cascading',\n",
       " 'casting',\n",
       " 'caused',\n",
       " 'causing',\n",
       " 'cautioned',\n",
       " 'ceiling',\n",
       " 'centralized',\n",
       " 'certified',\n",
       " 'chaired',\n",
       " 'challenging',\n",
       " 'championing',\n",
       " 'change-ringing',\n",
       " 'changed',\n",
       " 'changing',\n",
       " 'characterized',\n",
       " 'characterizing',\n",
       " 'charged',\n",
       " 'charging',\n",
       " 'chastised',\n",
       " 'cheating',\n",
       " 'checking',\n",
       " 'cheerleading',\n",
       " 'chilled',\n",
       " 'choosing',\n",
       " 'chopped',\n",
       " 'circulated',\n",
       " 'cited',\n",
       " 'citing',\n",
       " 'citizen-sparked',\n",
       " 'city-owned',\n",
       " 'claimed',\n",
       " 'claiming',\n",
       " 'clamped',\n",
       " 'clarified',\n",
       " 'clashed',\n",
       " 'classed',\n",
       " 'classified',\n",
       " 'cleaned',\n",
       " 'cleaner-burning',\n",
       " 'cleared',\n",
       " 'clearing',\n",
       " 'clicked',\n",
       " 'climbed',\n",
       " 'climbing',\n",
       " 'clipped',\n",
       " 'clobbered',\n",
       " 'closed',\n",
       " 'closing',\n",
       " 'clothing',\n",
       " 'clouding',\n",
       " 'cluttered',\n",
       " 'co-founded',\n",
       " 'coaching',\n",
       " 'coal-fired',\n",
       " 'coated',\n",
       " 'codified',\n",
       " 'collaborated',\n",
       " 'collapsed',\n",
       " 'collected',\n",
       " 'collecting',\n",
       " 'collective-bargaining',\n",
       " 'colored',\n",
       " 'combined',\n",
       " 'coming',\n",
       " 'commanded',\n",
       " 'commenting',\n",
       " 'committed',\n",
       " 'committing',\n",
       " 'compared',\n",
       " 'compelling',\n",
       " 'competed',\n",
       " 'competing',\n",
       " 'compiled',\n",
       " 'complained',\n",
       " 'complaining',\n",
       " 'completed',\n",
       " 'completing',\n",
       " 'complicated',\n",
       " 'composed',\n",
       " 'composting',\n",
       " 'compressed',\n",
       " 'computer-aided',\n",
       " 'computer-assisted',\n",
       " 'computer-generated',\n",
       " 'computerized',\n",
       " 'computing',\n",
       " 'conceding',\n",
       " 'concentrated',\n",
       " 'concentrating',\n",
       " 'concerned',\n",
       " 'concluded',\n",
       " 'condemned',\n",
       " 'condemning',\n",
       " 'conducted',\n",
       " 'conducting',\n",
       " 'confined',\n",
       " 'confirmed',\n",
       " 'confused',\n",
       " 'connected',\n",
       " 'consented',\n",
       " 'considered',\n",
       " 'considering',\n",
       " 'consisting',\n",
       " 'construed',\n",
       " 'consulting',\n",
       " 'contacted',\n",
       " 'contained',\n",
       " 'containing',\n",
       " 'contesting',\n",
       " 'continued',\n",
       " 'continuing',\n",
       " 'contracted',\n",
       " 'contributed',\n",
       " 'contributing',\n",
       " 'controlled',\n",
       " 'controlling',\n",
       " 'converted',\n",
       " 'converting',\n",
       " 'convicted',\n",
       " 'convinced',\n",
       " 'cooled',\n",
       " 'cooperating',\n",
       " 'copied',\n",
       " 'copying',\n",
       " 'corn-buying',\n",
       " 'corrected',\n",
       " 'correcting',\n",
       " 'cost-cutting',\n",
       " 'cost-sharing',\n",
       " 'counseling',\n",
       " 'counting',\n",
       " 'coupled',\n",
       " 'court-ordered',\n",
       " 'covered',\n",
       " 'covering',\n",
       " 'cranked',\n",
       " 'crashing',\n",
       " 'created',\n",
       " 'creating',\n",
       " 'credit-rating',\n",
       " 'crippled',\n",
       " 'criticized',\n",
       " 'crossed',\n",
       " 'crossing',\n",
       " 'crowded',\n",
       " 'cruising',\n",
       " 'crushed',\n",
       " 'crying',\n",
       " 'cultivated',\n",
       " 'curbed',\n",
       " 'curbing',\n",
       " 'curled',\n",
       " 'current-carrying',\n",
       " 'curtailed',\n",
       " 'cushioned',\n",
       " 'customized',\n",
       " 'cutting',\n",
       " 'damaged',\n",
       " 'damaging',\n",
       " 'dancing',\n",
       " 'darned',\n",
       " 'dashed',\n",
       " 'dating',\n",
       " 'dead-eyed',\n",
       " 'dealing',\n",
       " 'decided',\n",
       " 'declared',\n",
       " 'declaring',\n",
       " 'declined',\n",
       " 'declining',\n",
       " 'decorated',\n",
       " 'decried',\n",
       " 'deducting',\n",
       " 'deemed',\n",
       " 'defeated',\n",
       " 'defended',\n",
       " 'defined',\n",
       " 'defying',\n",
       " 'delayed',\n",
       " 'deliberating',\n",
       " 'delisted',\n",
       " 'delivered',\n",
       " 'delivering',\n",
       " 'demanding',\n",
       " 'demonstrating',\n",
       " 'denied',\n",
       " 'denouncing',\n",
       " 'denying',\n",
       " 'depended',\n",
       " 'depending',\n",
       " 'depleted',\n",
       " 'depressed',\n",
       " 'deprived',\n",
       " 'derived',\n",
       " 'descending',\n",
       " 'described',\n",
       " 'deserving',\n",
       " 'designated',\n",
       " 'designed',\n",
       " 'designing',\n",
       " 'desired',\n",
       " 'despised',\n",
       " 'detailed',\n",
       " 'deteriorated',\n",
       " 'deteriorating',\n",
       " 'determined',\n",
       " 'deterring',\n",
       " 'devastating',\n",
       " 'developed',\n",
       " 'developing',\n",
       " 'devised',\n",
       " 'devoted',\n",
       " 'devouring',\n",
       " 'diagnosed',\n",
       " 'died',\n",
       " 'diluted',\n",
       " 'diming',\n",
       " 'diminished',\n",
       " 'directed',\n",
       " 'directing',\n",
       " 'disaffected',\n",
       " 'disagreed',\n",
       " 'disappointed',\n",
       " 'disappointing',\n",
       " 'disapproved',\n",
       " 'discarded',\n",
       " 'disciplined',\n",
       " 'disclosed',\n",
       " 'disclosing',\n",
       " 'discontinued',\n",
       " 'discontinuing',\n",
       " 'discouraging',\n",
       " 'discovered',\n",
       " 'discussed',\n",
       " 'discussing',\n",
       " 'disembodied',\n",
       " 'dismayed',\n",
       " 'dismissed',\n",
       " 'disposed',\n",
       " 'disputed',\n",
       " 'disseminating',\n",
       " 'distinguished',\n",
       " 'distorted',\n",
       " 'distributed',\n",
       " 'disturbing',\n",
       " 'diversified',\n",
       " 'diversifying',\n",
       " 'divided',\n",
       " 'dividing',\n",
       " 'documented',\n",
       " 'doing',\n",
       " 'doling',\n",
       " 'dollar-denominated',\n",
       " 'dominated',\n",
       " 'dominating',\n",
       " 'doubled',\n",
       " 'doubted',\n",
       " 'downgraded',\n",
       " 'downgrading',\n",
       " 'drafted',\n",
       " 'drawing',\n",
       " 'dreamed',\n",
       " 'dressed',\n",
       " 'drifted',\n",
       " 'drinking',\n",
       " 'driving',\n",
       " 'drooled',\n",
       " 'dropped',\n",
       " 'dubbed',\n",
       " 'duckling',\n",
       " 'dumbfounded',\n",
       " 'dumped',\n",
       " 'during',\n",
       " 'dwindling',\n",
       " 'earned',\n",
       " 'earning',\n",
       " 'eased',\n",
       " 'easing',\n",
       " 'eating',\n",
       " 'echoed',\n",
       " 'edged',\n",
       " 'editing',\n",
       " 'educated',\n",
       " 'elected',\n",
       " 'eliminated',\n",
       " 'eliminating',\n",
       " 'embarrassing',\n",
       " 'embroiled',\n",
       " 'emerged',\n",
       " 'emerging',\n",
       " 'emphasized',\n",
       " 'employed',\n",
       " 'empowered',\n",
       " 'enabled',\n",
       " 'enabling',\n",
       " 'enacted',\n",
       " 'encircling',\n",
       " 'enclosed',\n",
       " 'encouraging',\n",
       " 'encroaching',\n",
       " 'ended',\n",
       " 'ending',\n",
       " 'endorsed',\n",
       " 'engaged',\n",
       " 'engaging',\n",
       " 'engineered',\n",
       " 'engineering',\n",
       " 'enhanced',\n",
       " 'enjoyed',\n",
       " 'enjoying',\n",
       " 'enlarged',\n",
       " 'enraged',\n",
       " 'ensnarled',\n",
       " 'entangled',\n",
       " 'entered',\n",
       " 'entering',\n",
       " 'entertaining',\n",
       " 'enticed',\n",
       " 'entitled',\n",
       " 'entrenched',\n",
       " 'entrusted',\n",
       " 'equaling',\n",
       " 'equipped',\n",
       " 'escalated',\n",
       " 'escaped',\n",
       " 'established',\n",
       " 'establishing',\n",
       " 'estimated',\n",
       " 'evaluated',\n",
       " 'evaluating',\n",
       " 'evaporated',\n",
       " 'evening',\n",
       " 'everything',\n",
       " 'evoking',\n",
       " 'evolved',\n",
       " 'exacerbated',\n",
       " 'examined',\n",
       " 'exceed',\n",
       " 'exceeded',\n",
       " 'exceeding',\n",
       " 'exchanging',\n",
       " 'excited',\n",
       " 'exciting',\n",
       " 'executed',\n",
       " 'executing',\n",
       " 'exercised',\n",
       " 'exerting',\n",
       " 'exhausted',\n",
       " 'exhibited',\n",
       " 'existed',\n",
       " 'existing',\n",
       " 'expanded',\n",
       " 'expanding',\n",
       " 'expected',\n",
       " 'expecting',\n",
       " 'expedited',\n",
       " 'expelled',\n",
       " 'experienced',\n",
       " 'experiencing',\n",
       " 'expired',\n",
       " 'explained',\n",
       " 'explaining',\n",
       " 'exploded',\n",
       " 'export-oriented',\n",
       " 'exposed',\n",
       " 'expressed',\n",
       " 'expressing',\n",
       " 'expunged',\n",
       " 'extended',\n",
       " 'extending',\n",
       " 'exuded',\n",
       " 'eyeing',\n",
       " 'fabled',\n",
       " 'faced',\n",
       " 'facing',\n",
       " 'factoring',\n",
       " 'faded',\n",
       " 'failed',\n",
       " 'failing',\n",
       " 'fainting',\n",
       " 'falling',\n",
       " 'faltered',\n",
       " 'famed',\n",
       " 'family-planning',\n",
       " 'fared',\n",
       " 'fashioned',\n",
       " 'fast-growing',\n",
       " 'fastest-growing',\n",
       " 'fattened',\n",
       " 'favored',\n",
       " 'fawning',\n",
       " 'feared',\n",
       " 'featured',\n",
       " 'featuring',\n",
       " 'fed',\n",
       " 'feed',\n",
       " 'feeling',\n",
       " 'fetching',\n",
       " 'fielded',\n",
       " 'fighting',\n",
       " 'filed',\n",
       " 'filing',\n",
       " 'filled',\n",
       " 'filling',\n",
       " 'finalized',\n",
       " 'financed',\n",
       " 'financing',\n",
       " 'finding',\n",
       " 'fined',\n",
       " 'finished',\n",
       " 'fired',\n",
       " 'firmed',\n",
       " 'fixed',\n",
       " 'fizzled',\n",
       " 'fled',\n",
       " 'fledgling',\n",
       " 'fleeting',\n",
       " 'flirted',\n",
       " 'floated',\n",
       " 'flooded',\n",
       " 'focused',\n",
       " 'focusing',\n",
       " 'folded',\n",
       " 'followed',\n",
       " 'following',\n",
       " 'forced',\n",
       " 'forcing',\n",
       " 'forecasting',\n",
       " 'foreign-led',\n",
       " 'formed',\n",
       " 'forthcoming',\n",
       " 'founded',\n",
       " 'foundering',\n",
       " 'fretted',\n",
       " 'frightened',\n",
       " 'frustrating',\n",
       " 'fueled',\n",
       " 'fueling',\n",
       " 'full-fledged',\n",
       " 'fuming',\n",
       " 'functioning',\n",
       " 'funded',\n",
       " 'funding',\n",
       " 'fundraising',\n",
       " 'futures-related',\n",
       " 'gained',\n",
       " 'gaining',\n",
       " 'galling',\n",
       " 'galvanized',\n",
       " 'gambling',\n",
       " 'gauging',\n",
       " 'generated',\n",
       " 'getting',\n",
       " 'giving',\n",
       " 'going',\n",
       " 'good-hearted',\n",
       " 'good-natured',\n",
       " 'gored',\n",
       " 'government-certified',\n",
       " 'government-funded',\n",
       " 'government-owned',\n",
       " 'graduated',\n",
       " 'granted',\n",
       " 'granting',\n",
       " 'greed',\n",
       " 'gripping',\n",
       " 'growing',\n",
       " 'guaranteed',\n",
       " 'guarding',\n",
       " 'guided',\n",
       " 'gut-wrenching',\n",
       " 'hailed',\n",
       " 'hailing',\n",
       " 'halted',\n",
       " 'hampered',\n",
       " 'handed',\n",
       " 'handled',\n",
       " 'handling',\n",
       " 'happened',\n",
       " 'happening',\n",
       " 'hard-charging',\n",
       " 'hard-drinking',\n",
       " 'hard-hitting',\n",
       " 'harmed',\n",
       " 'harped',\n",
       " 'harvested',\n",
       " 'hauled',\n",
       " 'hauling',\n",
       " 'having',\n",
       " 'headed',\n",
       " 'heading',\n",
       " 'headlined',\n",
       " 'healing',\n",
       " 'hearing',\n",
       " 'heated',\n",
       " 'heating',\n",
       " 'hedging',\n",
       " 'heightened',\n",
       " 'helped',\n",
       " 'helping',\n",
       " 'high-flying',\n",
       " 'high-minded',\n",
       " 'high-polluting',\n",
       " 'high-priced',\n",
       " 'high-rolling',\n",
       " 'high-speed',\n",
       " 'higher-salaried',\n",
       " 'highest-pitched',\n",
       " 'hired',\n",
       " 'hitting',\n",
       " 'holding',\n",
       " 'hoped',\n",
       " 'hosted',\n",
       " 'housing',\n",
       " 'hugging',\n",
       " 'hundred',\n",
       " 'hunted',\n",
       " 'hurting',\n",
       " 'identified',\n",
       " 'ignored',\n",
       " 'ignoring',\n",
       " 'impaired',\n",
       " 'impeding',\n",
       " 'impending',\n",
       " 'implemented',\n",
       " 'implied',\n",
       " 'imported',\n",
       " 'imposed',\n",
       " 'imposing',\n",
       " 'impressed',\n",
       " 'improved',\n",
       " 'improving',\n",
       " 'incentive-backed',\n",
       " 'inched',\n",
       " 'inching',\n",
       " 'included',\n",
       " 'including',\n",
       " 'incorporated',\n",
       " 'increased',\n",
       " 'increasing',\n",
       " 'incurred',\n",
       " 'indeed',\n",
       " 'index-related',\n",
       " 'indicated',\n",
       " 'indicating',\n",
       " 'indulging',\n",
       " 'industrialized',\n",
       " 'industry-supported',\n",
       " 'inflated',\n",
       " 'influenced',\n",
       " 'influencing',\n",
       " 'infringed',\n",
       " 'inherited',\n",
       " 'initialing',\n",
       " 'initiated',\n",
       " 'initiating',\n",
       " 'injecting',\n",
       " 'injuring',\n",
       " 'inkling',\n",
       " 'inquiring',\n",
       " 'inserted',\n",
       " 'insider-trading',\n",
       " 'insinuating',\n",
       " 'insisted',\n",
       " 'inspired',\n",
       " 'installed',\n",
       " 'installing',\n",
       " 'instituted',\n",
       " 'instructed',\n",
       " 'insured',\n",
       " 'integrated',\n",
       " 'intended',\n",
       " 'intentioned',\n",
       " 'interest-bearing',\n",
       " 'interested',\n",
       " 'interesting',\n",
       " 'interrogated',\n",
       " 'interviewed',\n",
       " 'intriguing',\n",
       " 'introduced',\n",
       " 'introducing',\n",
       " 'invented',\n",
       " 'inverted',\n",
       " 'invested',\n",
       " 'investigating',\n",
       " 'investing',\n",
       " 'inviting',\n",
       " 'involved',\n",
       " 'involving',\n",
       " 'issued',\n",
       " 'issuing',\n",
       " 'jeopardizing',\n",
       " 'joined',\n",
       " 'joining',\n",
       " 'judged',\n",
       " 'jumped',\n",
       " 'jumping',\n",
       " 'justified',\n",
       " 'justifying',\n",
       " 'keeping',\n",
       " 'kicked',\n",
       " 'kidnapping',\n",
       " 'killed',\n",
       " 'killing',\n",
       " 'knitted',\n",
       " 'knocked',\n",
       " 'labeled',\n",
       " 'labeling',\n",
       " 'labor-backed',\n",
       " 'lacked',\n",
       " 'lagging',\n",
       " 'land-idling',\n",
       " 'landing',\n",
       " 'lasted',\n",
       " 'lasting',\n",
       " 'lauded',\n",
       " 'laughing',\n",
       " 'launched',\n",
       " 'lawmaking',\n",
       " 'laying',\n",
       " 'leading',\n",
       " 'learned',\n",
       " 'learning',\n",
       " 'leasing',\n",
       " 'leaving',\n",
       " 'led',\n",
       " 'lending',\n",
       " 'lengthened',\n",
       " 'lessening',\n",
       " 'letter-writing',\n",
       " 'letting',\n",
       " 'leveling',\n",
       " 'leveraged',\n",
       " 'leveraging',\n",
       " 'licensed',\n",
       " 'licensing',\n",
       " 'lifted',\n",
       " ...]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [^ ] is an exclusion disjunction. The ^ inside brackets is different than\n",
    "#a ^ outside brackets\n",
    "\n",
    "wsj = sorted(set(nltk.corpus.treebank.words()))\n",
    "\n",
    "# \\ is used to deactivate wildcards and use them as literals\n",
    "[w for w in wsj if re.search('^[0-9]+\\.[0-9]+$', w)]\n",
    "\n",
    "[w for w in wsj if re.search('^[A-Z]+\\$$', w)]\n",
    "\n",
    "# {} limits the length of preceding matches\n",
    "[w for w in wsj if re.search('^[0-9]{4}$', w)]\n",
    "\n",
    "[w for w in wsj if re.search('^[0-9]+-[a-z]{3,5}$', w)]\n",
    " \n",
    "[w for w in wsj if re.search('^[a-z]{5,}-[a-z]{2,3}-[a-z]{,6}$', w)]\n",
    "\n",
    "# () is scope and | is an XOR for whole substrings \n",
    "[w for w in wsj if re.search('(ed|ing)$', w)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Applications of Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Word Pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['u',\n",
       " 'e',\n",
       " 'a',\n",
       " 'i',\n",
       " 'a',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'e',\n",
       " 'i',\n",
       " 'a',\n",
       " 'i',\n",
       " 'o',\n",
       " 'i',\n",
       " 'o',\n",
       " 'u']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using findall to find all matches with no overlap\n",
    "word = 'supercalifragilisticexpialidocious'\n",
    "re.findall(r'[aeiou]', word)\n",
    "len(re.findall(r'[aeiou]', word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('io', 549),\n",
       " ('ea', 476),\n",
       " ('ie', 331),\n",
       " ('ou', 329),\n",
       " ('ai', 261),\n",
       " ('ia', 253),\n",
       " ('ee', 217),\n",
       " ('oo', 174),\n",
       " ('ua', 109),\n",
       " ('au', 106),\n",
       " ('ue', 105),\n",
       " ('ui', 95),\n",
       " ('ei', 86),\n",
       " ('oi', 65),\n",
       " ('oa', 59),\n",
       " ('eo', 39),\n",
       " ('iou', 27),\n",
       " ('eu', 18),\n",
       " ('oe', 15),\n",
       " ('iu', 14),\n",
       " ('ae', 11),\n",
       " ('eau', 10),\n",
       " ('uo', 8),\n",
       " ('oui', 6),\n",
       " ('ao', 6),\n",
       " ('eou', 5),\n",
       " ('uou', 5),\n",
       " ('uee', 4),\n",
       " ('aa', 3),\n",
       " ('ieu', 3),\n",
       " ('uie', 3),\n",
       " ('eei', 2),\n",
       " ('iai', 1),\n",
       " ('oei', 1),\n",
       " ('uu', 1),\n",
       " ('aii', 1),\n",
       " ('aiia', 1),\n",
       " ('aia', 1),\n",
       " ('iao', 1),\n",
       " ('eea', 1),\n",
       " ('ueui', 1),\n",
       " ('ioa', 1),\n",
       " ('ooi', 1)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding th frequency of a sequence of two or more vowels in words from a text\n",
    "wsj = sorted(set(nltk.corpus.treebank.words()))\n",
    "fd = nltk.FreqDist(vs for word in wsj\n",
    "                       for vs in re.findall(r'[aeiou]{2,}', word))\n",
    "fd.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2009, 12, 31]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[int(n) for n in re.findall(r'[0-9]{2,}', '2009-12-31')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing More with Word Pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unvrsl Dclrtn of Hmn Rghts Prmble Whrs rcgntn of the inhrnt dgnty and\n",
      "of the eql and inlnble rghts of all mmbrs of the hmn fmly is the fndtn\n",
      "of frdm , jstce and pce in the wrld , Whrs dsrgrd and cntmpt fr hmn\n",
      "rghts hve rsltd in brbrs acts whch hve outrgd the cnscnce of mnknd ,\n",
      "and the advnt of a wrld in whch hmn bngs shll enjy frdm of spch and\n"
     ]
    }
   ],
   "source": [
    "# Keeping starting and ending vowels and discarding internal vowels\n",
    "\n",
    "regexp = r'^[AEIOUaeiou]+|[AEIOUaeiou]+$|[^AEIOUaeiou]'\n",
    "def compress(word):\n",
    "    pieces = re.findall(regexp, word)\n",
    "    return ''.join(pieces)\n",
    "\n",
    "english_udhr = nltk.corpus.udhr.words('English-Latin1')\n",
    "print(nltk.tokenwrap(compress(w) for w in english_udhr[:75]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    a   e   i   o   u \n",
      "k 418 148  94 420 173 \n",
      "p  83  31 105  34  51 \n",
      "r 187  63  84  89  79 \n",
      "s   0   0 100   2   1 \n",
      "t  47   8   0 148  37 \n",
      "v  93  27 105  48  49 \n"
     ]
    }
   ],
   "source": [
    "# Conditional Frequency Distribution of consonant and vowel in Rotokas\n",
    "rotokas_words = nltk.corpus.toolbox.words('rotokas.dic')\n",
    "\n",
    "# cvs is a non-overlap list of consonant followed by vowel\n",
    "cvs = [cv for w in rotokas_words for cv in re.findall(r'[ptksvr][aeiou]', w)]\n",
    "cfd = nltk.ConditionalFreqDist(cvs)\n",
    "cfd.tabulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kasuari']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['kaapo',\n",
       " 'kaapopato',\n",
       " 'kaipori',\n",
       " 'kaiporipie',\n",
       " 'kaiporivira',\n",
       " 'kapo',\n",
       " 'kapoa',\n",
       " 'kapokao',\n",
       " 'kapokapo',\n",
       " 'kapokapo',\n",
       " 'kapokapoa',\n",
       " 'kapokapoa',\n",
       " 'kapokapora',\n",
       " 'kapokapora',\n",
       " 'kapokaporo',\n",
       " 'kapokaporo',\n",
       " 'kapokari',\n",
       " 'kapokarito',\n",
       " 'kapokoa',\n",
       " 'kapoo',\n",
       " 'kapooto',\n",
       " 'kapoovira',\n",
       " 'kapopaa',\n",
       " 'kaporo',\n",
       " 'kaporo',\n",
       " 'kaporopa',\n",
       " 'kaporoto',\n",
       " 'kapoto',\n",
       " 'karokaropo',\n",
       " 'karopo',\n",
       " 'kepo',\n",
       " 'kepoi',\n",
       " 'keposi',\n",
       " 'kepoto']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making a dictionary of lists of the conditional words\n",
    "cv_word_pairs = [(cv, w) for w in rotokas_words\n",
    "                          for cv in re.findall(r'[ptksvr][aeiou]', w)]\n",
    "cv_index = nltk.Index(cv_word_pairs)\n",
    "cv_index['su']\n",
    "\n",
    "cv_index['po']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ka', 'kaa'),\n",
       " ('ka', 'kaa'),\n",
       " ('ka', 'kaa'),\n",
       " ('ka', 'kaakaaro'),\n",
       " ('ka', 'kaakaaro'),\n",
       " ('ro', 'kaakaaro'),\n",
       " ('ka', 'kaakaaviko'),\n",
       " ('ka', 'kaakaaviko'),\n",
       " ('vi', 'kaakaaviko'),\n",
       " ('ko', 'kaakaaviko'),\n",
       " ('ka', 'kaakaavo'),\n",
       " ('ka', 'kaakaavo'),\n",
       " ('vo', 'kaakaavo'),\n",
       " ('ka', 'kaakaoko'),\n",
       " ('ka', 'kaakaoko'),\n",
       " ('ko', 'kaakaoko'),\n",
       " ('ka', 'kaakasi'),\n",
       " ('ka', 'kaakasi'),\n",
       " ('si', 'kaakasi'),\n",
       " ('ka', 'kaakau'),\n",
       " ('ka', 'kaakau'),\n",
       " ('ka', 'kaakauko'),\n",
       " ('ka', 'kaakauko'),\n",
       " ('ko', 'kaakauko'),\n",
       " ('ka', 'kaakito'),\n",
       " ('ki', 'kaakito'),\n",
       " ('to', 'kaakito'),\n",
       " ('ka', 'kaakuupato'),\n",
       " ('ku', 'kaakuupato'),\n",
       " ('pa', 'kaakuupato'),\n",
       " ('to', 'kaakuupato'),\n",
       " ('ka', 'kaaova'),\n",
       " ('va', 'kaaova'),\n",
       " ('ka', 'kaapa'),\n",
       " ('pa', 'kaapa'),\n",
       " ('ka', 'kaapea'),\n",
       " ('pe', 'kaapea'),\n",
       " ('ka', 'kaapie'),\n",
       " ('pi', 'kaapie'),\n",
       " ('ka', 'kaapie'),\n",
       " ('pi', 'kaapie'),\n",
       " ('ka', 'kaapiepato'),\n",
       " ('pi', 'kaapiepato'),\n",
       " ('pa', 'kaapiepato'),\n",
       " ('to', 'kaapiepato'),\n",
       " ('ka', 'kaapisi'),\n",
       " ('pi', 'kaapisi'),\n",
       " ('si', 'kaapisi'),\n",
       " ('ka', 'kaapisivira'),\n",
       " ('pi', 'kaapisivira'),\n",
       " ('si', 'kaapisivira'),\n",
       " ('vi', 'kaapisivira'),\n",
       " ('ra', 'kaapisivira'),\n",
       " ('ka', 'kaapo'),\n",
       " ('po', 'kaapo'),\n",
       " ('ka', 'kaapopato'),\n",
       " ('po', 'kaapopato'),\n",
       " ('pa', 'kaapopato'),\n",
       " ('to', 'kaapopato'),\n",
       " ('ka', 'kaara'),\n",
       " ('ra', 'kaara'),\n",
       " ('ka', 'kaare'),\n",
       " ('re', 'kaare'),\n",
       " ('ka', 'kaareko'),\n",
       " ('re', 'kaareko'),\n",
       " ('ko', 'kaareko'),\n",
       " ('ka', 'kaarekopie'),\n",
       " ('re', 'kaarekopie'),\n",
       " ('ko', 'kaarekopie'),\n",
       " ('pi', 'kaarekopie'),\n",
       " ('ka', 'kaareto'),\n",
       " ('re', 'kaareto'),\n",
       " ('to', 'kaareto'),\n",
       " ('re', 'Kaareva'),\n",
       " ('va', 'Kaareva'),\n",
       " ('ka', 'kaava'),\n",
       " ('va', 'kaava'),\n",
       " ('ka', 'kaavaaua'),\n",
       " ('va', 'kaavaaua'),\n",
       " ('ka', 'kaaveaka'),\n",
       " ('ve', 'kaaveaka'),\n",
       " ('ka', 'kaaveaka'),\n",
       " ('ka', 'kaaveakapie'),\n",
       " ('ve', 'kaaveakapie'),\n",
       " ('ka', 'kaaveakapie'),\n",
       " ('pi', 'kaaveakapie'),\n",
       " ('ka', 'kaaveakapievira'),\n",
       " ('ve', 'kaaveakapievira'),\n",
       " ('ka', 'kaaveakapievira'),\n",
       " ('pi', 'kaaveakapievira'),\n",
       " ('vi', 'kaaveakapievira'),\n",
       " ('ra', 'kaaveakapievira'),\n",
       " ('ka', 'kaaveakavira'),\n",
       " ('ve', 'kaaveakavira'),\n",
       " ('ka', 'kaaveakavira'),\n",
       " ('vi', 'kaaveakavira'),\n",
       " ('ra', 'kaaveakavira'),\n",
       " ('ka', 'kae'),\n",
       " ('ka', 'kae'),\n",
       " ('ka', 'kaekae'),\n",
       " ('ka', 'kaekae'),\n",
       " ('ka', 'kaekae'),\n",
       " ('ka', 'kaekae'),\n",
       " ('ka', 'kaekaearo'),\n",
       " ('ka', 'kaekaearo'),\n",
       " ('ro', 'kaekaearo'),\n",
       " ('ka', 'kaekaeo'),\n",
       " ('ka', 'kaekaeo'),\n",
       " ('ka', 'kaekaesoto'),\n",
       " ('ka', 'kaekaesoto'),\n",
       " ('so', 'kaekaesoto'),\n",
       " ('to', 'kaekaesoto'),\n",
       " ('ka', 'kaekaevira'),\n",
       " ('ka', 'kaekaevira'),\n",
       " ('vi', 'kaekaevira'),\n",
       " ('ra', 'kaekaevira'),\n",
       " ('ka', 'kaekeru'),\n",
       " ('ke', 'kaekeru'),\n",
       " ('ru', 'kaekeru'),\n",
       " ('ka', 'kaepaa'),\n",
       " ('pa', 'kaepaa'),\n",
       " ('ka', 'kaepie'),\n",
       " ('pi', 'kaepie'),\n",
       " ('ka', 'kaepie'),\n",
       " ('pi', 'kaepie'),\n",
       " ('ka', 'kaepievira'),\n",
       " ('pi', 'kaepievira'),\n",
       " ('vi', 'kaepievira'),\n",
       " ('ra', 'kaepievira'),\n",
       " ('ka', 'kaereasi'),\n",
       " ('re', 'kaereasi'),\n",
       " ('si', 'kaereasi'),\n",
       " ('ka', 'kaereasivira'),\n",
       " ('re', 'kaereasivira'),\n",
       " ('si', 'kaereasivira'),\n",
       " ('vi', 'kaereasivira'),\n",
       " ('ra', 'kaereasivira'),\n",
       " ('ka', 'kaetu'),\n",
       " ('tu', 'kaetu'),\n",
       " ('ka', 'kaetupie'),\n",
       " ('tu', 'kaetupie'),\n",
       " ('pi', 'kaetupie'),\n",
       " ('ka', 'kaetuvira'),\n",
       " ('tu', 'kaetuvira'),\n",
       " ('vi', 'kaetuvira'),\n",
       " ('ra', 'kaetuvira'),\n",
       " ('ka', 'kaeviro'),\n",
       " ('vi', 'kaeviro'),\n",
       " ('ro', 'kaeviro'),\n",
       " ('ka', 'kagave'),\n",
       " ('ve', 'kagave'),\n",
       " ('ka', 'kaie'),\n",
       " ('ka', 'kaiea'),\n",
       " ('ka', 'kaikaio'),\n",
       " ('ka', 'kaikaio'),\n",
       " ('ka', 'kaipori'),\n",
       " ('po', 'kaipori'),\n",
       " ('ri', 'kaipori'),\n",
       " ('ka', 'kaiporipie'),\n",
       " ('po', 'kaiporipie'),\n",
       " ('ri', 'kaiporipie'),\n",
       " ('pi', 'kaiporipie'),\n",
       " ('ka', 'kaiporivira'),\n",
       " ('po', 'kaiporivira'),\n",
       " ('ri', 'kaiporivira'),\n",
       " ('vi', 'kaiporivira'),\n",
       " ('ra', 'kaiporivira'),\n",
       " ('ka', 'kairi'),\n",
       " ('ri', 'kairi'),\n",
       " ('ka', 'kairiro'),\n",
       " ('ri', 'kairiro'),\n",
       " ('ro', 'kairiro'),\n",
       " ('ka', 'kairo'),\n",
       " ('ro', 'kairo'),\n",
       " ('ka', 'kaita'),\n",
       " ('ta', 'kaita'),\n",
       " ('ka', 'kaitutu'),\n",
       " ('tu', 'kaitutu'),\n",
       " ('tu', 'kaitutu'),\n",
       " ('ka', 'kaitutupie'),\n",
       " ('tu', 'kaitutupie'),\n",
       " ('tu', 'kaitutupie'),\n",
       " ('pi', 'kaitutupie'),\n",
       " ('ka', 'kaitutuvira'),\n",
       " ('tu', 'kaitutuvira'),\n",
       " ('tu', 'kaitutuvira'),\n",
       " ('vi', 'kaitutuvira'),\n",
       " ('ra', 'kaitutuvira'),\n",
       " ('ka', 'kakae'),\n",
       " ('ka', 'kakae'),\n",
       " ('ka', 'kakae'),\n",
       " ('ka', 'kakae'),\n",
       " ('ka', 'kakae'),\n",
       " ('ka', 'kakae'),\n",
       " ('ka', 'kakaevira'),\n",
       " ('ka', 'kakaevira'),\n",
       " ('vi', 'kakaevira'),\n",
       " ('ra', 'kakaevira'),\n",
       " ('ka', 'kakapikoa'),\n",
       " ('ka', 'kakapikoa'),\n",
       " ('pi', 'kakapikoa'),\n",
       " ('ko', 'kakapikoa'),\n",
       " ('ka', 'kakapikoto'),\n",
       " ('ka', 'kakapikoto'),\n",
       " ('pi', 'kakapikoto'),\n",
       " ('ko', 'kakapikoto'),\n",
       " ('to', 'kakapikoto'),\n",
       " ('ka', 'kakapu'),\n",
       " ('ka', 'kakapu'),\n",
       " ('pu', 'kakapu'),\n",
       " ('ka', 'kakapua'),\n",
       " ('ka', 'kakapua'),\n",
       " ('pu', 'kakapua'),\n",
       " ('ka', 'kakara'),\n",
       " ('ka', 'kakara'),\n",
       " ('ra', 'kakara'),\n",
       " ('ka', 'Kakarapaia'),\n",
       " ('ra', 'Kakarapaia'),\n",
       " ('pa', 'Kakarapaia'),\n",
       " ('ka', 'kakarau'),\n",
       " ('ka', 'kakarau'),\n",
       " ('ra', 'kakarau'),\n",
       " ('ka', 'Kakarera'),\n",
       " ('re', 'Kakarera'),\n",
       " ('ra', 'Kakarera'),\n",
       " ('ka', 'kakata'),\n",
       " ('ka', 'kakata'),\n",
       " ('ta', 'kakata'),\n",
       " ('ka', 'kakate'),\n",
       " ('ka', 'kakate'),\n",
       " ('te', 'kakate'),\n",
       " ('ka', 'kakatuara'),\n",
       " ('ka', 'kakatuara'),\n",
       " ('tu', 'kakatuara'),\n",
       " ('ra', 'kakatuara'),\n",
       " ('ka', 'kakau'),\n",
       " ('ka', 'kakau'),\n",
       " ('ka', 'kakauoa'),\n",
       " ('ka', 'kakauoa'),\n",
       " ('ka', 'kakavea'),\n",
       " ('ka', 'kakavea'),\n",
       " ('ve', 'kakavea'),\n",
       " ('ka', 'kakavoro'),\n",
       " ('ka', 'kakavoro'),\n",
       " ('vo', 'kakavoro'),\n",
       " ('ro', 'kakavoro'),\n",
       " ('ka', 'kakavu'),\n",
       " ('ka', 'kakavu'),\n",
       " ('vu', 'kakavu'),\n",
       " ('ka', 'kakeoto'),\n",
       " ('ke', 'kakeoto'),\n",
       " ('to', 'kakeoto'),\n",
       " ('ka', 'kaki'),\n",
       " ('ki', 'kaki'),\n",
       " ('ka', 'kaki'),\n",
       " ('ki', 'kaki'),\n",
       " ('ka', 'kakiaki'),\n",
       " ('ki', 'kakiaki'),\n",
       " ('ki', 'kakiaki'),\n",
       " ('ka', 'kakiri'),\n",
       " ('ki', 'kakiri'),\n",
       " ('ri', 'kakiri'),\n",
       " ('ka', 'kakiua'),\n",
       " ('ki', 'kakiua'),\n",
       " ('ka', 'kaku'),\n",
       " ('ku', 'kaku'),\n",
       " ('ka', 'kakua'),\n",
       " ('ku', 'kakua'),\n",
       " ('ka', 'kakuaku'),\n",
       " ('ku', 'kakuaku'),\n",
       " ('ku', 'kakuaku'),\n",
       " ('ka', 'kakupaa'),\n",
       " ('ku', 'kakupaa'),\n",
       " ('pa', 'kakupaa'),\n",
       " ('ka', 'kakuparei'),\n",
       " ('ku', 'kakuparei'),\n",
       " ('pa', 'kakuparei'),\n",
       " ('re', 'kakuparei'),\n",
       " ('ka', 'kakupato'),\n",
       " ('ku', 'kakupato'),\n",
       " ('pa', 'kakupato'),\n",
       " ('to', 'kakupato'),\n",
       " ('ka', 'kakupie'),\n",
       " ('ku', 'kakupie'),\n",
       " ('pi', 'kakupie'),\n",
       " ('ka', 'kakupute'),\n",
       " ('ku', 'kakupute'),\n",
       " ('pu', 'kakupute'),\n",
       " ('te', 'kakupute'),\n",
       " ('ka', 'kakutauo'),\n",
       " ('ku', 'kakutauo'),\n",
       " ('ta', 'kakutauo'),\n",
       " ('ka', 'kakuto'),\n",
       " ('ku', 'kakuto'),\n",
       " ('to', 'kakuto'),\n",
       " ('ka', 'kakutuiato'),\n",
       " ('ku', 'kakutuiato'),\n",
       " ('tu', 'kakutuiato'),\n",
       " ('to', 'kakutuiato'),\n",
       " ('ka', 'kakuva'),\n",
       " ('ku', 'kakuva'),\n",
       " ('va', 'kakuva'),\n",
       " ('ka', 'kakuvira'),\n",
       " ('ku', 'kakuvira'),\n",
       " ('vi', 'kakuvira'),\n",
       " ('ra', 'kakuvira'),\n",
       " ('ka', 'kameoro'),\n",
       " ('ro', 'kameoro'),\n",
       " ('ka', 'kandora'),\n",
       " ('ra', 'kandora'),\n",
       " ('ka', 'kaokao'),\n",
       " ('ka', 'kaokao'),\n",
       " ('ka', 'kaokaoara'),\n",
       " ('ka', 'kaokaoara'),\n",
       " ('ra', 'kaokaoara'),\n",
       " ('ka', 'kaokaoto'),\n",
       " ('ka', 'kaokaoto'),\n",
       " ('to', 'kaokaoto'),\n",
       " ('ka', 'kapa'),\n",
       " ('pa', 'kapa'),\n",
       " ('ka', 'kapa'),\n",
       " ('pa', 'kapa'),\n",
       " ('ka', 'kapaava'),\n",
       " ('pa', 'kapaava'),\n",
       " ('va', 'kapaava'),\n",
       " ('ka', 'kapai'),\n",
       " ('pa', 'kapai'),\n",
       " ('ka', 'kapara'),\n",
       " ('pa', 'kapara'),\n",
       " ('ra', 'kapara'),\n",
       " ('ka', 'kaparu'),\n",
       " ('pa', 'kaparu'),\n",
       " ('ru', 'kaparu'),\n",
       " ('ka', 'kaparuvira'),\n",
       " ('pa', 'kaparuvira'),\n",
       " ('ru', 'kaparuvira'),\n",
       " ('vi', 'kaparuvira'),\n",
       " ('ra', 'kaparuvira'),\n",
       " ('ka', 'kapatau'),\n",
       " ('pa', 'kapatau'),\n",
       " ('ta', 'kapatau'),\n",
       " ('ka', 'kapatoro'),\n",
       " ('pa', 'kapatoro'),\n",
       " ('to', 'kapatoro'),\n",
       " ('ro', 'kapatoro'),\n",
       " ('ka', 'kapatoroto'),\n",
       " ('pa', 'kapatoroto'),\n",
       " ('to', 'kapatoroto'),\n",
       " ('ro', 'kapatoroto'),\n",
       " ('to', 'kapatoroto'),\n",
       " ('ka', 'kape'),\n",
       " ('pe', 'kape'),\n",
       " ('ka', 'kapeaa'),\n",
       " ('pe', 'kapeaa'),\n",
       " ('ka', 'kapeaa'),\n",
       " ('pe', 'kapeaa'),\n",
       " ('ka', 'kapeaavira'),\n",
       " ('pe', 'kapeaavira'),\n",
       " ('vi', 'kapeaavira'),\n",
       " ('ra', 'kapeaavira'),\n",
       " ('ka', 'kapekape'),\n",
       " ('pe', 'kapekape'),\n",
       " ('ka', 'kapekape'),\n",
       " ('pe', 'kapekape'),\n",
       " ('ka', 'kapekapevira'),\n",
       " ('pe', 'kapekapevira'),\n",
       " ('ka', 'kapekapevira'),\n",
       " ('pe', 'kapekapevira'),\n",
       " ('vi', 'kapekapevira'),\n",
       " ('ra', 'kapekapevira'),\n",
       " ('ka', 'kapere'),\n",
       " ('pe', 'kapere'),\n",
       " ('re', 'kapere'),\n",
       " ('ka', 'kaperepie'),\n",
       " ('pe', 'kaperepie'),\n",
       " ('re', 'kaperepie'),\n",
       " ('pi', 'kaperepie'),\n",
       " ('ka', 'kapi'),\n",
       " ('pi', 'kapi'),\n",
       " ('ka', 'kapiaa'),\n",
       " ('pi', 'kapiaa'),\n",
       " ('ka', 'kapiaavira'),\n",
       " ('pi', 'kapiaavira'),\n",
       " ('vi', 'kapiaavira'),\n",
       " ('ra', 'kapiaavira'),\n",
       " ('ka', 'kapikapi'),\n",
       " ('pi', 'kapikapi'),\n",
       " ('ka', 'kapikapi'),\n",
       " ('pi', 'kapikapi'),\n",
       " ('ka', 'kapiro'),\n",
       " ('pi', 'kapiro'),\n",
       " ('ro', 'kapiro'),\n",
       " ('ka', 'kapiroa'),\n",
       " ('pi', 'kapiroa'),\n",
       " ('ro', 'kapiroa'),\n",
       " ('ka', 'kapiroko'),\n",
       " ('pi', 'kapiroko'),\n",
       " ('ro', 'kapiroko'),\n",
       " ('ko', 'kapiroko'),\n",
       " ('ka', 'kapisi'),\n",
       " ('pi', 'kapisi'),\n",
       " ('si', 'kapisi'),\n",
       " ('ka', 'kapisito'),\n",
       " ('pi', 'kapisito'),\n",
       " ('si', 'kapisito'),\n",
       " ('to', 'kapisito'),\n",
       " ('ka', 'kapiu'),\n",
       " ('pi', 'kapiu'),\n",
       " ('ka', 'kapiua'),\n",
       " ('pi', 'kapiua'),\n",
       " ('ka', 'kapo'),\n",
       " ('po', 'kapo'),\n",
       " ('ka', 'kapoa'),\n",
       " ('po', 'kapoa'),\n",
       " ('ka', 'kapokao'),\n",
       " ('po', 'kapokao'),\n",
       " ('ka', 'kapokao'),\n",
       " ('ka', 'kapokapo'),\n",
       " ('po', 'kapokapo'),\n",
       " ('ka', 'kapokapo'),\n",
       " ('po', 'kapokapo'),\n",
       " ('ka', 'kapokapoa'),\n",
       " ('po', 'kapokapoa'),\n",
       " ('ka', 'kapokapoa'),\n",
       " ('po', 'kapokapoa'),\n",
       " ('ka', 'kapokapora'),\n",
       " ('po', 'kapokapora'),\n",
       " ('ka', 'kapokapora'),\n",
       " ('po', 'kapokapora'),\n",
       " ('ra', 'kapokapora'),\n",
       " ('ka', 'kapokaporo'),\n",
       " ('po', 'kapokaporo'),\n",
       " ('ka', 'kapokaporo'),\n",
       " ('po', 'kapokaporo'),\n",
       " ('ro', 'kapokaporo'),\n",
       " ('ka', 'kapokari'),\n",
       " ('po', 'kapokari'),\n",
       " ('ka', 'kapokari'),\n",
       " ('ri', 'kapokari'),\n",
       " ('ka', 'kapokarito'),\n",
       " ('po', 'kapokarito'),\n",
       " ('ka', 'kapokarito'),\n",
       " ('ri', 'kapokarito'),\n",
       " ('to', 'kapokarito'),\n",
       " ('ka', 'kapokoa'),\n",
       " ('po', 'kapokoa'),\n",
       " ('ko', 'kapokoa'),\n",
       " ('ka', 'kapoo'),\n",
       " ('po', 'kapoo'),\n",
       " ('ka', 'kapooto'),\n",
       " ('po', 'kapooto'),\n",
       " ('to', 'kapooto'),\n",
       " ('ka', 'kapoovira'),\n",
       " ('po', 'kapoovira'),\n",
       " ('vi', 'kapoovira'),\n",
       " ('ra', 'kapoovira'),\n",
       " ('ka', 'kapopaa'),\n",
       " ('po', 'kapopaa'),\n",
       " ('pa', 'kapopaa'),\n",
       " ('ka', 'kaporo'),\n",
       " ('po', 'kaporo'),\n",
       " ('ro', 'kaporo'),\n",
       " ('ka', 'kaporo'),\n",
       " ('po', 'kaporo'),\n",
       " ('ro', 'kaporo'),\n",
       " ('ka', 'kaporopa'),\n",
       " ('po', 'kaporopa'),\n",
       " ('ro', 'kaporopa'),\n",
       " ('pa', 'kaporopa'),\n",
       " ('ka', 'kaporoto'),\n",
       " ('po', 'kaporoto'),\n",
       " ('ro', 'kaporoto'),\n",
       " ('to', 'kaporoto'),\n",
       " ('ka', 'kapoto'),\n",
       " ('po', 'kapoto'),\n",
       " ('to', 'kapoto'),\n",
       " ('ka', 'kapu'),\n",
       " ('pu', 'kapu'),\n",
       " ('ka', 'kapua'),\n",
       " ('pu', 'kapua'),\n",
       " ('ka', 'kapua'),\n",
       " ('pu', 'kapua'),\n",
       " ('ka', 'kapuapato'),\n",
       " ('pu', 'kapuapato'),\n",
       " ('pa', 'kapuapato'),\n",
       " ('to', 'kapuapato'),\n",
       " ('ka', 'kapuapie'),\n",
       " ('pu', 'kapuapie'),\n",
       " ('pi', 'kapuapie'),\n",
       " ('ka', 'kapuasisi'),\n",
       " ('pu', 'kapuasisi'),\n",
       " ('si', 'kapuasisi'),\n",
       " ('si', 'kapuasisi'),\n",
       " ('ka', 'kapupie'),\n",
       " ('pu', 'kapupie'),\n",
       " ('pi', 'kapupie'),\n",
       " ('ka', 'kapupiea'),\n",
       " ('pu', 'kapupiea'),\n",
       " ('pi', 'kapupiea'),\n",
       " ('ka', 'kapupiepaa'),\n",
       " ('pu', 'kapupiepaa'),\n",
       " ('pi', 'kapupiepaa'),\n",
       " ('pa', 'kapupiepaa'),\n",
       " ('ka', 'kapuu'),\n",
       " ('pu', 'kapuu'),\n",
       " ('ka', 'kapuupie'),\n",
       " ('pu', 'kapuupie'),\n",
       " ('pi', 'kapuupie'),\n",
       " ('ka', 'kapuupiepa'),\n",
       " ('pu', 'kapuupiepa'),\n",
       " ('pi', 'kapuupiepa'),\n",
       " ('pa', 'kapuupiepa'),\n",
       " ('ra', 'Kara'),\n",
       " ('ka', 'kara'),\n",
       " ('ra', 'kara'),\n",
       " ('ka', 'karaava'),\n",
       " ('ra', 'karaava'),\n",
       " ('va', 'karaava'),\n",
       " ('ka', 'karakarao'),\n",
       " ('ra', 'karakarao'),\n",
       " ('ka', 'karakarao'),\n",
       " ('ra', 'karakarao'),\n",
       " ('ka', 'karakaraoa'),\n",
       " ('ra', 'karakaraoa'),\n",
       " ('ka', 'karakaraoa'),\n",
       " ('ra', 'karakaraoa'),\n",
       " ('ka', 'karakaraoto'),\n",
       " ('ra', 'karakaraoto'),\n",
       " ('ka', 'karakaraoto'),\n",
       " ('ra', 'karakaraoto'),\n",
       " ('to', 'karakaraoto'),\n",
       " ('ka', 'karakaraovira'),\n",
       " ('ra', 'karakaraovira'),\n",
       " ('ka', 'karakaraovira'),\n",
       " ('ra', 'karakaraovira'),\n",
       " ('vi', 'karakaraovira'),\n",
       " ('ra', 'karakaraovira'),\n",
       " ('ka', 'karakaroto'),\n",
       " ('ra', 'karakaroto'),\n",
       " ('ka', 'karakaroto'),\n",
       " ('ro', 'karakaroto'),\n",
       " ('to', 'karakaroto'),\n",
       " ('ka', 'karakuku'),\n",
       " ('ra', 'karakuku'),\n",
       " ('ku', 'karakuku'),\n",
       " ('ku', 'karakuku'),\n",
       " ('ka', 'karaova'),\n",
       " ('ra', 'karaova'),\n",
       " ('va', 'karaova'),\n",
       " ('ka', 'karapi'),\n",
       " ('ra', 'karapi'),\n",
       " ('pi', 'karapi'),\n",
       " ('ka', 'karapivira'),\n",
       " ('ra', 'karapivira'),\n",
       " ('pi', 'karapivira'),\n",
       " ('vi', 'karapivira'),\n",
       " ('ra', 'karapivira'),\n",
       " ('ka', 'karara'),\n",
       " ('ra', 'karara'),\n",
       " ('ra', 'karara'),\n",
       " ('ka', 'karata'),\n",
       " ('ra', 'karata'),\n",
       " ('ta', 'karata'),\n",
       " ('ka', 'karato'),\n",
       " ('ra', 'karato'),\n",
       " ('to', 'karato'),\n",
       " ('ka', 'karavau'),\n",
       " ('ra', 'karavau'),\n",
       " ('va', 'karavau'),\n",
       " ('ka', 'karavisi'),\n",
       " ('ra', 'karavisi'),\n",
       " ('vi', 'karavisi'),\n",
       " ('si', 'karavisi'),\n",
       " ('ka', 'karavisito'),\n",
       " ('ra', 'karavisito'),\n",
       " ('vi', 'karavisito'),\n",
       " ('si', 'karavisito'),\n",
       " ('to', 'karavisito'),\n",
       " ('ka', 'karavuru'),\n",
       " ('ra', 'karavuru'),\n",
       " ('vu', 'karavuru'),\n",
       " ('ru', 'karavuru'),\n",
       " ('ka', 'kare'),\n",
       " ('re', 'kare'),\n",
       " ('ka', 'kare'),\n",
       " ('re', 'kare'),\n",
       " ('ka', 'karekare'),\n",
       " ('re', 'karekare'),\n",
       " ('ka', 'karekare'),\n",
       " ('re', 'karekare'),\n",
       " ('ka', 'karekare'),\n",
       " ('re', 'karekare'),\n",
       " ('ka', 'karekare'),\n",
       " ('re', 'karekare'),\n",
       " ('ka', 'karekarererava'),\n",
       " ('re', 'karekarererava'),\n",
       " ('ka', 'karekarererava'),\n",
       " ('re', 'karekarererava'),\n",
       " ('re', 'karekarererava'),\n",
       " ('ra', 'karekarererava'),\n",
       " ('va', 'karekarererava'),\n",
       " ('ka', 'karekareto'),\n",
       " ('re', 'karekareto'),\n",
       " ('ka', 'karekareto'),\n",
       " ('re', 'karekareto'),\n",
       " ('to', 'karekareto'),\n",
       " ('ka', 'kareke'),\n",
       " ('re', 'kareke'),\n",
       " ('ke', 'kareke'),\n",
       " ('ka', 'karekepie'),\n",
       " ('re', 'karekepie'),\n",
       " ('ke', 'karekepie'),\n",
       " ('pi', 'karekepie'),\n",
       " ('ka', 'karekova'),\n",
       " ('re', 'karekova'),\n",
       " ('ko', 'karekova'),\n",
       " ('va', 'karekova'),\n",
       " ('ka', 'kareo'),\n",
       " ('re', 'kareo'),\n",
       " ('ka', 'kareovira'),\n",
       " ('re', 'kareovira'),\n",
       " ('vi', 'kareovira'),\n",
       " ('ra', 'kareovira'),\n",
       " ('ka', 'karepie'),\n",
       " ('re', 'karepie'),\n",
       " ('pi', 'karepie'),\n",
       " ('ka', 'karepieto'),\n",
       " ('re', 'karepieto'),\n",
       " ('pi', 'karepieto'),\n",
       " ('to', 'karepieto'),\n",
       " ('re', 'Karepirie'),\n",
       " ('pi', 'Karepirie'),\n",
       " ('ri', 'Karepirie'),\n",
       " ('ka', 'kari'),\n",
       " ('ri', 'kari'),\n",
       " ('ka', 'karia'),\n",
       " ('ri', 'karia'),\n",
       " ('ka', 'kariava'),\n",
       " ('ri', 'kariava'),\n",
       " ('va', 'kariava'),\n",
       " ('ka', 'karikari'),\n",
       " ('ri', 'karikari'),\n",
       " ('ka', 'karikari'),\n",
       " ('ri', 'karikari'),\n",
       " ('ka', 'karirapa'),\n",
       " ('ri', 'karirapa'),\n",
       " ('ra', 'karirapa'),\n",
       " ('pa', 'karirapa'),\n",
       " ('ka', 'karisito'),\n",
       " ('ri', 'karisito'),\n",
       " ('si', 'karisito'),\n",
       " ('to', 'karisito'),\n",
       " ('ka', 'karivai'),\n",
       " ('ri', 'karivai'),\n",
       " ('va', 'karivai'),\n",
       " ('ka', 'karivaito'),\n",
       " ('ri', 'karivaito'),\n",
       " ('va', 'karivaito'),\n",
       " ('to', 'karivaito'),\n",
       " ('ka', 'karivara'),\n",
       " ('ri', 'karivara'),\n",
       " ('va', 'karivara'),\n",
       " ('ra', 'karivara'),\n",
       " ('ka', 'karo'),\n",
       " ('ro', 'karo'),\n",
       " ('ka', 'karokaropo'),\n",
       " ('ro', 'karokaropo'),\n",
       " ('ka', 'karokaropo'),\n",
       " ('ro', 'karokaropo'),\n",
       " ('po', 'karokaropo'),\n",
       " ('ka', 'karopato'),\n",
       " ('ro', 'karopato'),\n",
       " ('pa', 'karopato'),\n",
       " ('to', 'karopato'),\n",
       " ('ka', 'karopo'),\n",
       " ('ro', 'karopo'),\n",
       " ('po', 'karopo'),\n",
       " ('ka', 'karot'),\n",
       " ('ro', 'karot'),\n",
       " ('ka', 'karoto'),\n",
       " ('ro', 'karoto'),\n",
       " ('to', 'karoto'),\n",
       " ('ka', 'karova'),\n",
       " ('ro', 'karova'),\n",
       " ('va', 'karova'),\n",
       " ('ka', 'karu'),\n",
       " ('ru', 'karu'),\n",
       " ('ka', 'karuka'),\n",
       " ('ru', 'karuka'),\n",
       " ('ka', 'karuka'),\n",
       " ('ka', 'karukaru'),\n",
       " ('ru', 'karukaru'),\n",
       " ('ka', 'karukaru'),\n",
       " ('ru', 'karukaru'),\n",
       " ('ka', 'karukava'),\n",
       " ('ru', 'karukava'),\n",
       " ('ka', 'karukava'),\n",
       " ('va', 'karukava'),\n",
       " ('ru', 'Karuru'),\n",
       " ('ru', 'Karuru'),\n",
       " ('ka', 'karutu'),\n",
       " ('ru', 'karutu'),\n",
       " ('tu', 'karutu'),\n",
       " ('ka', 'karuvira'),\n",
       " ('ru', 'karuvira'),\n",
       " ('vi', 'karuvira'),\n",
       " ('ra', 'karuvira'),\n",
       " ('ka', 'kasi'),\n",
       " ('si', 'kasi'),\n",
       " ('ka', 'kasi'),\n",
       " ('si', 'kasi'),\n",
       " ('ka', 'kasi'),\n",
       " ('si', 'kasi'),\n",
       " ('ka', 'kasiarao'),\n",
       " ('si', 'kasiarao'),\n",
       " ('ra', 'kasiarao'),\n",
       " ('ka', 'kasiava'),\n",
       " ('si', 'kasiava'),\n",
       " ('va', 'kasiava'),\n",
       " ('ka', 'kasikasi'),\n",
       " ('si', 'kasikasi'),\n",
       " ('ka', 'kasikasi'),\n",
       " ('si', 'kasikasi'),\n",
       " ('si', 'Kasikasiua'),\n",
       " ('ka', 'Kasikasiua'),\n",
       " ('si', 'Kasikasiua'),\n",
       " ('ka', 'kasipie'),\n",
       " ('si', 'kasipie'),\n",
       " ('pi', 'kasipie'),\n",
       " ('ka', 'kasipu'),\n",
       " ('si', 'kasipu'),\n",
       " ('pu', 'kasipu'),\n",
       " ('ka', 'kasipupie'),\n",
       " ('si', 'kasipupie'),\n",
       " ('pu', 'kasipupie'),\n",
       " ('pi', 'kasipupie'),\n",
       " ('ka', 'kasipuvira'),\n",
       " ('si', 'kasipuvira'),\n",
       " ('pu', 'kasipuvira'),\n",
       " ('vi', 'kasipuvira'),\n",
       " ('ra', 'kasipuvira'),\n",
       " ('ka', 'kasirao'),\n",
       " ('si', 'kasirao'),\n",
       " ('ra', 'kasirao'),\n",
       " ('ka', 'kasiraopie'),\n",
       " ('si', 'kasiraopie'),\n",
       " ('ra', 'kasiraopie'),\n",
       " ('pi', 'kasiraopie'),\n",
       " ('ka', 'kasiraovira'),\n",
       " ('si', 'kasiraovira'),\n",
       " ('ra', 'kasiraovira'),\n",
       " ('vi', 'kasiraovira'),\n",
       " ('ra', 'kasiraovira'),\n",
       " ('ka', 'kasiu'),\n",
       " ('si', 'kasiu'),\n",
       " ('ka', 'kasiura'),\n",
       " ('si', 'kasiura'),\n",
       " ('ra', 'kasiura'),\n",
       " ('ka', 'kasivari'),\n",
       " ('si', 'kasivari'),\n",
       " ('va', 'kasivari'),\n",
       " ('ri', 'kasivari'),\n",
       " ('ka', 'kasuari'),\n",
       " ('su', 'kasuari'),\n",
       " ('ri', 'kasuari'),\n",
       " ('ka', 'kata'),\n",
       " ('ta', 'kata'),\n",
       " ('ka', 'katai'),\n",
       " ('ta', 'katai'),\n",
       " ('ka', 'kataitoarei'),\n",
       " ('ta', 'kataitoarei'),\n",
       " ('to', 'kataitoarei'),\n",
       " ('re', 'kataitoarei'),\n",
       " ('ka', 'katakatai'),\n",
       " ('ta', 'katakatai'),\n",
       " ('ka', 'katakatai'),\n",
       " ('ta', 'katakatai'),\n",
       " ('ka', 'katakataivira'),\n",
       " ('ta', 'katakataivira'),\n",
       " ('ka', 'katakataivira'),\n",
       " ('ta', 'katakataivira'),\n",
       " ('vi', 'katakataivira'),\n",
       " ('ra', 'katakataivira'),\n",
       " ('ka', 'kataraua'),\n",
       " ('ta', 'kataraua'),\n",
       " ('ra', 'kataraua'),\n",
       " ('ka', 'katarauto'),\n",
       " ('ta', 'katarauto'),\n",
       " ('ra', 'katarauto'),\n",
       " ('to', 'katarauto'),\n",
       " ('ka', 'katavira'),\n",
       " ('ta', 'katavira'),\n",
       " ('vi', 'katavira'),\n",
       " ('ra', 'katavira'),\n",
       " ('ka', 'katokato'),\n",
       " ('to', 'katokato'),\n",
       " ('ka', 'katokato'),\n",
       " ('to', 'katokato'),\n",
       " ('ka', 'katokatoto'),\n",
       " ('to', 'katokatoto'),\n",
       " ('ka', 'katokatoto'),\n",
       " ('to', 'katokatoto'),\n",
       " ('to', 'katokatoto'),\n",
       " ('ka', 'katokatovira'),\n",
       " ('to', 'katokatovira'),\n",
       " ('ka', 'katokatovira'),\n",
       " ('to', 'katokatovira'),\n",
       " ('vi', 'katokatovira'),\n",
       " ('ra', 'katokatovira'),\n",
       " ('ka', 'katokoi'),\n",
       " ('to', 'katokoi'),\n",
       " ('ko', 'katokoi'),\n",
       " ('ka', 'katopato'),\n",
       " ('to', 'katopato'),\n",
       " ('pa', 'katopato'),\n",
       " ('to', 'katopato'),\n",
       " ('ka', 'katoto'),\n",
       " ('to', 'katoto'),\n",
       " ('to', 'katoto'),\n",
       " ('ka', 'katuara'),\n",
       " ('tu', 'katuara'),\n",
       " ('ra', 'katuara'),\n",
       " ('ka', 'katuarato'),\n",
       " ('tu', 'katuarato'),\n",
       " ('ra', 'katuarato'),\n",
       " ('to', 'katuarato'),\n",
       " ('ka', 'katukatu'),\n",
       " ('tu', 'katukatu'),\n",
       " ('ka', 'katukatu'),\n",
       " ('tu', 'katukatu'),\n",
       " ('ka', 'katuta'),\n",
       " ('tu', 'katuta'),\n",
       " ('ta', 'katuta'),\n",
       " ('ka', 'kau'),\n",
       " ('ka', 'kaukau'),\n",
       " ('ka', 'kaukau'),\n",
       " ('ka', 'kaukaupie'),\n",
       " ('ka', 'kaukaupie'),\n",
       " ('pi', 'kaukaupie'),\n",
       " ('ka', 'kaukauvira'),\n",
       " ('ka', 'kaukauvira'),\n",
       " ('vi', 'kaukauvira'),\n",
       " ('ra', 'kaukauvira'),\n",
       " ('ka', 'kaukovo'),\n",
       " ('ko', 'kaukovo'),\n",
       " ('vo', 'kaukovo'),\n",
       " ('ka', 'kauo'),\n",
       " ('ka', 'kauokauo'),\n",
       " ('ka', 'kauokauo'),\n",
       " ('ka', 'kaureo'),\n",
       " ('re', 'kaureo'),\n",
       " ('ka', 'kaureoto'),\n",
       " ('re', 'kaureoto'),\n",
       " ('to', 'kaureoto'),\n",
       " ('ka', 'kausiopa'),\n",
       " ('si', 'kausiopa'),\n",
       " ('pa', 'kausiopa'),\n",
       " ('ka', 'kausiovira'),\n",
       " ('si', 'kausiovira'),\n",
       " ('vi', 'kausiovira'),\n",
       " ('ra', 'kausiovira'),\n",
       " ('va', 'Kava'),\n",
       " ('ka', 'kavakavau'),\n",
       " ('va', 'kavakavau'),\n",
       " ('ka', 'kavakavau'),\n",
       " ('va', 'kavakavau'),\n",
       " ('ka', 'kavatao'),\n",
       " ('va', 'kavatao'),\n",
       " ('ta', 'kavatao'),\n",
       " ('va', 'Kavatara'),\n",
       " ('ta', 'Kavatara'),\n",
       " ('ra', 'Kavatara'),\n",
       " ('ka', 'kavau'),\n",
       " ('va', 'kavau'),\n",
       " ('ka', 'kavau'),\n",
       " ('va', 'kavau'),\n",
       " ('ka', 'kavau asiava'),\n",
       " ('va', 'kavau asiava'),\n",
       " ('si', 'kavau asiava'),\n",
       " ('va', 'kavau asiava'),\n",
       " ('ka', 'kave'),\n",
       " ('ve', 'kave'),\n",
       " ('ka', 'kavee'),\n",
       " ('ve', 'kavee'),\n",
       " ('ka', 'kaveepaa'),\n",
       " ('ve', 'kaveepaa'),\n",
       " ('pa', 'kaveepaa'),\n",
       " ('ka', 'kaverui'),\n",
       " ('ve', 'kaverui'),\n",
       " ('ru', 'kaverui'),\n",
       " ('ka', 'kaveruko'),\n",
       " ('ve', 'kaveruko'),\n",
       " ('ru', 'kaveruko'),\n",
       " ('ko', 'kaveruko'),\n",
       " ('ka', 'kavesi'),\n",
       " ('ve', 'kavesi'),\n",
       " ('si', 'kavesi'),\n",
       " ('ka', 'kavikavi'),\n",
       " ('vi', 'kavikavi'),\n",
       " ('ka', 'kavikavi'),\n",
       " ('vi', 'kavikavi'),\n",
       " ('ka', 'kavikaviru'),\n",
       " ('vi', 'kavikaviru'),\n",
       " ('ka', 'kavikaviru'),\n",
       " ('vi', 'kavikaviru'),\n",
       " ('ru', 'kavikaviru'),\n",
       " ('ka', 'kavikaviru'),\n",
       " ('vi', 'kavikaviru'),\n",
       " ('ka', 'kavikaviru'),\n",
       " ('vi', 'kavikaviru'),\n",
       " ('ru', 'kavikaviru'),\n",
       " ('ka', 'kaviko'),\n",
       " ('vi', 'kaviko'),\n",
       " ('ko', 'kaviko'),\n",
       " ('ka', 'kavikoa'),\n",
       " ('vi', 'kavikoa'),\n",
       " ('ko', 'kavikoa'),\n",
       " ('ka', 'kaviru'),\n",
       " ('vi', 'kaviru'),\n",
       " ('ru', 'kaviru'),\n",
       " ('ka', 'kaviru'),\n",
       " ('vi', 'kaviru'),\n",
       " ('ru', 'kaviru'),\n",
       " ('ka', 'kaviruto'),\n",
       " ('vi', 'kaviruto'),\n",
       " ('ru', 'kaviruto'),\n",
       " ('to', 'kaviruto'),\n",
       " ('ka', 'kaviruvira'),\n",
       " ('vi', 'kaviruvira'),\n",
       " ('ru', 'kaviruvira'),\n",
       " ('vi', 'kaviruvira'),\n",
       " ('ra', 'kaviruvira'),\n",
       " ('ka', 'kavo'),\n",
       " ('vo', 'kavo'),\n",
       " ('ka', 'kavokavo'),\n",
       " ('vo', 'kavokavo'),\n",
       " ('ka', 'kavokavo'),\n",
       " ('vo', 'kavokavo'),\n",
       " ('ka', 'kavokavoa'),\n",
       " ('vo', 'kavokavoa'),\n",
       " ('ka', 'kavokavoa'),\n",
       " ('vo', 'kavokavoa'),\n",
       " ('ka', 'kavokavoto'),\n",
       " ('vo', 'kavokavoto'),\n",
       " ('ka', 'kavokavoto'),\n",
       " ('vo', 'kavokavoto'),\n",
       " ('to', 'kavokavoto'),\n",
       " ('ka', 'kavora'),\n",
       " ('vo', 'kavora'),\n",
       " ('ra', 'kavora'),\n",
       " ('ka', 'kavorato'),\n",
       " ('vo', 'kavorato'),\n",
       " ('ra', 'kavorato'),\n",
       " ('to', 'kavorato'),\n",
       " ('ka', 'kavori'),\n",
       " ('vo', 'kavori'),\n",
       " ('ri', 'kavori'),\n",
       " ('ka', 'kavori'),\n",
       " ('vo', 'kavori'),\n",
       " ('ri', 'kavori'),\n",
       " ('ka', 'kavorou'),\n",
       " ('vo', 'kavorou'),\n",
       " ('ro', 'kavorou'),\n",
       " ('ka', 'kavovoa'),\n",
       " ('vo', 'kavovoa'),\n",
       " ('vo', 'kavovoa'),\n",
       " ('ka', 'kavovovira'),\n",
       " ('vo', 'kavovovira'),\n",
       " ('vo', 'kavovovira'),\n",
       " ('vi', 'kavovovira'),\n",
       " ('ra', 'kavovovira'),\n",
       " ('ka', 'kavu'),\n",
       " ('vu', 'kavu'),\n",
       " ('ka', 'kavu'),\n",
       " ('vu', 'kavu'),\n",
       " ('ka', 'kavuava'),\n",
       " ('vu', 'kavuava'),\n",
       " ('va', 'kavuava'),\n",
       " ('ka', 'kavupie'),\n",
       " ('vu', 'kavupie'),\n",
       " ('pi', 'kavupie'),\n",
       " ('ka', 'kavura'),\n",
       " ('vu', 'kavura'),\n",
       " ('ra', 'kavura'),\n",
       " ('ka', 'kavurao'),\n",
       " ('vu', 'kavurao'),\n",
       " ('ra', 'kavurao'),\n",
       " ('ka', 'kavusi'),\n",
       " ('vu', 'kavusi'),\n",
       " ('si', 'kavusi'),\n",
       " ('ka', 'kavuvo'),\n",
       " ('vu', 'kavuvo'),\n",
       " ('vo', 'kavuvo'),\n",
       " ('ke', 'kea'),\n",
       " ('ke', 'keakea'),\n",
       " ('ke', 'keakea'),\n",
       " ('ke', 'keakeato'),\n",
       " ('ke', 'keakeato'),\n",
       " ('to', 'keakeato'),\n",
       " ('ke', 'keari'),\n",
       " ...]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_word_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brute force stem-strip function\n",
    "def stem(word):\n",
    "        for suffix in ['ing', 'ly', 'ed', 'ious', 'ies', 'ive', 'es', 's', 'ment']:\n",
    "            if word.endswith(suffix):\n",
    "                return word[:-len(suffix)]\n",
    "        return word            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DENNIS',\n",
       " ':',\n",
       " 'Listen',\n",
       " ',',\n",
       " 'strange',\n",
       " 'women',\n",
       " 'ly',\n",
       " 'in',\n",
       " 'pond',\n",
       " 'distribut',\n",
       " 'sword',\n",
       " 'i',\n",
       " 'no',\n",
       " 'basi',\n",
       " 'for',\n",
       " 'a',\n",
       " 'system',\n",
       " 'of',\n",
       " 'govern',\n",
       " '.',\n",
       " 'Supreme',\n",
       " 'execut',\n",
       " 'power',\n",
       " 'deriv',\n",
       " 'from',\n",
       " 'a',\n",
       " 'mandate',\n",
       " 'from',\n",
       " 'the',\n",
       " 'mass',\n",
       " ',',\n",
       " 'not',\n",
       " 'from',\n",
       " 'some',\n",
       " 'farcical',\n",
       " 'aquatic',\n",
       " 'ceremony',\n",
       " '.']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another function, using regular expression\n",
    "# *? corresponds to a non-greedy Kleene star\n",
    "# The last ? is just the usual opitional ocurrence of the previous \n",
    "#regular expression\n",
    "\n",
    "def re_stem(word):\n",
    "    regexp =  r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'\n",
    "    stem, suffix = re.findall(regexp, word)[0]\n",
    "    return stem\n",
    "\n",
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "is no basis for a system of government.  Supreme executive power derives from\n",
    "a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "tokens = word_tokenize(raw)\n",
    "[re_stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching Tokenized Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monied; nervous; dangerous; white; white; white; pious; queer; good;\n",
      "mature; white; Cape; great; wise; wise; butterless; white; fiendish;\n",
      "pale; furious; better; certain; complete; dismasted; younger; brave;\n",
      "brave; brave; brave\n",
      "you rule bro; telling you bro; u twizted bro\n",
      "lol lol lol; lmao lol lol; lol lol lol; la la la la la; la la la; la\n",
      "la la; lovely lol lol love; lol lol lol.; la la la; la la la\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg, nps_chat\n",
    "moby = nltk.Text(gutenberg.words('melville-moby_dick.txt'))\n",
    "\n",
    "# Find any pattern 'a ___ man' and return only the missing word\n",
    "moby.findall(r\"<a> (<.*>) <man>\") \n",
    "\n",
    "# Find any pattern '___ ___ bro' on chat corpus\n",
    "chat = nltk.Text(nps_chat.words())\n",
    "chat.findall(r\"<.*> <.*> <bro>\")\n",
    "\n",
    "# Find sequences of 3 or more words that start with 'l'\n",
    "chat.findall(r\"<l.*>{3,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speed and other activities; water and other liquids; tomb and other\n",
      "landmarks; Statues and other monuments; pearls and other jewels;\n",
      "charts and other items; roads and other features; figures and other\n",
      "objects; military and other areas; demands and other factors;\n",
      "abstracts and other compilations; iron and other metals\n"
     ]
    }
   ],
   "source": [
    "# Finding hypernyms based on the phrase 'x and other y's'\n",
    "# See that 'demand and other factors' should be manually excluded (it is a false positive)\n",
    "from nltk.corpus import brown\n",
    "hobbies_learned = nltk.Text(brown.words(categories=['hobbies', 'learned']))\n",
    "hobbies_learned.findall(r\"<\\w*> <and> <other> <\\w*s>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization consists in making cpitalized independent words\n",
    "\n",
    "Lemmatization (substitute by word lemma) and Stemming (strip affixes) can be further added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing example text\n",
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "is no basis for a system of government.  Supreme executive power derives from\n",
    "a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "\n",
    "tokens = word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['denni',\n",
       " ':',\n",
       " 'listen',\n",
       " ',',\n",
       " 'strang',\n",
       " 'women',\n",
       " 'lie',\n",
       " 'in',\n",
       " 'pond',\n",
       " 'distribut',\n",
       " 'sword',\n",
       " 'is',\n",
       " 'no',\n",
       " 'basi',\n",
       " 'for',\n",
       " 'a',\n",
       " 'system',\n",
       " 'of',\n",
       " 'govern',\n",
       " '.',\n",
       " 'suprem',\n",
       " 'execut',\n",
       " 'power',\n",
       " 'deriv',\n",
       " 'from',\n",
       " 'a',\n",
       " 'mandat',\n",
       " 'from',\n",
       " 'the',\n",
       " 'mass',\n",
       " ',',\n",
       " 'not',\n",
       " 'from',\n",
       " 'some',\n",
       " 'farcic',\n",
       " 'aquat',\n",
       " 'ceremoni',\n",
       " '.']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['den',\n",
       " ':',\n",
       " 'list',\n",
       " ',',\n",
       " 'strange',\n",
       " 'wom',\n",
       " 'lying',\n",
       " 'in',\n",
       " 'pond',\n",
       " 'distribut',\n",
       " 'sword',\n",
       " 'is',\n",
       " 'no',\n",
       " 'bas',\n",
       " 'for',\n",
       " 'a',\n",
       " 'system',\n",
       " 'of',\n",
       " 'govern',\n",
       " '.',\n",
       " 'suprem',\n",
       " 'execut',\n",
       " 'pow',\n",
       " 'der',\n",
       " 'from',\n",
       " 'a',\n",
       " 'mand',\n",
       " 'from',\n",
       " 'the',\n",
       " 'mass',\n",
       " ',',\n",
       " 'not',\n",
       " 'from',\n",
       " 'som',\n",
       " 'farc',\n",
       " 'aqu',\n",
       " 'ceremony',\n",
       " '.']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing Porter and Lancaster stemmers\n",
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "[porter.stem(t) for t in tokens]\n",
    "\n",
    "[lancaster.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexedText(object):\n",
    "\n",
    "    def __init__(self, stemmer, text):\n",
    "        self._text = text\n",
    "        self._stemmer = stemmer\n",
    "        self._index = nltk.Index((self._stem(word), i)\n",
    "                                 for (i, word) in enumerate(text))\n",
    "\n",
    "    def concordance(self, word, width=40):\n",
    "        key = self._stem(word)\n",
    "        wc = int(width/4)                # words of context\n",
    "        for i in self._index[key]:\n",
    "            lcontext = ' '.join(self._text[i-wc:i])\n",
    "            rcontext = ' '.join(self._text[i:i+wc])\n",
    "            ldisplay = '{:>{width}}'.format(lcontext[-width:], width=width)\n",
    "            rdisplay = '{:{width}}'.format(rcontext[:width], width=width)\n",
    "            print(ldisplay, rdisplay)\n",
    "\n",
    "    def _stem(self, word):\n",
    "        return self._stemmer.stem(word).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r king ! DENNIS : Listen , strange women lying in ponds distributing swords is no\n",
      " beat a very brave retreat . ROBIN : All lies ! MINSTREL : [ singing ] Bravest of\n",
      "       Nay . Nay . Come . Come . You may lie here . Oh , but you are wounded !   \n",
      "doctors immediately ! No , no , please ! Lie down . [ clap clap ] PIGLET : Well  \n",
      "ere is much danger , for beyond the cave lies the Gorge of Eternal Peril , which \n",
      "   you . Oh ... TIM : To the north there lies a cave -- the cave of Caerbannog --\n",
      "h it and lived ! Bones of full fifty men lie strewn about its lair . So , brave k\n",
      "not stop our fight ' til each one of you lies dead , and the Holy Grail returns t\n"
     ]
    }
   ],
   "source": [
    "# Testing concordance on stem 'lie' (i.e. return derived words)\n",
    "porter = nltk.PorterStemmer()\n",
    "grail = nltk.corpus.webtext.words('grail.txt')\n",
    "text = IndexedText(porter, grail)\n",
    "text.concordance('lie')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DENNIS',\n",
       " ':',\n",
       " 'Listen',\n",
       " ',',\n",
       " 'strange',\n",
       " 'woman',\n",
       " 'lying',\n",
       " 'in',\n",
       " 'pond',\n",
       " 'distributing',\n",
       " 'sword',\n",
       " 'is',\n",
       " 'no',\n",
       " 'basis',\n",
       " 'for',\n",
       " 'a',\n",
       " 'system',\n",
       " 'of',\n",
       " 'government',\n",
       " '.',\n",
       " 'Supreme',\n",
       " 'executive',\n",
       " 'power',\n",
       " 'derives',\n",
       " 'from',\n",
       " 'a',\n",
       " 'mandate',\n",
       " 'from',\n",
       " 'the',\n",
       " 'mass',\n",
       " ',',\n",
       " 'not',\n",
       " 'from',\n",
       " 'some',\n",
       " 'farcical',\n",
       " 'aquatic',\n",
       " 'ceremony',\n",
       " '.']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing WordNet lemmatizer, which strips affizes only if stem form is in the dictionary\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "[wnl.lemmatize(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Expressions for Tokenizing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SImple Approaches to Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = \"\"\"'When I'M a Duchess,' she said to herself, (not in a very hopeful tone\n",
    "though), 'I won't have any pepper in my kitchen AT ALL. Soup does very\n",
    "well without--Maybe it's always pepper that makes people hot-tempered,'...\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'When\",\n",
       " \"I'M\",\n",
       " 'a',\n",
       " \"Duchess,'\",\n",
       " 'she',\n",
       " 'said',\n",
       " 'to',\n",
       " 'herself,',\n",
       " '(not',\n",
       " 'in',\n",
       " 'a',\n",
       " 'very',\n",
       " 'hopeful',\n",
       " 'tone\\nthough),',\n",
       " \"'I\",\n",
       " \"won't\",\n",
       " 'have',\n",
       " 'any',\n",
       " 'pepper',\n",
       " 'in',\n",
       " 'my',\n",
       " 'kitchen',\n",
       " 'AT',\n",
       " 'ALL.',\n",
       " 'Soup',\n",
       " 'does',\n",
       " 'very\\nwell',\n",
       " 'without--Maybe',\n",
       " \"it's\",\n",
       " 'always',\n",
       " 'pepper',\n",
       " 'that',\n",
       " 'makes',\n",
       " 'people',\n",
       " \"hot-tempered,'...\"]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[\"'When\",\n",
       " \"I'M\",\n",
       " 'a',\n",
       " \"Duchess,'\",\n",
       " 'she',\n",
       " 'said',\n",
       " 'to',\n",
       " 'herself,',\n",
       " '(not',\n",
       " 'in',\n",
       " 'a',\n",
       " 'very',\n",
       " 'hopeful',\n",
       " 'tone',\n",
       " 'though),',\n",
       " \"'I\",\n",
       " \"won't\",\n",
       " 'have',\n",
       " 'any',\n",
       " 'pepper',\n",
       " 'in',\n",
       " 'my',\n",
       " 'kitchen',\n",
       " 'AT',\n",
       " 'ALL.',\n",
       " 'Soup',\n",
       " 'does',\n",
       " 'very',\n",
       " 'well',\n",
       " 'without--Maybe',\n",
       " \"it's\",\n",
       " 'always',\n",
       " 'pepper',\n",
       " 'that',\n",
       " 'makes',\n",
       " 'people',\n",
       " \"hot-tempered,'...\"]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using whitespace\n",
    "re.split(r' ', raw)\n",
    "\n",
    "# Using Regular Expression\n",
    "# Don't forget to use 'r', so python doesn't interpret \\ as a wildcard\n",
    "re.split(r'[ \\t\\n]+', raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'When',\n",
       " 'I',\n",
       " 'M',\n",
       " 'a',\n",
       " 'Duchess',\n",
       " 'she',\n",
       " 'said',\n",
       " 'to',\n",
       " 'herself',\n",
       " 'not',\n",
       " 'in',\n",
       " 'a',\n",
       " 'very',\n",
       " 'hopeful',\n",
       " 'tone',\n",
       " 'though',\n",
       " 'I',\n",
       " 'won',\n",
       " 't',\n",
       " 'have',\n",
       " 'any',\n",
       " 'pepper',\n",
       " 'in',\n",
       " 'my',\n",
       " 'kitchen',\n",
       " 'AT',\n",
       " 'ALL',\n",
       " 'Soup',\n",
       " 'does',\n",
       " 'very',\n",
       " 'well',\n",
       " 'without',\n",
       " 'Maybe',\n",
       " 'it',\n",
       " 's',\n",
       " 'always',\n",
       " 'pepper',\n",
       " 'that',\n",
       " 'makes',\n",
       " 'people',\n",
       " 'hot',\n",
       " 'tempered',\n",
       " '']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A little more selective re to strip off non-word characters\n",
    "# Still not the best tokenizer, since it returns the empty string at the beggining and end of the text\n",
    "re.split(r'\\W+', raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'When\",\n",
       " 'I',\n",
       " \"'M\",\n",
       " 'a',\n",
       " 'Duchess',\n",
       " ',',\n",
       " \"'\",\n",
       " 'she',\n",
       " 'said',\n",
       " 'to',\n",
       " 'herself',\n",
       " ',',\n",
       " '(not',\n",
       " 'in',\n",
       " 'a',\n",
       " 'very',\n",
       " 'hopeful',\n",
       " 'tone',\n",
       " 'though',\n",
       " ')',\n",
       " ',',\n",
       " \"'I\",\n",
       " 'won',\n",
       " \"'t\",\n",
       " 'have',\n",
       " 'any',\n",
       " 'pepper',\n",
       " 'in',\n",
       " 'my',\n",
       " 'kitchen',\n",
       " 'AT',\n",
       " 'ALL',\n",
       " '.',\n",
       " 'Soup',\n",
       " 'does',\n",
       " 'very',\n",
       " 'well',\n",
       " 'without',\n",
       " '-',\n",
       " '-Maybe',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'always',\n",
       " 'pepper',\n",
       " 'that',\n",
       " 'makes',\n",
       " 'people',\n",
       " 'hot',\n",
       " '-tempered',\n",
       " ',',\n",
       " \"'\",\n",
       " '.',\n",
       " '.',\n",
       " '.']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use re to find any word character or any non-whitespace character followed by 0 or more wordcharacter\n",
    "re.findall(r'\\w+|\\S\\w*', raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'\", 'When', \"I'M\", 'a', 'Duchess', ',', \"'\", 'she', 'said', 'to', 'herself', ',', '(', 'not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', ')', ',', \"'\", 'I', \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', '.', 'Soup', 'does', 'very', 'well', 'without', '--', 'Maybe', \"it's\", 'always', 'pepper', 'that', 'makes', 'people', 'hot-tempered', ',', \"'\", '...']\n"
     ]
    }
   ],
   "source": [
    "# Going a bit further to include words with internal hyphens or apostrophes as single token\n",
    "# Also add a match for quote\n",
    "\n",
    "print(re.findall(r\"\\w+(?:[-']\\w+)*|'|[-.(]+|\\S\\w*\", raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See Table 3.4 for a list of regular expression symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK's Regular Expression Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', '', ''),\n",
       " ('A.', '', ''),\n",
       " ('', '-print', ''),\n",
       " ('', '', ''),\n",
       " ('', '', '.40'),\n",
       " ('', '', '')]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using nltk.regexp_tokenize() to do the same as re\n",
    "text = 'That U.S.A. poster-print costs $12.40...'\n",
    "pattern = r\"\"\"(?x)  # set flag to allow verbose regexps\n",
    "  ([A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "| \\w+(-\\w+)*        # words with optional internal hyphens\n",
    "| \\$?\\d+(\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
    "| \\.\\.\\.            # ellipsis\n",
    "| [][.,;\"'?():-_`]  # these are separate tokens; includes ], [\n",
    "\"\"\"\n",
    "\n",
    "nltk.regexp_tokenize(text, pattern)\n",
    "\n",
    "# The output seems wrong..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to develop a tokenizer, it can be compared with expert-tokenized text\n",
    "\n",
    "Use e.g. nltk.corpus.treebank_raw.raw() to tokenize and nltk.corpus.treebank.words() to compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"Nonsense!\"',\n",
      " 'said Gregory, who was very rational when anyone else\\nattempted paradox.',\n",
      " '\"Why do all the clerks and navvies in the\\n'\n",
      " 'railway trains look so sad and tired, so very sad and tired?',\n",
      " 'I will\\ntell you.',\n",
      " 'It is because they know that the train is going right.',\n",
      " 'It\\n'\n",
      " 'is because they know that whatever place they have taken a ticket\\n'\n",
      " 'for that place they will reach.',\n",
      " 'It is because after they have\\n'\n",
      " 'passed Sloane Square they know that the next station must be\\n'\n",
      " 'Victoria, and nothing but Victoria.',\n",
      " 'Oh, their wild rapture!',\n",
      " 'oh,\\n'\n",
      " 'their eyes like stars and their souls again in Eden, if the next\\n'\n",
      " 'station were unaccountably Baker Street!\"',\n",
      " '\"It is you who are unpoetical,\" replied the poet Syme.']\n"
     ]
    }
   ],
   "source": [
    "# Some corpus have segmented sents, but we need to have freedom to segment up to words\n",
    "\n",
    "# Use of Punkt segmenter to segment a text into sentences\n",
    "text = nltk.corpus.gutenberg.raw('chesterton-thursday.txt')\n",
    "sents = nltk.sent_tokenize(text)\n",
    "pprint.pprint(sents[79:89])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['do',\n",
       " 'you',\n",
       " 'see',\n",
       " 'the',\n",
       " 'kitty',\n",
       " 'see',\n",
       " 'the',\n",
       " 'doggy',\n",
       " 'do',\n",
       " 'you',\n",
       " 'like',\n",
       " 'the',\n",
       " 'kitty',\n",
       " 'like',\n",
       " 'the',\n",
       " 'doggy']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Introducing a function that segments a text based on a boolean vector (1 = split)\n",
    "\n",
    "def segment(text, segs):\n",
    "    words = []\n",
    "    last = 0\n",
    "    for i in range(len(segs)):\n",
    "        if segs[i] == '1':\n",
    "            words.append(text[last:i+1])\n",
    "            last = i+1\n",
    "    words.append(text[last:])\n",
    "    return words\n",
    "\n",
    "text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
    "seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
    "seg2 = \"0100100100100001001001000010100100010010000100010010000\"\n",
    "\n",
    "segment(text, seg1)\n",
    "\n",
    "segment(text, seg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now defining an evalution function, based on (Brent, 1995)\n",
    "def evaluate(text, segs):\n",
    "    words = segment(text, segs)\n",
    "    text_size = len(words)\n",
    "    lexicon_size = sum(len(word) + 1 for word in set(words))\n",
    "    return text_size + lexicon_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doyou',\n",
       " 'see',\n",
       " 'thekitt',\n",
       " 'y',\n",
       " 'see',\n",
       " 'thedogg',\n",
       " 'y',\n",
       " 'doyou',\n",
       " 'like',\n",
       " 'thekitt',\n",
       " 'y',\n",
       " 'like',\n",
       " 'thedogg',\n",
       " 'y']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluating three segmentations\n",
    "# LEast score = best\n",
    "text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
    "seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
    "seg2 = \"0100100100100001001001000010100100010010000100010010000\"\n",
    "seg3 = \"0000100100000011001000000110000100010000001100010000001\"\n",
    "segment(text, seg3)\n",
    "\n",
    "evaluate(text, seg1)\n",
    "evaluate(text, seg2)\n",
    "evaluate(text, seg3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function that tries to minimize the objective function\n",
    "from random import randint\n",
    "\n",
    "def flip(segs, pos):\n",
    "    return segs[:pos] + str(1-int(segs[pos])) + segs[pos+1:]\n",
    "\n",
    "def flip_n(segs, n):\n",
    "    for i in range(n):\n",
    "        segs = flip(segs, randint(0, len(segs)-1))\n",
    "    return segs\n",
    "\n",
    "def anneal(text, segs, iterations, cooling_rate):\n",
    "    temperature = float(len(segs))\n",
    "    while temperature > 0.5:\n",
    "        best_segs, best = segs, evaluate(text, segs)\n",
    "        for i in range(iterations):\n",
    "            guess = flip_n(segs, round(temperature))\n",
    "            score = evaluate(text, guess)\n",
    "            if score < best:\n",
    "                best, best_segs = score, guess\n",
    "        score, segs = best, best_segs\n",
    "        temperature = temperature / cooling_rate\n",
    "        print(evaluate(text, segs), segment(text, segs))\n",
    "    print()\n",
    "    return segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "61 ['doyou', 'seethe', 'kitty', 'seethe', 'd', 'oggy', 'd', 'oyou', 'likethe', 'kitty', 'likethe', 'do', 'gg', 'y']\n",
      "61 ['doyou', 'seethe', 'kitty', 'seethe', 'd', 'oggy', 'd', 'oyou', 'likethe', 'kitty', 'likethe', 'do', 'gg', 'y']\n",
      "61 ['doyou', 'seethe', 'kitty', 'seethe', 'd', 'oggy', 'd', 'oyou', 'likethe', 'kitty', 'likethe', 'do', 'gg', 'y']\n",
      "61 ['doyou', 'seethe', 'kitty', 'seethe', 'd', 'oggy', 'd', 'oyou', 'likethe', 'kitty', 'likethe', 'do', 'gg', 'y']\n",
      "61 ['doyou', 'seethe', 'kitty', 'seethe', 'd', 'oggy', 'd', 'oyou', 'likethe', 'kitty', 'likethe', 'do', 'gg', 'y']\n",
      "59 ['doyou', 'seethe', 'kitt', 'y', 'seethe', 'd', 'oggy', 'doyou', 'lik', 'ethe', 'kitt', 'y', 'lik', 'ethe', 'do', 'gg', 'y']\n",
      "59 ['doyou', 'seethe', 'kitt', 'y', 'seethe', 'd', 'oggy', 'doyou', 'lik', 'ethe', 'kitt', 'y', 'lik', 'ethe', 'do', 'gg', 'y']\n",
      "57 ['doyou', 'seethe', 'kitt', 'y', 'seethe', 'doggy', 'doyou', 'lik', 'ethe', 'kitt', 'y', 'lik', 'ethe', 'do', 'gg', 'y']\n",
      "53 ['doyou', 'seethe', 'kitt', 'y', 'seethe', 'doggy', 'doyou', 'lik', 'ethe', 'kitt', 'ylik', 'ethe', 'doggy']\n",
      "51 ['doyou', 'se', 'ethe', 'kitt', 'y', 'se', 'ethe', 'doggy', 'doyou', 'lik', 'ethe', 'kitt', 'ylik', 'ethe', 'doggy']\n",
      "47 ['doyou', 'se', 'ethe', 'kitt', 'y', 'se', 'ethe', 'doggy', 'doyou', 'lik', 'ethe', 'kitt', 'y', 'lik', 'ethe', 'doggy']\n",
      "44 ['doyou', 'se', 'ethe', 'kitty', 'se', 'ethe', 'doggy', 'doyou', 'lik', 'ethe', 'kitty', 'lik', 'ethe', 'doggy']\n",
      "44 ['doyou', 'se', 'ethe', 'kitty', 'se', 'ethe', 'doggy', 'doyou', 'lik', 'ethe', 'kitty', 'lik', 'ethe', 'doggy']\n",
      "44 ['doyou', 'se', 'ethe', 'kitty', 'se', 'ethe', 'doggy', 'doyou', 'lik', 'ethe', 'kitty', 'lik', 'ethe', 'doggy']\n",
      "44 ['doyou', 'se', 'ethe', 'kitty', 'se', 'ethe', 'doggy', 'doyou', 'lik', 'ethe', 'kitty', 'lik', 'ethe', 'doggy']\n",
      "44 ['doyou', 'se', 'ethe', 'kitty', 'se', 'ethe', 'doggy', 'doyou', 'lik', 'ethe', 'kitty', 'lik', 'ethe', 'doggy']\n",
      "44 ['doyou', 'se', 'ethe', 'kitty', 'se', 'ethe', 'doggy', 'doyou', 'lik', 'ethe', 'kitty', 'lik', 'ethe', 'doggy']\n",
      "44 ['doyou', 'se', 'ethe', 'kitty', 'se', 'ethe', 'doggy', 'doyou', 'lik', 'ethe', 'kitty', 'lik', 'ethe', 'doggy']\n",
      "44 ['doyou', 'se', 'ethe', 'kitty', 'se', 'ethe', 'doggy', 'doyou', 'lik', 'ethe', 'kitty', 'lik', 'ethe', 'doggy']\n",
      "44 ['doyou', 'se', 'ethe', 'kitty', 'se', 'ethe', 'doggy', 'doyou', 'lik', 'ethe', 'kitty', 'lik', 'ethe', 'doggy']\n",
      "44 ['doyou', 'se', 'ethe', 'kitty', 'se', 'ethe', 'doggy', 'doyou', 'lik', 'ethe', 'kitty', 'lik', 'ethe', 'doggy']\n",
      "44 ['doyou', 'se', 'ethe', 'kitty', 'se', 'ethe', 'doggy', 'doyou', 'lik', 'ethe', 'kitty', 'lik', 'ethe', 'doggy']\n",
      "44 ['doyou', 'se', 'ethe', 'kitty', 'se', 'ethe', 'doggy', 'doyou', 'lik', 'ethe', 'kitty', 'lik', 'ethe', 'doggy']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0000101000100001010001000010000100100010000100100010000'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
    "seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
    "anneal(text, seg1, 50000, 1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatting: from Lists to Strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Lists to Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We called him Tortoise because he taught us .'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'We;called;him;Tortoise;because;he;taught;us;.'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'WecalledhimTortoisebecausehetaughtus.'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Joining a list trough some example of joiners\n",
    "silly = ['We', 'called', 'him', 'Tortoise', 'because', 'he', 'taught', 'us', '.']\n",
    "' '.join(silly)\n",
    "';'.join(silly)\n",
    "''.join(silly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strings and Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat -> 3; dog -> 4; snake -> 1; "
     ]
    }
   ],
   "source": [
    "# A way to format a frequency distribution as a readable string of characters\n",
    "fdist = nltk.FreqDist(['dog', 'cat', 'dog', 'cat', 'dog', 'snake', 'dog', 'cat'])\n",
    "for word in sorted(fdist):\n",
    "    print(word, '->', fdist[word], end='; ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat->3; dog->4; snake->1; "
     ]
    }
   ],
   "source": [
    "for word in sorted(fdist):\n",
    "    print('{}->{};'.format(word, fdist[word]), end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lee wants a sandwich right now\n",
      "Lee wants a spam fritter right now\n",
      "Lee wants a pancake right now\n"
     ]
    }
   ],
   "source": [
    "template = 'Lee wants a {} right now'\n",
    "menu = ['sandwich', 'spam fritter', 'pancake']\n",
    "for snack in menu:\n",
    "    print(template.format(snack))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lining things up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    41'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'41    '"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'oi tudo bem         '"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'         oi tudo bem'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Right justified number output\n",
    "'{:6}'.format(41)\n",
    "\n",
    "# Left justified number output\n",
    "'{:<6}'.format(41)\n",
    "\n",
    "# The opposite for string\n",
    "'{:20}'.format('oi tudo bem')\n",
    "'{:>20}'.format('oi tudo bem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PI value: 3.1415926535897931159979634685441851615906'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Formatting precision: 40 digits after decimal point\n",
    "import math\n",
    "'PI value: {:.40f}'.format(math.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accuracy for 9375 words: 34.1867%'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# % symbol will be understood as percentage (no need to multiply by 100)\n",
    "count, total = 3205, 9375\n",
    "\"accuracy for {} words: {:.4%}\".format(total, count / total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a table viewer\n",
    "def tabulate(cfdist, words, categories):\n",
    "    print('{:16}'.format('Category'), end=' ')                    # column headings\n",
    "    for word in words:\n",
    "        print('{:>6}'.format(word), end=' ')\n",
    "    print()\n",
    "    for category in categories:\n",
    "        print('{:16}'.format(category), end=' ')                  # row heading\n",
    "        for word in words:                                        # for each word\n",
    "            print('{:6}'.format(cfdist[category][word]), end=' ') # print table cell\n",
    "        print()                                                   # end the row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category            can  could    may  might   must   will \n",
      "news                 93     86     66     38     50    389 \n",
      "religion             82     59     78     12     54     71 \n",
      "hobbies             268     58    131     22     83    264 \n",
      "science_fiction      16     49      4     12      8     16 \n",
      "romance              74    193     11     51     45     43 \n",
      "humor                16     30      8      8      9     13 \n"
     ]
    }
   ],
   "source": [
    "# Creating a frequency distribution and testing tabulate() function\n",
    "from nltk.corpus import brown\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "          (genre, word)\n",
    "          for genre in brown.categories()\n",
    "          for word in brown.words(categories=genre))\n",
    "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
    "tabulate(cfd, modals, genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Monty Python   '"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final note, the width of the formatting doesn't need to be fixed heuristically\n",
    "#it can be set to be the value of a variable\n",
    "\n",
    "'{:{width}}'.format(\"Monty Python\", width=15)\n",
    "# Where width could be the max length of all the labels, for example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writting the Results to a File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening a file named 'output.txt' and writing the 'english-kyv.txt' to it\n",
    "output_file = open('output.txt', 'w')\n",
    "words = set(nltk.corpus.genesis.words('english-kjv.txt'))\n",
    "for word in sorted(words):\n",
    "    print(word, file=output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Wrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After (5), all (3), is (2), said (4), and (3), done (4), , (1),\n",
      "more (4), is (2), said (4), than (4), done (4), . (1),\n"
     ]
    }
   ],
   "source": [
    "# Use textwrap's fill to break lines when characters get over a limit (default: 70)\n",
    "from textwrap import fill\n",
    "\n",
    "saying = ['After', 'all', 'is', 'said', 'and', 'done', ',',\n",
    "          'more', 'is', 'said', 'than', 'done', '.']\n",
    "\n",
    "format = '%s_(%d),'\n",
    "pieces = [format % (word, len(word)) for word in saying]\n",
    "output = ' '.join(pieces)\n",
    "wrapped = fill(output)\n",
    "print(wrapped.replace('_', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
